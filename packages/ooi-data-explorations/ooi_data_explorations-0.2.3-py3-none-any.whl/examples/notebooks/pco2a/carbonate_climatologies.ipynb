{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1b8777-67ac-4a8c-b436-d2ef4db9a50b",
   "metadata": {},
   "source": [
    "# Carbonate Climatologies using PCO2a sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06cd9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "from ooi_data_explorations.common import get_annotations, load_gc_thredds, add_annotation_qc_flags\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_pco2a import pco2a_datalogger\n",
    "from ooi_data_explorations.uncabled.process_phsen import phsen_datalogger, phsen_instrument\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420b7072-7bb2-4c70-ac33-e2a0e0506c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP10CNSM-RID27-07-PLIMSA000\n",
      "2024-05-01T00:00:00+00:00\n",
      "2024-10-31T23:59:59.999000+00:00\n",
      "pco2a_a_dcl_instrument_water_recovered\n",
      "recovered_host\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('ooiparams_refdes'))\n",
    "print(os.environ.get('ooiparams_startTime'))\n",
    "print(os.environ.get('ooiparams_endTime'))\n",
    "print(os.environ.get('ooiparams_stream'))\n",
    "print(os.environ.get('ooiparams_method'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85445909-48ed-4c2a-b144-af441ac93064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciw-gitlab-registry.intra.oceanobservatories.org:5050/kubernetes/jupyterhub_images/datascience-matlab:latest\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('JUPYTER_IMAGE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c49e17-6d3d-4839-b4a7-cff0ca6f6776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SHELL', '/bin/bash')\n",
      "('KUBERNETES_SERVICE_PORT_HTTPS', '443')\n",
      "('JUPYTERHUB_ADMIN_ACCESS', '1')\n",
      "('KUBERNETES_SERVICE_PORT', '443')\n",
      "('PROXY_API_SERVICE_HOST', '10.152.183.40')\n",
      "('CONDA_EXE', '/opt/conda/bin/conda')\n",
      "('_CE_M', '')\n",
      "('JUPYTERHUB_SERVICE_URL', 'http://0.0.0.0:8888/user/joffrey.peters@whoi.edu/')\n",
      "('HOSTNAME', 'jupyter-joffrey-2epeters-40whoi-2eedu')\n",
      "('LANGUAGE', 'C.UTF-8')\n",
      "('JUPYTERHUB_API_TOKEN', '0941b8c2994f488581a22f22c70316a2')\n",
      "('_START_SH_EXECUTED', '1')\n",
      "('PROXY_API_SERVICE_PORT', '8001')\n",
      "('JUPYTERHUB_BASE_URL', '/')\n",
      "('NB_UID', '1000')\n",
      "('JULIA_PKGDIR', '/opt/julia')\n",
      "('MEM_LIMIT', '34359738368')\n",
      "('PROXY_PUBLIC_PORT_80_TCP', 'tcp://10.152.183.224:80')\n",
      "('PROXY_PUBLIC_PORT', 'tcp://10.152.183.224:443')\n",
      "('XML_CATALOG_FILES', 'file:///opt/conda/etc/xml/catalog file:///etc/xml/catalog')\n",
      "('PROXY_PUBLIC_SERVICE_PORT_HTTP', '80')\n",
      "('PWD', '/home/jovyan')\n",
      "('GSETTINGS_SCHEMA_DIR', '/opt/conda/share/glib-2.0/schemas')\n",
      "('CONDA_PREFIX', '/opt/conda')\n",
      "('MEM_GUARANTEE', '8589934592')\n",
      "('GSETTINGS_SCHEMA_DIR_CONDA_BACKUP', '')\n",
      "('JUPYTER_IMAGE', 'ciw-gitlab-registry.intra.oceanobservatories.org:5050/kubernetes/jupyterhub_images/datascience-matlab:latest')\n",
      "('PROXY_API_PORT_8001_TCP_ADDR', '10.152.183.40')\n",
      "('RSTUDIO_WHICH_R', '/opt/conda/bin/R')\n",
      "('HUB_SERVICE_HOST', '10.152.183.83')\n",
      "('JUPYTERHUB_SERVER_NAME', '')\n",
      "('JUPYTERHUB_DEFAULT_URL', '/lab')\n",
      "('HOME', '/home/jovyan')\n",
      "('LANG', 'C.UTF-8')\n",
      "('KUBERNETES_PORT_443_TCP', 'tcp://10.152.183.1:443')\n",
      "('JPY_API_TOKEN', '0941b8c2994f488581a22f22c70316a2')\n",
      "('HUB_SERVICE_PORT', '8081')\n",
      "('PROXY_API_PORT_8001_TCP_PORT', '8001')\n",
      "('NB_GID', '100')\n",
      "('CONDA_PROMPT_MODIFIER', '(base) ')\n",
      "('JUPYTERHUB_SERVICE_PREFIX', '/user/joffrey.peters@whoi.edu/')\n",
      "('JUPYTERHUB_OAUTH_CALLBACK_URL', '/user/joffrey.peters@whoi.edu/oauth_callback')\n",
      "('PROXY_PUBLIC_PORT_443_TCP', 'tcp://10.152.183.224:443')\n",
      "('PROXY_PUBLIC_PORT_443_TCP_PORT', '443')\n",
      "('PROXY_PUBLIC_SERVICE_HOST', '10.152.183.224')\n",
      "('JUPYTERHUB_SINGLEUSER_APP', 'jupyter_server.serverapp.ServerApp')\n",
      "('PROXY_PUBLIC_PORT_80_TCP_PROTO', 'tcp')\n",
      "('JUPYTERHUB_OAUTH_CLIENT_ALLOWED_SCOPES', '[]')\n",
      "('HUB_PORT', 'tcp://10.152.183.83:8081')\n",
      "('PROXY_PUBLIC_PORT_80_TCP_ADDR', '10.152.183.224')\n",
      "('JUPYTER_IMAGE_SPEC', 'ciw-gitlab-registry.intra.oceanobservatories.org:5050/kubernetes/jupyterhub_images/datascience-matlab:latest')\n",
      "('HUB_PORT_8081_TCP', 'tcp://10.152.183.83:8081')\n",
      "('_CE_CONDA', '')\n",
      "('PROXY_API_PORT', 'tcp://10.152.183.40:8001')\n",
      "('CONDA_SHLVL', '1')\n",
      "('PROXY_PUBLIC_PORT_443_TCP_PROTO', 'tcp')\n",
      "('JUPYTERHUB_OAUTH_ACCESS_SCOPES', '[\"access:servers!server=joffrey.peters@whoi.edu/\", \"access:servers!user=joffrey.peters@whoi.edu\"]')\n",
      "('PROXY_PUBLIC_SERVICE_PORT_HTTPS', '443')\n",
      "('JUPYTERHUB_COOKIE_HOST_PREFIX_ENABLED', '0')\n",
      "('PROXY_API_PORT_8001_TCP_PROTO', 'tcp')\n",
      "('SHLVL', '0')\n",
      "('CONDA_DIR', '/opt/conda')\n",
      "('KUBERNETES_PORT_443_TCP_PROTO', 'tcp')\n",
      "('JULIA_DEPOT_PATH', '/opt/julia')\n",
      "('JUPYTERHUB_API_URL', 'http://hub:8081/hub/api')\n",
      "('JUPYTERHUB_CLIENT_ID', 'jupyterhub-user-joffrey.peters%40whoi.edu')\n",
      "('PROXY_PUBLIC_PORT_80_TCP_PORT', '80')\n",
      "('JUPYTERHUB_OAUTH_SCOPES', '[\"access:servers!server=joffrey.peters@whoi.edu/\", \"access:servers!user=joffrey.peters@whoi.edu\"]')\n",
      "('KUBERNETES_PORT_443_TCP_ADDR', '10.152.183.1')\n",
      "('JUPYTERHUB_HOST', '')\n",
      "('PROXY_PUBLIC_PORT_443_TCP_ADDR', '10.152.183.224')\n",
      "('CONDA_PYTHON_EXE', '/opt/conda/bin/python')\n",
      "('JUPYTER_PORT', '8888')\n",
      "('HUB_SERVICE_PORT_HUB', '8081')\n",
      "('CONDA_DEFAULT_ENV', 'base')\n",
      "('CPU_GUARANTEE', '16.0')\n",
      "('CPU_LIMIT', '24.0')\n",
      "('NB_USER', 'jovyan')\n",
      "('KUBERNETES_SERVICE_HOST', '10.152.183.1')\n",
      "('LC_ALL', 'C.UTF-8')\n",
      "('KUBERNETES_PORT', 'tcp://10.152.183.1:443')\n",
      "('KUBERNETES_PORT_443_TCP_PORT', '443')\n",
      "('PROXY_API_PORT_8001_TCP', 'tcp://10.152.183.40:8001')\n",
      "('PROXY_PUBLIC_SERVICE_PORT', '443')\n",
      "('HUB_PORT_8081_TCP_PORT', '8081')\n",
      "('PATH', '/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin')\n",
      "('HUB_PORT_8081_TCP_ADDR', '10.152.183.83')\n",
      "('HUB_PORT_8081_TCP_PROTO', 'tcp')\n",
      "('JUPYTERHUB_USER', 'joffrey.peters@whoi.edu')\n",
      "('JUPYTERHUB_ACTIVITY_URL', 'http://hub:8081/hub/api/users/joffrey.peters@whoi.edu/activity')\n",
      "('DEBIAN_FRONTEND', 'noninteractive')\n",
      "('GIT_PYTHON_REFRESH', 'quiet')\n",
      "('JPY_SESSION_NAME', '/home/jovyan/ooi-data-explorations/python/examples/notebooks/pco2a/carbonate_climatologies.ipynb')\n",
      "('JPY_PARENT_PID', '7')\n",
      "('PYDEVD_USE_FRAME_EVAL', 'NO')\n",
      "('TERM', 'xterm-color')\n",
      "('CLICOLOR', '1')\n",
      "('FORCE_COLOR', '1')\n",
      "('CLICOLOR_FORCE', '1')\n",
      "('PAGER', 'cat')\n",
      "('GIT_PAGER', 'cat')\n",
      "('MPLBACKEND', 'module://matplotlib_inline.backend_inline')\n"
     ]
    }
   ],
   "source": [
    "for var in os.environ.items():\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f76c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology\n",
    "clm = Climatology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357c99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pco2a(site, node, sensor):\n",
    "    \"\"\"\n",
    "    Download the PCO2A water measurement data files from the OOI Gold Copy \n",
    "    THREDDS server, combine the data from the two different data delivery \n",
    "    methods, and median average the data into daily averages.\n",
    "    \"\"\"\n",
    "    # set the stream names and the regex tag to use to select the data files of interest\n",
    "    tstream = 'pco2a_a_dcl_instrument_water'\n",
    "    rstream = 'pco2a_a_dcl_instrument_water_recovered'\n",
    "    tag = '.*PCO2A.*water.*\\\\.nc$'\n",
    "\n",
    "    # download annotations associated with this site\n",
    "    annotations = get_annotations(site, node, sensor)\n",
    "\n",
    "    # download the telemetered data and re-process it to create a more useful and coherent data set\n",
    "    print('### -- Downloading the telemetered data')\n",
    "    telem = load_gc_thredds(site, node, sensor, 'telemetered', tstream, tag)\n",
    "    telem = pco2a_datalogger(telem)\n",
    "    \n",
    "    # download the recovered host data and re-process it to create a more useful and coherent data set\n",
    "    print('### -- Downloading the recovered_host data')\n",
    "    rhost = load_gc_thredds(site, node, sensor, 'recovered_host', rstream, tag)\n",
    "    rhost = pco2a_datalogger(rhost)\n",
    "\n",
    "    # create a roll-up annotation flag\n",
    "    telem = add_annotation_qc_flags(telem, annotations)\n",
    "    rhost = add_annotation_qc_flags(rhost, annotations)\n",
    "    \n",
    "    # clean-up the data, removing values that are marked as suspect or fail in the annotations\n",
    "    telem = telem.where((telem.partial_pressure_co2_ssw_annotations_qc_results < 3) & \n",
    "                        (telem.rollup_annotations_qc_results < 3))\n",
    "    rhost = rhost.where((rhost.partial_pressure_co2_ssw_annotations_qc_results < 3) & \n",
    "                        (rhost.rollup_annotations_qc_results < 3))\n",
    "\n",
    "    # combine the two datasets into a single, merged time series resampled to daily median averages\n",
    "    merged = combine_datasets(telem, rhost, None, 1440)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1da13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_phsen(site, node, sensor):\n",
    "    \"\"\"\n",
    "    Download the PHSEN data files from the OOI Gold Copy THREDDS server, combine \n",
    "    the data from the three different data delivery methods, and median average \n",
    "    the data into daily averages.\n",
    "    \"\"\"\n",
    "    # download annotations associated with this site\n",
    "    annotations = get_annotations(site, node, sensor)\n",
    "\n",
    "    tag = '.*PHSEN.*\\\\.nc$'\n",
    "    print('### -- Downloading the telemetered data')\n",
    "    telem = load_gc_thredds(site, node, sensor, 'telemetered', 'phsen_abcdef_dcl_instrument', tag)\n",
    "    telem = phsen_datalogger(telem)\n",
    "    \n",
    "    # download the recovered host data and re-process it to create a more useful and coherent data set\n",
    "    print('### -- Downloading the recovered_host data')\n",
    "    rhost = load_gc_thredds(site, node, sensor, 'recovered_host', 'phsen_abcdef_dcl_instrument_recovered', tag)\n",
    "    rhost = phsen_datalogger(rhost)\n",
    "\n",
    "    # download the recovered instrument data and re-process it to create a more useful and coherent data set\n",
    "    print('### -- Downloading the recovered_inst data')\n",
    "    rinst = load_gc_thredds(site, node, sensor, 'recovered_inst', 'phsen_abcdef_instrument', tag)\n",
    "    rinst = phsen_instrument(rinst)\n",
    "    \n",
    "    # create a roll-up annotation flag\n",
    "    telem = add_annotation_qc_flags(telem, annotations)\n",
    "    rhost = add_annotation_qc_flags(rhost, annotations)\n",
    "    rinst = add_annotation_qc_flags(rinst, annotations)\n",
    "\n",
    "    # clean-up the data, removing values that fail the pH quality checks or were marked as fail in the annotations\n",
    "    telem = telem.where((telem.seawater_ph_quality_flag != 4) & (telem.rollup_annotations_qc_results != 4))\n",
    "    rhost = rhost.where((rhost.seawater_ph_quality_flag != 4) & (rhost.rollup_annotations_qc_results != 4))\n",
    "    rinst = rinst.where((rinst.seawater_ph_quality_flag != 4) & (rinst.rollup_annotations_qc_results != 4))\n",
    "    \n",
    "    # combine the three datasets into a single, merged time series resampled to daily median averages\n",
    "    merged = combine_datasets(telem, rhost, rinst, 1440)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4e674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Downloading the PCO2A data from CE02SHSM ###\n",
      "### -- Downloading the telemetered data\n",
      "Downloading 803 data file(s) from the OOI Gold Copy THREDSS catalog\n",
      "Downloading and Processing the Data Files: 100%|██████████| 803/803 [00:56<00:00, 14.20it/s]\n",
      "Merging the data files into a single dataset\n",
      "### -- Downloading the recovered_host data\n",
      "Downloading 23 data file(s) from the OOI Gold Copy THREDSS catalog\n",
      "Downloading and Processing the Data Files: 100%|██████████| 23/23 [00:06<00:00,  3.64it/s]\n",
      "Merging the data files into a single dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "failed to prevent overwriting existing key units in attrs on variable 'time'. This is probably an encoding field used by xarray to describe how a variable is serialized. To proceed, remove this key from the variable's attributes manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m### Downloading the PCO2A data from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ###\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m site)\n\u001b[1;32m     24\u001b[0m pco2a\u001b[38;5;241m.\u001b[39mappend(merge_pco2a(site, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSBD12\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m04-PCO2AA000\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m \u001b[43mpco2a\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpco2a_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNETCDF4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh5netcdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# pause, the THREDDS server doesn't like getting hammered\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m### Downloading the PHSEN data from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ###\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m site)\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/core/dataset.py:2372\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   2373\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/backends/api.py:1873\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1871\u001b[0m     \u001b[38;5;66;03m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1872\u001b[0m     \u001b[38;5;66;03m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1873\u001b[0m     \u001b[43mdump_to_store\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\n\u001b[1;32m   1875\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoclose:\n\u001b[1;32m   1877\u001b[0m         store\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/backends/api.py:1920\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder:\n\u001b[1;32m   1918\u001b[0m     variables, attrs \u001b[38;5;241m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1920\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/backends/common.py:447\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     writer \u001b[38;5;241m=\u001b[39m ArrayWriter()\n\u001b[0;32m--> 447\u001b[0m variables, attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[38;5;241m=\u001b[39munlimited_dims)\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/backends/common.py:536\u001b[0m, in \u001b[0;36mWritableCFDataStore.encode\u001b[0;34m(self, variables, attributes)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, variables, attributes):\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# All NetCDF files get CF encoded by default, without this attempting\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# to write times, for example, would fail.\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m     variables, attributes \u001b[38;5;241m=\u001b[39m \u001b[43mcf_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     variables \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_variable(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    538\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_attribute(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m attributes\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/conventions.py:829\u001b[0m, in \u001b[0;36mcf_encoder\u001b[0;34m(variables, attributes)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# add encoding for time bounds variables if present.\u001b[39;00m\n\u001b[1;32m    827\u001b[0m _update_bounds_encoding(variables)\n\u001b[0;32m--> 829\u001b[0m new_vars \u001b[38;5;241m=\u001b[39m {k: \u001b[43mencode_cf_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# Remove attrs from bounds variables (issue #2921)\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m new_vars\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/conventions.py:197\u001b[0m, in \u001b[0;36mencode_cf_variable\u001b[0;34m(var, needs_copy, name)\u001b[0m\n\u001b[1;32m    185\u001b[0m ensure_not_multiindex(var, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coder \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    188\u001b[0m     times\u001b[38;5;241m.\u001b[39mCFDatetimeCoder(),\n\u001b[1;32m    189\u001b[0m     times\u001b[38;5;241m.\u001b[39mCFTimedeltaCoder(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     variables\u001b[38;5;241m.\u001b[39mBooleanCoder(),\n\u001b[1;32m    196\u001b[0m ]:\n\u001b[0;32m--> 197\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[43mcoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# TODO(kmuehlbauer): check if ensure_dtype_not_object can be moved to backends:\u001b[39;00m\n\u001b[1;32m    200\u001b[0m var \u001b[38;5;241m=\u001b[39m ensure_dtype_not_object(var, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/coding/times.py:991\u001b[0m, in \u001b[0;36mCFDatetimeCoder.encode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    988\u001b[0m dtype \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    989\u001b[0m (data, units, calendar) \u001b[38;5;241m=\u001b[39m encode_cf_datetime(data, units, calendar, dtype)\n\u001b[0;32m--> 991\u001b[0m \u001b[43msafe_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m safe_setitem(attrs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalendar\u001b[39m\u001b[38;5;124m\"\u001b[39m, calendar, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Variable(dims, data, attrs, encoding, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/envs/ooi-data-explorations/lib/python3.12/site-packages/xarray/coding/variables.py:198\u001b[0m, in \u001b[0;36msafe_setitem\u001b[0;34m(dest, key, value, name)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dest:\n\u001b[1;32m    197\u001b[0m     var_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to prevent overwriting existing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in attrs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is probably an encoding field used by xarray to describe \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow a variable is serialized. To proceed, remove this key from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe variable\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms attributes manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m dest[key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mValueError\u001b[0m: failed to prevent overwriting existing key units in attrs on variable 'time'. This is probably an encoding field used by xarray to describe how a variable is serialized. To proceed, remove this key from the variable's attributes manually."
     ]
    }
   ],
   "source": [
    "# download the pco2a and phsen data from the 4 Endurance array Coastal Surface Moorings, \n",
    "# merging the data from the different data delivery methods and saving the results to disk\n",
    "# for further analysis.\n",
    "sites = ['CE02SHSM', 'CE04OSSM', 'CE07SHSM', 'CE09OSSM']\n",
    "pco2a = []\n",
    "phsen = []\n",
    "for num, site in enumerate(sites):\n",
    "    # save the PCO2A data to local files in the home directory under ooidata\n",
    "    pco2_path = os.path.join(os.path.expanduser('~'), 'ooidata/%s/buoy/pco2a' % site.lower())\n",
    "    if not os.path.exists(pco2_path):\n",
    "        os.makedirs(pco2_path)\n",
    "    \n",
    "    pco2a_file = os.path.join(pco2_path, '%s.pco2a.merged.nc' % site.lower())\n",
    "\n",
    "    # save the PHSEN data to local files in the home directory under ooidata\n",
    "    ph_path = os.path.join(os.path.expanduser('~'), 'ooidata/%s/nsif/phsen' % site.lower())\n",
    "    if not os.path.exists(ph_path):\n",
    "        os.makedirs(ph_path)\n",
    "\n",
    "    phsen_file = os.path.join(ph_path, '%s.phsen.merged.nc' % site.lower())\n",
    "\n",
    "    # download the data, or ...\n",
    "    print('### Downloading the PCO2A data from %s ###' % site)\n",
    "    pco2a.append(merge_pco2a(site, 'SBD12', '04-PCO2AA000'))\n",
    "    # Because of a recent change in `combine_datasets()`, it no longer works as expected with xarray's `to_netcdf()`\n",
    "    # Now, we have to remove units and calendar attributes for time variable, which xarray populates itself\n",
    "    pco2a[num].time.attrs.pop(\"units\")\n",
    "    pco2a[num].time.attrs.pop(\"calendar\")\n",
    "    pco2a[num].to_netcdf(pco2a_file, mode='w', format='NETCDF4', engine='h5netcdf')\n",
    "    time.sleep(10)  # pause, the THREDDS server doesn't like getting hammered\n",
    "    \n",
    "    print('### Downloading the PHSEN data from %s ###' % site)\n",
    "    phsen.append(merge_phsen(site, 'RID26', '06-PHSEND000'))\n",
    "    # Because of a recent change in `combine_datasets()`, it no longer works as expected with xarray's `to_netcdf()`\n",
    "    # Now, we have to remove units and calendar attributes for time variable, which xarray populates itself\n",
    "    phsen[num].time.attrs.pop(\"units\")\n",
    "    phsen[num].time.attrs.pop(\"calendar\")\n",
    "    phsen[num].to_netcdf(phsen_file, mode='w', format='NETCDF4', engine='h5netcdf')\n",
    "    time.sleep(10)  # pause, the THREDDS server doesn't like getting hammered\n",
    "\n",
    "    # ... load the already downloaded data\n",
    "    # pco2a.append(xr.load_dataset(pco2a_file))\n",
    "    # phsen.append(xr.load_dataset(phsen_file))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582834bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup arrays with the fugacity and pH data from Fassbender et al 2018\n",
    "buoy_fco2 = np.array([\n",
    "    [1, 341, 24, 398, 368, 29, 1485],\n",
    "    [2, 358, 30, 221, 343, 38, 1347],\n",
    "    [3, 415, 74, 245, 322, 34, 1295],\n",
    "    [4, 245, 41, 279, 277, 36, 1199],\n",
    "    [5, 204, 33, 484, 279, 41, 1158],\n",
    "    [6, 212, 48, 1080, 260, 51, 1244],\n",
    "    [7, 258, 68, 1461, 281, 57, 1715],\n",
    "    [8, 283, 58, 1648, 276, 52, 1812],\n",
    "    [9, 325, 45, 1346, 316, 51, 1619],\n",
    "    [10, 342, 25, 1028, 330, 47, 1753],\n",
    "    [11, 358, 40, 645, 364, 34, 1506],\n",
    "    [12, 349, 26, 495, 371, 21, 1484]\n",
    "])\n",
    "\n",
    "region_fco2 = np.array([\n",
    "    [1, 358, 57, 373, 120, 361, 163],\n",
    "    [2, 385, 65, 341, 19, 363, 446],\n",
    "    [3, 356, 170, 311, 98, 343, 250],\n",
    "    [4, 264, 7, 277, 23, 320, 66],\n",
    "    [5, 337, 34, 287, 135, 320, 364],\n",
    "    [6, 288, 1700, 287, 3456, 322, 2791],\n",
    "    [7, 282, 6969, 257, 1545, 340, 4026],\n",
    "    [8, 275, 6301, 275, 3614, 330, 1876],\n",
    "    [9, 285, 750, 323, 719, 344, 504],\n",
    "    [10, 359, 5, 350, 24, 344, 88],\n",
    "    [11, 351, 13, 340, 46, 349, 109],\n",
    "    [12, np.nan, 0, np.nan, 0, 357, 29],\n",
    "])\n",
    "\n",
    "buoy_ph = np.array([\n",
    "    [1, 8.07, 0.03, 398, 8.06, 0.03, 1485],\n",
    "    [2, 8.08, 0.03, 222, 8.08, 0.04, 1347],\n",
    "    [3, 8.02, 0.06, 248, 8.11, 0.04, 1295],\n",
    "    [4, 8.22, 0.06, 279, 8.17, 0.05, 1199],\n",
    "    [5, 8.27, 0.06, 495, 8.17, 0.05, 1158],\n",
    "    [6, 8.26, 0.09, 1117, 8.2, 0.07, 1244],\n",
    "    [7, 8.21, 0.09, 1461, 8.17, 0.07, 1715],\n",
    "    [8, 8.17, 0.08, 1648, 8.18, 0.07, 1812],\n",
    "    [9, 8.11, 0.07, 1547, 8.13, 0.06, 1619],\n",
    "    [10, 8.09, 0.03, 1172, 8.11, 0.06, 1753],\n",
    "    [11, 8.06, 0.05, 646, 8.07, 0.04, 1506],\n",
    "    [12, 8.07, 0.03, 496, 8.06, 0.02, 1484],  \n",
    "])\n",
    "\n",
    "region_ph = np.array([\n",
    "    [1, 8.07, 57, 8.06, 120, 8.07, 163],\n",
    "    [2, 8.05, 65, 8.09, 19, 8.07, 446],\n",
    "    [3, 8.07, 170, 8.12, 98, 8.09, 250],\n",
    "    [4, 8.19, 7, 8.16, 23, 8.12, 66],\n",
    "    [5, 8.1, 34, 8.14, 135, 8.12, 358],\n",
    "    [6, 8.15, 1700, 8.15, 3456, 8.11, 2791],\n",
    "    [7, 8.16, 6969, 8.2, 1545, 8.09, 4024],\n",
    "    [8, 8.14, 5585, 8.16, 3059, 8.1, 1770],\n",
    "    [9, 8.16, 749, 8.1, 719, 8.09, 504],\n",
    "    [10, 8.07, 5, 8.08, 24, 8.09, 88],\n",
    "    [11, 8.08, 13, 8.09, 46, 8.08, 109],\n",
    "    [12, np.nan, 0, np.nan, 0, 8.07, 29],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the climatological model and setup defaults for plotting the pCO2 and pH data\n",
    "clm = Climatology()\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot the PCO2A data alongside Fassbender et al 2018 ######\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(17, 11)\n",
    "\n",
    "# calculate the monthly climatologies for the Washington Offshore mooring and group the data monthly \n",
    "num = 3\n",
    "clm.fit(pco2a[num]['partial_pressure_co2_ssw'])\n",
    "r2 = r'2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_pco2 = pco2a[num].resample(time='M').mean()\n",
    "grps_pco2 = mnthly_pco2.groupby('time.month')\n",
    "\n",
    "# Plot the Washington Offshore data\n",
    "for grp in grps_pco2:\n",
    "    axs[0, 0].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['partial_pressure_co2_ssw'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[0, 0].plot(buoy_fco2[:, 0], buoy_fco2[:, 4], '-k', label='Cape Elizabeth')\n",
    "axs[0, 0].plot(buoy_fco2[:, 0], buoy_fco2[:, 4] - buoy_fco2[:, 5], ':k', \n",
    "               buoy_fco2[:, 0], buoy_fco2[:, 4] + buoy_fco2[:, 5], ':k')\n",
    "axs[0, 0].plot(region_fco2[:, 0], region_fco2[:, 3], '-', color='royalblue', label='Outer Coast')\n",
    "axs[0, 0].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[0, 0].legend(prop={'size': 12}, loc=1)\n",
    "axs[0, 0].set_title('Washington Offshore')\n",
    "\n",
    "# calculate the monthly climatologies for the Washington Shelf mooring and group the data monthly \n",
    "num = 2\n",
    "clm.fit(pco2a[num]['partial_pressure_co2_ssw'])\n",
    "r2 = '2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_pco2 = pco2a[num].resample(time='M').mean()\n",
    "grps_pco2 = mnthly_pco2.groupby('time.month')\n",
    "\n",
    "# Plot the Washington Shelf data\n",
    "for grp in grps_pco2:\n",
    "    axs[0, 1].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['partial_pressure_co2_ssw'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[0, 1].plot(buoy_fco2[:, 0], buoy_fco2[:, 4], '-k', label='Cape Elizabeth')\n",
    "axs[0, 1].plot(buoy_fco2[:, 0], buoy_fco2[:, 4] - buoy_fco2[:, 5], ':k', \n",
    "               buoy_fco2[:, 0], buoy_fco2[:, 4] + buoy_fco2[:, 5], ':k')\n",
    "axs[0, 1].plot(region_fco2[:, 0], region_fco2[:, 3], '-', color='royalblue', label='Outer Coast')\n",
    "axs[0, 1].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[0, 1].legend(prop={'size': 12}, loc=1)\n",
    "axs[0, 1].set_title('Washington Shelf')\n",
    "\n",
    "# calculate the monthly climatologies for the Oregon Offshore mooring and group the data monthly \n",
    "num = 1\n",
    "clm.fit(pco2a[num]['partial_pressure_co2_ssw'])\n",
    "r2 = '2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_pco2 = pco2a[num].resample(time='M').mean()\n",
    "grps_pco2 = mnthly_pco2.groupby('time.month')\n",
    "\n",
    "# Plot the Oregon Offshore data\n",
    "for grp in grps_pco2:\n",
    "    axs[1, 0].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['partial_pressure_co2_ssw'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[1, 0].plot(region_fco2[:, 0], region_fco2[:, 5], '-', color='royalblue', label=r'North Pacific')\n",
    "axs[1, 0].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[1, 0].legend(prop={'size': 12}, loc=1)\n",
    "axs[1, 0].set_title('Oregon Offshore')\n",
    "\n",
    "# calculate the monthly climatologies for the Oregon Shelf mooring and group the data monthly \n",
    "num = 0\n",
    "clm.fit(pco2a[num]['partial_pressure_co2_ssw'])\n",
    "r2 = '2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_pco2 = pco2a[num].resample(time='M').mean()\n",
    "grps_pco2 = mnthly_pco2.groupby('time.month')\n",
    "\n",
    "# Plot the Oregon Shelf data\n",
    "for grp in grps_pco2:\n",
    "    axs[1, 1].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['partial_pressure_co2_ssw'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[1, 1].plot(region_fco2[:, 0], region_fco2[:, 5], '-', color='royalblue', label=r'North Pacific')\n",
    "axs[1, 1].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[1, 1].legend(prop={'size': 12}, loc=1)\n",
    "axs[1, 1].set_title('Oregon Shelf')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel=r'pCO$^2$ ($\\mu$atm)')\n",
    "    ax.set_xticks(np.arange(1, 13))\n",
    "    ax.set_xticklabels(months)\n",
    "    ax.set_xlim((0.5, 12.5))\n",
    "    ax.set_ylim((150, 500))\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "plt.show()\n",
    "fig.savefig(os.path.join('regional_pco2_comparisons.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c224b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Plot the PHSEN data alongside Fassbender et al 2018 ######\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(17, 11)\n",
    "\n",
    "# calculate the monthly climatologies for the Washington Offshore mooring and group the data monthly \n",
    "num = 3\n",
    "clm.fit(phsen[num]['seawater_ph'])\n",
    "r2 = r'2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_ph = phsen[num].resample(time='M').mean()\n",
    "grps_ph = mnthly_ph.groupby('time.month')\n",
    "\n",
    "# Plot the Washington Offshore data\n",
    "for grp in grps_ph:\n",
    "    axs[0, 0].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['seawater_ph'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[0, 0].plot(buoy_ph[:, 0], buoy_ph[:, 4], '-k', label='Cape Elizabeth')\n",
    "axs[0, 0].plot(buoy_ph[:, 0], buoy_ph[:, 4] - buoy_ph[:, 5], ':k', \n",
    "               buoy_ph[:, 0], buoy_ph[:, 4] + buoy_ph[:, 5], ':k')\n",
    "axs[0, 0].plot(region_ph[:, 0], region_ph[:, 3], '-', color='royalblue', label='Outer Coast')\n",
    "axs[0, 0].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[0, 0].legend(prop={'size': 12}, loc=1)\n",
    "axs[0, 0].set_title('Washington Offshore')\n",
    "\n",
    "# calculate the monthly climatologies for the Washington Shelf mooring and group the data monthly \n",
    "num = 2\n",
    "clm.fit(phsen[num]['seawater_ph'])\n",
    "r2 = '2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_ph = phsen[num].resample(time='M').mean()\n",
    "grps_ph = mnthly_ph.groupby('time.month')\n",
    "\n",
    "# Plot the Washington Shelf data\n",
    "for grp in grps_ph:\n",
    "    axs[0, 1].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['seawater_ph'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[0, 1].plot(buoy_ph[:, 0], buoy_ph[:, 4], '-k', label='Cape Elizabeth')\n",
    "axs[0, 1].plot(buoy_ph[:, 0], buoy_ph[:, 4] - buoy_ph[:, 5], ':k', \n",
    "               buoy_ph[:, 0], buoy_ph[:, 4] + buoy_ph[:, 5], ':k')\n",
    "axs[0, 1].plot(region_ph[:, 0], region_ph[:, 3], '-', color='royalblue', label='Outer Coast')\n",
    "axs[0, 1].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[0, 1].legend(prop={'size': 12}, loc=1)\n",
    "axs[0, 1].set_title('Washington Shelf')\n",
    "\n",
    "# calculate the monthly climatologies for the Oregon Offshore mooring and group the data monthly \n",
    "num = 1\n",
    "clm.fit(phsen[num]['seawater_ph'])\n",
    "r2 = '2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_ph = phsen[num].resample(time='M').mean()\n",
    "grps_ph = mnthly_ph.groupby('time.month')\n",
    "\n",
    "# Plot the Oregon Offshore data\n",
    "for grp in grps_ph:\n",
    "    axs[1, 0].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['seawater_ph'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[1, 0].plot(region_ph[:, 0], region_ph[:, 5], '-', color='royalblue', label=r'North Pacific')\n",
    "axs[1, 0].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[1, 0].legend(prop={'size': 12}, loc=1)\n",
    "axs[1, 0].set_title('Oregon Offshore')\n",
    "\n",
    "# calculate the monthly climatologies for the Oregon Shelf mooring and group the data monthly \n",
    "num = 0\n",
    "clm.fit(phsen[num]['seawater_ph'])\n",
    "r2 = '2-Cycle: R$^2$ = %.3f' % clm.regression['variance_explained'][0]\n",
    "mnthly_ph = phsen[num].resample(time='M').mean()\n",
    "grps_ph = mnthly_ph.groupby('time.month')\n",
    "\n",
    "# Plot the Oregon Shelf data\n",
    "for grp in grps_ph:\n",
    "    axs[1, 1].plot(grp[1].time.to_index().strftime('%m').astype(int), grp[1]['seawater_ph'],\n",
    "                   'o', color='lightgrey')\n",
    "axs[1, 1].plot(region_ph[:, 0], region_ph[:, 5], '-', color='royalblue', label=r'North Pacific')\n",
    "axs[1, 1].plot(clm.monthly_fit.index, clm.monthly_fit.values, '-o', color='darkorange', label=r2)\n",
    "axs[1, 1].legend(prop={'size': 12}, loc=1)\n",
    "axs[1, 1].set_title('Oregon Shelf')\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='pH')\n",
    "    ax.set_xticks(np.arange(1, 13))\n",
    "    ax.set_xticklabels(months)\n",
    "    ax.set_xlim((0.5, 12.5))\n",
    "    ax.set_ylim((7.9, 8.4))\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "plt.show()\n",
    "fig.savefig('regional_ph_comparisons.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ooi",
   "language": "python",
   "name": "ooi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
