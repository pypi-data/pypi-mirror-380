{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cfadf9",
   "metadata": {},
   "source": [
    "# METBK\n",
    "\n",
    "### Purpose\n",
    "The purpose of this notebook is to calculate the gross range user min/max values for populating QARTOD parameter tables for data streams for OOI - CGSN data streams for the ASIMET bulk-meteorology measurements (METBK). The METBK package measures a collection of atmospheric parameters: wind speed (u-, v-vectors), air temperature, barometric pressure, short- and longwave irradiance, relative humidity, and precipitation. It also measures the sea surface salinity and temperature. There are number of base-and-derived data products delivered for the METBK: we test only the base measurements, and ignore the hourly-averaged as well as derived flux products. The METBK is deployed only on surface moorings.\n",
    "  \n",
    "### Test Parameters\n",
    "\n",
    "| Variable | Sensor | Range | Resolution | Accuracy |\n",
    "|----------|--------|-------|------------|----------|\n",
    "| met_windavg_mag_corr_east | Gill Instruments WindObserver II Anemometer | 0 - 65 m/s, 0 - 360 deg | 0.01 m/s | 2%, 2 deg |\n",
    "| met_windavg_mag_corr_north| Gill Instruments WindObserver II Anemometer | 0 - 65 m/s, 0 - 360 deg | 0.01 m/s | 2%, 2 deg |\n",
    "| air_temperature | Rotronic MP-101A | -40C - 60C | 0.02C | 0.05C |\n",
    "| relative_humidity | Rotronic MP-101A | 0 - 100%RH | 0.01%RH | 1%RH |\n",
    "| barometric_pressure | Heise DXD (Dresser Instruments) | 850 - 1050 mb | 0.01 mb | 0.2 mb |\n",
    "| longwave_irradiance | Eppley Precision INfrared Radiometer (PIR) | 0 - 700 W/m2 | 0.1 W/m2 | 2 W/m2 |\n",
    "| shortwave_irradiance| Eppley Precision Spectral Pyranometer (PSP)| 0 - 2800 W/m2 | 0.1 W/m2 | 2 W/m2 |\n",
    "| precipitation | RM Young 50202 Self-siphoning rain gauge | 0 - 500 mm | 0.1 mm | 1 mm/hr |\n",
    "| sea_surface_conductivity | SBE 37 | 0 - 7 S/m | 0.00001 S/m | 0.0003 S/m |\n",
    "| met_salsurf | SBE 37 | 0 - 42 psu | - | - |\n",
    "| sea_surface_temperature | SBE 37 | -5 - 35C | 0.0001C | 0.002C (-5 - 35C) |\n",
    "\n",
    "**Special Case: Wind Data**\n",
    "\n",
    "For Wind Data, from the QARTOD Wind Manual: \n",
    "```\n",
    "Wind speed (WS) is used in the descriptions and examples, but the tests apply equally to direction\n",
    "and gust in most cases. A discontinuity in wind direction is caused when the wind veers through north,\n",
    "stepping from 359° to 0° and complicating the application of some of these tests. Operators may choose to\n",
    "conduct wind direction tests on the u and v wind direction components to circumvent the problem.\n",
    "```\n",
    "* ```eastward_wind_velocity```: windspeed in the eastward direction, for the METBK instrument, not corrected for magnetic declination\n",
    "* ```northward_wind_velocity```: windspeed in the northward direction, for the METBK instrument, not corrected for magnetic declination\n",
    "\n",
    "For the Gill Instruments WindObserver II 2-axis sonic anemometer operates on the following prinicple of operation: \n",
    "```\n",
    "The WindObserver II measures the times taken for an ultrasonic pulse of sound to travel\n",
    "from the North transducer to the South transducer, and compares it with the time for a\n",
    "pulse to travel from S to N transducer. Likewise times are compared between West and\n",
    "East, and E and W transducer.\n",
    "If, for example, a North wind is blowing, then the time taken for the pulse to travel from N\n",
    "to S will be faster than from S to N, whereas the W to E, and E to W times will be the\n",
    "same. The wind speed and direction (and the speed of sound) can then be calculated from\n",
    "the differences in the times of flight on each axis. This calculation is independent of\n",
    "factors such as temperature.\n",
    "```\n",
    "This suggests that the eastward and northward wind velocities should be the tested parameters. Additionally, the instrument used to measure wind speed (the Gill WindObserver II) has a range of 0-65 m/s, which is taken as the absolute maximum. Consequently, that means when the wind is blowing at a 45$^{\\circ}$ angle to the east and north instrument sensors that the maximum individual component speeds are $max(u, v) = \\sqrt{(ws^{2})/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666742bd",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d43f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime, pytz, re\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cea47",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox\n",
    "This toolbox is publicly available at https://github.com/reedan88/OOINet. It should be cloned onto your machine and the setup instructions followed before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OOINet M2M tool\n",
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M\n",
    "from ooinet.utils import convert_time, ntp_seconds_to_datetime, unix_epoch_time\n",
    "from ooinet.Instrument.common import process_file, add_annotation_qc_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2047e42",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox\n",
    "This toolbox is publicly available at https://github.com/oceanobservatories/ooi-data-explorations. Similarly to the ```ooinet``` toolbox above, it should be installed onto your machine following the setup instructions before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.common import get_annotations, get_vocabulary, load_gc_thredds\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_metbk import metbk_datalogger\n",
    "from ooi_data_explorations.qartod.qc_processing import identify_blocks, create_annotations, process_gross_range, \\\n",
    "    process_climatology, parse_qc, inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e9f7",
   "metadata": {},
   "source": [
    "#### Import plotting and visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67501483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ecadd",
   "metadata": {},
   "source": [
    "---\n",
    "## Identify Data Streams\n",
    "This section is necessary to identify all of the data stream associated with a specific instrument. This can be done by querying UFrame and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time.\n",
    "\n",
    "First, set the instrument to search for using OOI terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = \"METBK\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6eedd",
   "metadata": {},
   "source": [
    "### Query OOINet for Data Streams <br>\n",
    "First check if the datasets have already been downloaded; if not, use the ```M2M.search_datasets``` tool to search the OOINet API and return a table of all of the available datasets for the given instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b295fa0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M2M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/METBK_datasets.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/METBK_datasets.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mM2M\u001b[49m\u001b[38;5;241m.\u001b[39msearch_datasets(instrument\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETBK\u001b[39m\u001b[38;5;124m\"\u001b[39m, English_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Save the datasets\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/METBK_datasets.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M2M' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    datasets = pd.read_csv(\"../data/METBK_datasets.csv\")\n",
    "except:\n",
    "    datasets = M2M.search_datasets(instrument=\"METBK\", English_names=True)\n",
    "    # Save the datasets\n",
    "    datasets.to_csv(\"../data/METBK_datasets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de8866",
   "metadata": {},
   "source": [
    "Separate out the CGSN datasets from the EA datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgsn = datasets[\"array\"].apply(lambda x: True if x.startswith((\"CP\",\"GA\",\"GI\",\"GP\",\"GS\")) else False)\n",
    "datasets = datasets[cgsn]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0bac",
   "metadata": {},
   "source": [
    "---\n",
    "## Single Reference Designator\n",
    "The reference designator acts as a key for an instrument located at a specific location. First, select a reference designator (refdes) to request data from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_designators = sorted(datasets[\"refdes\"])\n",
    "print(\"Number of reference designators: \" + str(len(reference_designators)))\n",
    "for refdes in reference_designators:\n",
    "    print(refdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb66114",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "refdes = reference_designators[k]\n",
    "print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa03f4",
   "metadata": {},
   "source": [
    "#### Sensor Vocab\n",
    "The vocab provides information about the instrument model and type, its location (with descriptive names), depth, and manufacturer. Get the vocab for the given reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4386f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f248210",
   "metadata": {},
   "source": [
    "#### Sensor Deployments\n",
    "Download the deployment information for the selected reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf043193",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d996fb",
   "metadata": {},
   "source": [
    "#### Sensor Data Streams\n",
    "Next, select the specific data streams for the given reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51398c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6c3ad",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata \n",
    "The metadata contains the following important key pieces of data for each reference designator: **method**, **stream**, **particleKey**, and **count**. The method and stream are necessary for identifying and loading the relevant dataset. The particleKey tells us which data variables in the dataset we should be calculating the QARTOD parameters for. The count lets us know which dataset (the recovered instrument, recovered host, or telemetered) contains the most data and likely has the best record to use to calculate the QARTOD tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a4e3e",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters for calculating the relevant QARTOD test limits. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we through several steps to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    science_vars = filter_science_parameters(metadata)\n",
    "    # Next, eliminate the optode temperature from the stream\n",
    "    mask = science_vars[\"particleKey\"].apply(lambda x: False if \"temp\" in x else True)\n",
    "    science_vars = science_vars[mask]\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    science_vars = science_vars.explode(column=\"particleKey\")\n",
    "    return science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12987f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vars = filter_science_parameters(metadata)\n",
    "science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "science_vars = science_vars.reset_index()\n",
    "science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16045e17",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data\n",
    "When calculating the QARTOD data tables, we only want to utilize the most complete data record available for a given reference designator. We do this by getting all the available data streams, loading the data, and then combining them into a single dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_overlaps(ds, deployments):\n",
    "    \"\"\"Trim overlapping deployment data (necessary to use xr.open_mfdataset)\"\"\"\n",
    "    # --------------------------------\n",
    "    # Second, get the deployment times\n",
    "    deployments = deployments.sort_values(by=\"deploymentNumber\")\n",
    "    deployments = deployments.set_index(keys=\"deploymentNumber\")\n",
    "    # Shift the start times by (-1) \n",
    "    deployEnd = deployments[\"deployStart\"].shift(-1)\n",
    "    # Find where the deployEnd times are earlier than the deployStart times\n",
    "    mask = deployments[\"deployEnd\"] > deployEnd\n",
    "    # Wherever the deployEnd times occur after the shifted deployStart times, replace those deployEnd times\n",
    "    deployments[\"deployEnd\"][mask] = deployEnd[mask]\n",
    "    deployments[\"deployEnd\"] = deployments[\"deployEnd\"].apply(lambda x: pd.to_datetime(x))\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # With the deployments info, can write a preprocess function to filter \n",
    "    # the data based on the deployment number\n",
    "    depNum = np.unique(ds[\"deployment\"])\n",
    "    deployInfo = deployments.loc[depNum]\n",
    "    deployStart = deployInfo[\"deployStart\"].values[0]\n",
    "    deployEnd = deployInfo[\"deployEnd\"].values[0]\n",
    "    \n",
    "    # Select the dataset data which falls within the specified time range\n",
    "    ds = ds.sel(time=slice(deployStart, deployEnd))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def preprocess(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = metbk_datalogger(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9060d74",
   "metadata": {},
   "source": [
    "**Filter out the \"hourly\" datastreams; use only the regular dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c823c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = datastreams[\"stream\"].apply(lambda x: False if \"hourly\" in x else True)\n",
    "datastreams[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b25bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data\n",
    "To access data, there are two applicable methods. The first is to download the data and save the netCDF files locally. The second is to access and process the files remotely on the THREDDS server, without having to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0540c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available datasets\n",
    "for index in datastreams[mask].index:\n",
    "    # Get the method and stream\n",
    "    method = datastreams.loc[index][\"method\"]\n",
    "    stream = datastreams.loc[index][\"stream\"]\n",
    "\n",
    "    # Get the URL - first try the goldCopy thredds server\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "    # Clean the catalog\n",
    "    catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Get the links to the THREDDs server and load the data\n",
    "    dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "    if method == \"telemetered\":\n",
    "        tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess, parallel=True)\n",
    "    elif method == \"recovered_host\":\n",
    "        host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            host_data = xr.open_mfdataset(host_files, preprocess=preprocess, parallel=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55acc9a",
   "metadata": {},
   "source": [
    "**Combine the datasets into a single dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_datasets(tele_data, host_data, None, None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b44536",
   "metadata": {},
   "source": [
    "**Clean up workspace variables and free up memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4399d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data.close()\n",
    "tele_data.close()\n",
    "del tele_data, host_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9586fac",
   "metadata": {},
   "source": [
    "---\n",
    "## Process the DOSTA\n",
    "The METBK data needs reprocessing before we can generate the qartod tables. This includes removing data identified by annotations as bad and filtering out data that, based on HITL review, is outside of the expected instrument calibration ranges. Once these steps are done, then the cleaned up datasets can be used to generate more-accurate QARTOD tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1df6d",
   "metadata": {},
   "source": [
    "#### Process existing qc tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_qc(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in data.variables:\n",
    "    if \"qc_summary\" in var:\n",
    "        if \"shortwave_irradiance\" in var:\n",
    "            continue\n",
    "        # Get variable name\n",
    "        var_name = var.split(\"_qc\")[0]\n",
    "        # Now nan-out the appropriate \n",
    "        mask = (data[var] >= 3)\n",
    "        data[var_name][mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630bcf8",
   "metadata": {},
   "source": [
    "#### Annotations\n",
    "The annotations associated with a specific reference designator may contain relevant information on the performance or reliability of the data for a given dataset. The annotations are downloaded from OOINet as a json and processed into a pandas dataframe. Each annotation may apply to the entire dataset, to a specific stream, or to a specific variable. With the downloaed annotations, we can use the information contained in the ```qcFlag``` column to translate the annotations into QC flags, which can then be used to filter out bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = M2M.get_annotations(refdes)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a0b36",
   "metadata": {},
   "source": [
    "Pass in the annotations and the dataset to add the annotation ```qcFlag``` values to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_annotation_qc_flag(data, annotations)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa1ba5",
   "metadata": {},
   "source": [
    "Use the added ```rollup_annotations_qc_results``` values to filter out bad or suspect (```rollup_annotations_qc_results``` value of 3, 4, or 9) data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7facbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.where(data.rollup_annotations_qc_results <= 3, drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf621caa",
   "metadata": {},
   "source": [
    "#### Limit the data to data collected before 2021-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a675547",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = np.unique(data['time'], return_index=True)\n",
    "data = data.isel(time=index)\n",
    "data = data.sel(time=slice('2014-01-01T00:00:00', \"2021-01-01T00:00:00\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cceb69f",
   "metadata": {},
   "source": [
    "#### Specific filters for the METBK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data[\"sea_surface_conductivity\"] < 3.0\n",
    "data[\"sea_surface_conductivity\"][mask] = np.nan\n",
    "data[\"sea_surface_salinity\"][mask] = np.nan\n",
    "\n",
    "mask = (data[\"sea_surface_temperature\"] < 0) | (data[\"sea_surface_temperature\"] > 20)\n",
    "data[\"sea_surface_temperature\"][mask] = np.nan\n",
    "\n",
    "#mask = (data[\"air_temperature\"] > 20) \n",
    "#data[\"sea_surface_temperature\"][mask] = np.nan\n",
    "\n",
    "mask = (data[\"barometric_pressure\"] < 850) | (data[\"barometric_pressure\"] > 1050)\n",
    "data[\"barometric_pressure\"][mask] = np.nan\n",
    "\n",
    "mask = (data[\"relative_humidity\"] > 105) | (data[\"relative_humidity\"] < 0)\n",
    "data[\"relative_humidity\"][mask] = np.nan\n",
    "\n",
    "mask = (data[\"shortwave_irradiance\"] > 2800) | (data[\"shortwave_irradiance\"] < 0)\n",
    "data[\"shortwave_irradiance\"][mask] = np.nan\n",
    "\n",
    "mask = data[\"sea_surface_salinity\"] < 33.6\n",
    "data[\"sea_surface_salinity\"][mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e2c2e",
   "metadata": {},
   "source": [
    "**Plot a timeseries for each test parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07825d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset = sorted(np.random.choice(data.time, 100000, replace=False))\n",
    "for param in list(test_parameters.keys()):\n",
    "    if param in data.variables:\n",
    "        sensor_range = test_parameters.get(param)\n",
    "        fig, ax = plot_variable(data.sel(time=subset), param)\n",
    "        #ax.set_ylim(sensor_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf619b",
   "metadata": {},
   "source": [
    "---\n",
    "## Gross Range\n",
    "The Gross Range QARTOD test consists of two parameters: a fail range which indicates when the data is bad, and a suspect range which indicates when data is either questionable or interesting. The fail range values are set based upon the instrument/measurement and associated calibration. For example, the conductivity sensors are calibration for measurements between 0 (freshwater) and 9 (highly-saline waters). The suspect range values are calculated based on the mean of the available data $\\pm$3$\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.gross_range import GrossRange\n",
    "from ooi_data_explorations.qartod.plotting import *\n",
    "from ooi_data_explorations.qartod.qc_processing import format_gross_range, format_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e082c06",
   "metadata": {},
   "source": [
    "#### Test Parameters & Sensor Ranges\n",
    "Map out the data variables in the data set to the data stream inputs and the associated sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ea5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters = {\n",
    "    \"eastward_wind_velocity\": [-45, 45],\n",
    "    \"northward_wind_velocity\": [-45, 45],\n",
    "    \"air_temperature\": [ -40, 60],\n",
    "    \"relative_humidity\": [0, 100],\n",
    "    \"barometric_pressure\": [850, 1050],\n",
    "    \"longwave_irradiance\": [0, 700],\n",
    "    \"shortwave_irradiance\": [0, 2800],\n",
    "    \"precipitation\": [0, 500],\n",
    "    \"sea_surface_conductivity\": [0, 7],\n",
    "    \"sea_surface_temperature\": [-5, 35],\n",
    "    \"sea_surface_salinity\": [0, 42]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb761e",
   "metadata": {},
   "source": [
    "**Calculate the Gross Range Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094f167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "site, node, sensor = refdes.split(\"-\", 2)\n",
    "gross_range_table = pd.DataFrame()\n",
    "\n",
    "for param in test_parameters:\n",
    "    sensor_range = test_parameters.get(param)\n",
    "    if param == \"sea_surface_salinity\":\n",
    "        inp = \"met_salsurf\"\n",
    "    else:\n",
    "        inp = param\n",
    "    \n",
    "    if param in data.variables:\n",
    "        print(f\"##### Calculating gross range for {param} #####\")\n",
    "        # Check if there is enough data\n",
    "        if len(data[param].dropna(dim=\"time\")) < 100:\n",
    "            user_range = [np.nan, np.nan]\n",
    "            source = \"Not enough data to calculate user range.\"\n",
    "        else:\n",
    "            # If barometric pressure, have to remove the data where 900 appears since that is bad\n",
    "            if param == \"barometric_pressure\":\n",
    "                mask = (data[param] == 900)\n",
    "                data[param][mask] = np.nan\n",
    "            # If sea_surface_conductivity, fail mode is -0.0003\n",
    "            if param == \"sea_surface_conductivity\":\n",
    "                mask = (data[param] == -0.0003)\n",
    "                data[param][mask] = np.nan\n",
    "            gross_range = GrossRange(sensor_range[0], sensor_range[1])\n",
    "            gross_range.fit(data, param, check_normality=True)\n",
    "            user_range = [gross_range.suspect_min, gross_range.suspect_max]\n",
    "            source = gross_range.source\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            if stream == \"metbk_hourly\":\n",
    "                pass\n",
    "            else:\n",
    "                qc_dict = format_gross_range(inp, sensor_range, user_range, site, node, sensor, stream, source)\n",
    "                gross_range_table = gross_range_table.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # Plot the result\n",
    "        try:\n",
    "            fig, ax = plot_gross_range(data, param, gross_range)\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            if ymin < sensor_range[0]:\n",
    "                ymin = sensor_range[0]\n",
    "            if ymax > sensor_range[1]:\n",
    "                ymax = sensor_range[1]\n",
    "            ax.set_ylim((ymin, ymax))\n",
    "            #ax.set_ylim((200, 600))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbb1db",
   "metadata": {},
   "source": [
    "**Add the stream name and the source comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table['notes'] = ('User range based on data collected through {}.'.format(\"2021-01-01\"))\n",
    "gross_range_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7110bf",
   "metadata": {},
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588287f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in gross_range_table.index:\n",
    "    print(gross_range_table.loc[ind][\"qcConfig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b6973",
   "metadata": {},
   "source": [
    "**Save the gross range table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "gross_range_table.to_csv(f\"../results/gross_range/{refdes}.csv\", index=False, columns=GR_HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e013d60",
   "metadata": {},
   "source": [
    "---\n",
    "## Climatology\n",
    "For the climatology QARTOD test, First, we bin the data by month and take the mean. The binned-montly means are then fit with a 2 cycle harmonic via Ordinary-Least-Squares (OLS) regression. Ranges are calculated based on the 3$\\sigma$ calculated from the OLS-fitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_climatology_table(ds, param, tinp, zinp, sensor_range, depth_bins):\n",
    "    \"\"\"Function which calculates the climatology table based on the \"\"\"\n",
    "    \n",
    "    climatologyTable = pd.DataFrame()\n",
    "    \n",
    "    if depth_bins is None:\n",
    "        # Filter out the data outside the sensor range\n",
    "        m = (ds[param] > sensor_range[0]) & (ds[param] < sensor_range[1]) & (~np.isnan(ds[param]))\n",
    "        param_data = ds[param][m]\n",
    "        \n",
    "        # Fit the climatology for the selected data\n",
    "        pmin, pmax = [0, 0]\n",
    "        \n",
    "        try:\n",
    "            climatology = Climatology()\n",
    "            climatology.fit(param_data)\n",
    "\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Calculate the climatology data\n",
    "            vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "            vmin = np.floor(vmin*10000)/10000\n",
    "            for vind in vmin.index:\n",
    "                if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                    vmin[vind] = sensor_range[0]\n",
    "            vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "            for vind in vmax.index:\n",
    "                if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                    vmax[vind] = sensor_range[1]\n",
    "            vmax = np.floor(vmax*10000)/10000\n",
    "            vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "            vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "            # Build the climatology dataframe\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "        except:\n",
    "            # Here is where to create nans if insufficient data to fit\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Create a series filled with nans\n",
    "            vals = []\n",
    "            for i in np.arange(len(tspan)):\n",
    "                vals.append([np.nan, np.nan])\n",
    "            vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "            # Add to the data\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "            \n",
    "        del ds, vspan, tspan, zspan\n",
    "        gc.collect()\n",
    "        \n",
    "    else:        \n",
    "    # Iterate through the depth bins to calculate the climatology for each depth bin\n",
    "        for dbin in depth_bins:\n",
    "            # Get the pressure range to bin from\n",
    "            pmin, pmax = dbin[0], dbin[1]\n",
    "\n",
    "            # Select the data from the pressure range\n",
    "            bin_data = data.where((data[zinp] >= pmin) & (data[zinp] <= pmax), drop=True)\n",
    "\n",
    "            # sort based on time and make sure we have a monotonic dataset\n",
    "            bin_data = bin_data.sortby('time')\n",
    "            _, index = np.unique(bin_data['time'], return_index=True)\n",
    "            bin_data = bin_data.isel(time=index)\n",
    "\n",
    "            # Filter out the data outside the sensor range\n",
    "            m = (bin_data[param] > sensor_range[0]) & (bin_data[param] < sensor_range[1]) & (~np.isnan(bin_data[param]))\n",
    "            param_data = bin_data[param][m]\n",
    "\n",
    "            # Fit the climatology for the selected data\n",
    "            try:\n",
    "                climatology = Climatology()\n",
    "                climatology.fit(param_data)\n",
    "\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Calculate the climatology data\n",
    "                vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "                vmin = np.floor(vmin*10000)/10000\n",
    "                for vind in vmin.index:\n",
    "                    if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                        vmin[vind] = sensor_range[0]\n",
    "                vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "                vmax = np.floor(vmax*10000)/10000\n",
    "                for vind in vmax.index:\n",
    "                    if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                        vmax[vind] = sensor_range[1]\n",
    "                vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "                vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "                # Build the climatology dataframe\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            except:\n",
    "                # Here is where to create nans if insufficient data to fit\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Create a series filled with nans\n",
    "                vals = []\n",
    "                for i in np.arange(len(tspan)):\n",
    "                    vals.append([np.nan, np.nan])\n",
    "                vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "                # Add to the data\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            del climatology, bin_data, vspan, tspan, zspan\n",
    "            gc.collect()\n",
    "    \n",
    "    return climatologyTable, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b5289",
   "metadata": {},
   "source": [
    "**Get the depth bins and filter based on max depth.** <br>\n",
    "For the ```METBK``` which are only deployed on Surface Moorings, there is no depth bins needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c427519",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bins = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ef241",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the climatology lookup table\n",
    "climatologyLookup = pd.DataFrame()\n",
    "\n",
    "# Setup the Table Header\n",
    "TBL_HEADER = [\"[1,1]\",\"[2,2]\",\"[3,3]\",\"[4,4]\",\"[5,5]\",\"[6,6]\",\"[7,7]\",\"[8,8]\",\"[9,9]\",\"[10,10]\",\"[11,11]\",\"[12,12]\"]\n",
    "\n",
    "# Set the subsite-node-sensor\n",
    "subsite, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "# Iterate through the parameters\n",
    "for param in test_parameters:\n",
    "    if param in data.variables:\n",
    "        if param == \"sea_surface_salinity\":\n",
    "            inp = \"met_salsurf\"\n",
    "        else:\n",
    "            inp = param\n",
    "        # ----------------- Depth tables ---------------------\n",
    "        # Get the sensor range of the parameter to test\n",
    "        print(f\"##### Calculating climatology for {param} #####\")\n",
    "        sensor_range = test_parameters.get(param)\n",
    "        \n",
    "        # Generate the climatology table with the depth bins\n",
    "        climatologyTable, climatology = make_climatology_table(data, param, \"time\", \"depth\", sensor_range, depth_bins)\n",
    "        \n",
    "        # Get the variance\n",
    "        try:\n",
    "            variance = float(np.round(climatology.regression['variance_explained']*100, 1))\n",
    "        except:\n",
    "            variance = 0.0\n",
    "\n",
    "        # Create the tableName\n",
    "        tableName = f\"{refdes}-{param}.csv\"\n",
    "        \n",
    "        # Save the results\n",
    "        climatologyTable.to_csv(f\"../results/climatology/climatology_tables/{tableName}\", header=TBL_HEADER)\n",
    "        \n",
    "        # ------------------ Lookup tables ------------------\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            if stream == \"metbk_hourly\":\n",
    "                pass\n",
    "            else:\n",
    "                qc_dict = {\n",
    "                    \"subsite\": subsite,\n",
    "                    \"node\": node,\n",
    "                    \"sensor\": sensor,\n",
    "                    \"stream\": stream,\n",
    "                    \"parameters\": {\n",
    "                        \"inp\": inp,\n",
    "                        \"tinp\": \"time\",\n",
    "                        \"zinp\": \"depth\",\n",
    "                    },\n",
    "                    \"climatologyTable\": f\"climatology_tables/{refdes}-{param}.csv\",\n",
    "                    \"source\": f\"The variance explained by the climatology model is {variance}%.\",\n",
    "                    \"notes\": \"Climatology based on available data through 2021-01-01.\"\n",
    "                }\n",
    "                # Append to the lookup table\n",
    "                climatologyLookup = climatologyLookup.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the climatology ------------------\n",
    "        if data[param].time.size > 100000:\n",
    "            try:\n",
    "                subset = sorted(np.random.choice(data.time, 100000, replace=False))\n",
    "                subset_data = data.sel(time=subset)\n",
    "                fig, ax = plot_climatology(subset_data, param, climatology)\n",
    "                ax.set_ylim((sensor_range[0], sensor_range[1]))\n",
    "                del subset, subset_data\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                fig, ax = plot_climatology(data, param, climatology)\n",
    "                ax.set_ylim((sensor_range[0], sensor_range[1]))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399e7c4",
   "metadata": {},
   "source": [
    "**Check the last climatologyTable for reasonableness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5305a",
   "metadata": {},
   "source": [
    "**Check the climatologyLookup table that all the entries made it in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b34d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a061455",
   "metadata": {},
   "source": [
    "**Save the climatologyLookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e94f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lookup table results\n",
    "climatologyLookup.to_csv(f\"../results/climatology/{refdes}.csv\", index=False, columns=CLM_HEADER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
