{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a161113",
   "metadata": {},
   "source": [
    "# PAR\n",
    "\n",
    "### Purpose\n",
    "The purpose of this notebook is to calculate the gross range user min/max values as well as the seasonally-varying climatology range table for populating QARTOD parameter tables for data streams for OOI - CGSN data streams for the Photosynthetically-active-radiation (PAR) sensors deployed by OOI. This instrument is the Biospherical Instruments QSP-2200 and is deployed only on Coastal wire-following-profilers.\n",
    "\n",
    "Due to sparsity of identifiably-good data as well as the nature of PAR, we elected to generate the gross range and climatology values using data obtained from the \"NOAA MSL12 Ocean Color - Science Quality - VIIRS SNPP\" coupled with an vertical model with exponentially-decreasing PAR as a function of depth to derive the expected values and ranges.\n",
    "\n",
    "### Test Parameters\n",
    "\n",
    "| Dataset Name | OOINet Name | Range |\n",
    "| ------------ | ----------- | ----- |\n",
    "| parad_k_par  | parad_k_par | 0 - 5000 $\\mu$mol photons m$^{-2}$ s$^{-1}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efddbe66",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca591953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime, pytz, re\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fa4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec9baa",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8c7c4",
   "metadata": {},
   "source": [
    "#### Install the ```pysolar``` package if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "!pip install pysolar\n",
    "from pysolar.solar import get_altitude\n",
    "from pysolar.radiation import get_radiation_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8aadd5",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719868de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.qartod.qc_processing import process_gross_range, process_climatology, woa_standard_bins, \\\n",
    "    inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9191e0",
   "metadata": {},
   "source": [
    "---\n",
    "## Identify Data Streams\n",
    "This section is necessary to identify all of the data stream associated with a specific instrument. This can be done by querying UFrame and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time.\n",
    "\n",
    "First, set the instrument to search for using OOI terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a87057",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = \"PAR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f85a85",
   "metadata": {},
   "source": [
    "### Query OOINet for Data Streams <br>\n",
    "First check if the datasets have already been downloaded; if not, use the ```M2M.search_datasets``` tool to search the OOINet API and return a table of all of the available datasets for the given instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce40f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = pd.read_csv(\"../data/PAR_datasets.csv\")\n",
    "except:\n",
    "    datasets = M2M.search_datasets(instrument=\"PAR\", English_names=True)\n",
    "    # Save the datasets\n",
    "    datasets.to_csv(\"../data/PAR_datasets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c411f1",
   "metadata": {},
   "source": [
    "Separate out the CGSN datasets from the EA and RCA datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgsn = datasets[\"array\"].apply(lambda x: True if x.startswith((\"CP\",\"GA\",\"GI\",\"GP\",\"GS\")) else False)\n",
    "datasets = datasets[cgsn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a1a2d",
   "metadata": {},
   "source": [
    "Remove the ```PARAD``` mounted on gliders and AUVS (\"MOAS\") as well as surface-piercing profilers (CSPPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019035d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = datasets[\"refdes\"].apply(lambda x: False if \"MOAS\" in x or \"SP001\" in x else True)\n",
    "datasets = datasets[mask]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0bac",
   "metadata": {},
   "source": [
    "---\n",
    "## Single Reference Designator\n",
    "The reference designator acts as a key for an instrument located at a specific location. First, select a reference designator (refdes) to request data from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_designators = sorted(cgsn_datasets[\"refdes\"])\n",
    "print(\"Number of reference designators: \" + str(len(reference_designators)))\n",
    "for refdes in reference_designators:\n",
    "    print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f65e41",
   "metadata": {},
   "source": [
    "#### Select a reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc98f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=6\n",
    "#refdes = reference_designators[k]\n",
    "refdes = \"CP02PMUO-WFP01-05-PARADK000\"\n",
    "print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4dba4a",
   "metadata": {},
   "source": [
    "#### Sensor Vocab\n",
    "The vocab provides information about the instrument model and type, its location (with descriptive names), depth, and manufacturer. Get the vocab for the given reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29808548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e9503",
   "metadata": {},
   "source": [
    "#### Sensor Deployments\n",
    "Download the deployment information for the selected reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23375191",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8c20a",
   "metadata": {},
   "source": [
    "#### Sensor Data Streams\n",
    "Next, select the specific data streams for the given reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10bf03",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata \n",
    "The metadata contains the following important key pieces of data for each reference designator: **method**, **stream**, **particleKey**, and **count**. The method and stream are necessary for identifying and loading the relevant dataset. The particleKey tells us which data variables in the dataset we should be calculating the QARTOD parameters for. The count lets us know which dataset (the recovered instrument, recovered host, or telemetered) contains the most data and likely has the best record to use to calculate the QARTOD tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc1d43",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters for calculating the relevant QARTOD test limits. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we through several steps to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    science_vars = filter_science_parameters(metadata)\n",
    "    # Next, eliminate the optode temperature from the stream\n",
    "    mask = science_vars[\"particleKey\"].apply(lambda x: False if \"temp\" in x else True)\n",
    "    science_vars = science_vars[mask]\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    science_vars = science_vars.explode(column=\"particleKey\")\n",
    "    return science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vars = filter_science_parameters(metadata)\n",
    "science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "science_vars = science_vars.reset_index()\n",
    "science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b9603",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate the expected irradiance\n",
    "\n",
    "Data obtained from the \"NOAA MSL12 Ocean Color - Science Quality - VIIRS SNPP\" satellite data products page, downloading the L3 monthly KdPAR data from the FTP server (ERDDAP and THREDDS servers proved too unstable to rely on). See the data products web page for more information: https://coastwatch.noaa.gov/cw/satellite-data-products/ocean-color/science-quality/viirs-snpp.html. \n",
    "\n",
    "First check to see if the data has been downloaded, if not do so first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooinet import Download\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4dc9b3",
   "metadata": {},
   "source": [
    "Download the Coastwatch data via FTP in three steps:\n",
    "1. Connect to the FTP server\n",
    "2. Setup the directory to download the Coastwatch data to\n",
    "3. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a10fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "ftp_server = 'ftp.star.nesdis.noaa.gov'\n",
    "ftp_server_path = '/pub/socd1/mecb/coastwatch/viirs/science/L3/global/kd/monthly/WW00/'\n",
    "ftp = FTP(ftp_server)\n",
    "ftp.login(user='anonymous')\n",
    "ftp.cwd(ftp_server_path)\n",
    "# List the files\n",
    "files = ftp.nlst('*kdpar.nc')\n",
    "\n",
    "# Step 2\n",
    "saveDir = f\"../data/coastwatch/raw/\"\n",
    "saveDir = os.path.abspath(saveDir)\n",
    "Download.setup_download_dir(saveDir)\n",
    "\n",
    "# Step 3\n",
    "for file in files:\n",
    "    download_path = \"/\".join((saveDir, file))\n",
    "    # Check if the file has been downloaded\n",
    "    if not (os.path.isfile(download_path)) or (os.path.getsize(download_path) == 0):\n",
    "        with open(download_path, 'wb') as f:\n",
    "            ftp.retrbinary('RETR %s' % file, f.write)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c09917",
   "metadata": {},
   "source": [
    "Close the FTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af505a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d0511",
   "metadata": {},
   "source": [
    "---\n",
    "## Process the CoastWatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09140d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latitude and longitude of the deployments\n",
    "latitude = deployments[\"latitude\"].mean()\n",
    "longitude = deployments[\"longitude\"].mean()\n",
    "\n",
    "# Get the min depth and max depth of the instrument\n",
    "min_depth = vocab[\"mindepth\"].values\n",
    "max_depth = vocab[\"maxdepth\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04efe1",
   "metadata": {},
   "source": [
    "Load the coastwatch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    kd = xr.open_mfdataset(\"../data/coastwatch/raw/*.nc\", combine='nested', concat_dim='time', engine='netcdf4', parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a69e4e",
   "metadata": {},
   "source": [
    "Clean up the dataset & limit the geographic extent to the site under question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = kd.drop_vars(['coord_ref', 'palette'])\n",
    "kd = kd.where((kd['lat'] >= latitude - 0.09375) & (kd['lat'] <= latitude + 0.09375), drop=True)\n",
    "kd = kd.where((kd['lon'] >= longitude - 0.09375) & (kd['lon'] <= longitude + 0.09375), drop=True)\n",
    "kd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e295a3",
   "metadata": {},
   "source": [
    "Take the mean of the CoastWatch PAR data across the altitude, latitude, and longitude coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb808faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = kd.mean(dim=['altitude', 'lat', 'lon'], keep_attrs=True)\n",
    "kd = kd.sortby('time')\n",
    "kd = kd.compute()\n",
    "kd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064523df",
   "metadata": {},
   "source": [
    "Calculate clear sky irradiance at solar noon for this site using the Kd(PAR) time record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime(kd.time).to_pydatetime()\n",
    "surface = []\n",
    "for d in date:\n",
    "    dt = d.replace(tzinfo=pytz.timezone('US/Pacific'))\n",
    "    altitude = get_altitude(latitude, longitude, dt)\n",
    "    # PAR is approximately 50% of the shortwave radiation, and we need to convert from W/m^2 to umol/m^2/s\n",
    "    surface.append(get_radiation_direct(dt, altitude) * 0.5 / 0.21739130434)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc3379",
   "metadata": {},
   "source": [
    "Create a 2D array with Ed(PAR) estimated as a function of depth from the satellite Kd(PAR) values and model estimates of clear-sky irradiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19309c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(min_depth, max_depth + 0.125, 0.125)\n",
    "ed = np.zeros([len(date), len(depths)])\n",
    "for i in range(len(date)):\n",
    "    ed[i, :] = surface[i] * np.exp(-kd.kd_par.values[i] * depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f5481",
   "metadata": {},
   "source": [
    "Convert to an xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to an xarray dataset\n",
    "ed = xr.Dataset({\n",
    "    'parad_k_par': (['time', 'depth'], ed),\n",
    "}, coords={'time': kd.time.values, 'depth': depths})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf2a37",
   "metadata": {},
   "source": [
    "---\n",
    "## Gross Range\n",
    "The Gross Range QARTOD test consists of two parameters: a fail range which indicates when the data is bad, and a suspect range which indicates when data is either questionable or interesting. The fail range values are set based upon the instrument/measurement and associated calibration. The user range for ```PAR``` will be based on the estimation from near-surface values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8daba",
   "metadata": {},
   "source": [
    "Set the parameters and the gross range fail range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['parad_k_par']\n",
    "limits = [0, 5000]\n",
    "\n",
    "# create the initial gross range entry\n",
    "quantile = ed['depth'].quantile(0.01).values     # upper 1% of the depth array\n",
    "sub = ed.where(ed.depth <= quantile, drop=True)  # limit gross range estimation to near-surface values\n",
    "sub = sub.max(dim=['depth'], keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7353a96",
   "metadata": {},
   "source": [
    "Create the initial gross range entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = ed['depth'].quantile(0.01).values     # upper 1% of the depth array\n",
    "sub = ed.where(ed.depth <= quantile, drop=True)  # limit gross range estimation to near-surface values\n",
    "sub = sub.max(dim=['depth'], keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8f712",
   "metadata": {},
   "source": [
    "**Generate the gross range lookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the gross range lookup table\n",
    "site, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "gross_range_table = pd.DataFrame()\n",
    "for index in datastreams.index:\n",
    "    method = datastreams[\"method\"].loc[index]\n",
    "    stream = datastreams[\"stream\"].loc[index]\n",
    "    gr_lookup = process_gross_range(sub, parameters, limits, site=site, node=node, sensor=sensor, stream=stream)\n",
    "\n",
    "    # add the stream name and the source comment\n",
    "    gr_lookup['notes'] = ('User range modeled from data collected by the NOAA VIIRS satellite and estimates of '\n",
    "                          'clear sky irradiance from the pysolar package.')\n",
    "    gross_range_table = gross_range_table.append(gr_lookup, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb45457",
   "metadata": {},
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433ecec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gross_range_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94872944",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in gross_range_table.index:\n",
    "    print(gross_range_table.loc[ind][\"qcConfig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b14203",
   "metadata": {},
   "source": [
    "**Save the gross range table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2faf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table.to_csv(f\"../results/gross_range/{refdes}.csv\", index=False, columns=GR_HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369c66f",
   "metadata": {},
   "source": [
    "---\n",
    "## Climatology\n",
    "For the climatology QARTOD test, First, we bin the data by month and take the mean. The binned-montly means are then fit with a 2 cycle harmonic via Ordinary-Least-Squares (OLS) regression. Ranges are calculated based on the 3$\\sigma$ calculated from the OLS-fitting. For the PAR, the data we are fitting is coming from the CoastWatch depth model we generated earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e980276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_climatology_table(ds, param, tinp, zinp, sensor_range, depth_bins):\n",
    "    \"\"\"Function which calculates the climatology table based on the \"\"\"\n",
    "    \n",
    "    climatologyTable = pd.DataFrame()\n",
    "    \n",
    "    if depth_bins is None:\n",
    "        # Filter out the data outside the sensor range\n",
    "        m = (ds[param] > sensor_range[0]) & (ds[param] < sensor_range[1]) & (~np.isnan(ds[param]))\n",
    "        param_data = ds[param][m]\n",
    "        \n",
    "        # Fit the climatology for the selected data\n",
    "        pmin, pmax = [0, 0]\n",
    "        \n",
    "        try:\n",
    "            climatology = Climatology()\n",
    "            climatology.fit(param_data)\n",
    "\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Calculate the climatology data\n",
    "            vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "            vmin = np.floor(vmin*100000)/100000\n",
    "            for vind in vmin.index:\n",
    "                if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                    vmin[vind] = sensor_range[0]\n",
    "            vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "            for vind in vmax.index:\n",
    "                if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                    vmax[vind] = sensor_range[1]\n",
    "            vmax = np.floor(vmax*100000)/100000\n",
    "            vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "            vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "            # Build the climatology dataframe\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "        except:\n",
    "            # Here is where to create nans if insufficient data to fit\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Create a series filled with nans\n",
    "            vals = []\n",
    "            for i in np.arange(len(tspan)):\n",
    "                vals.append([np.nan, np.nan])\n",
    "            vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "            # Add to the data\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "            \n",
    "        del ds, vspan, tspan, zspan\n",
    "        gc.collect()\n",
    "        \n",
    "    else:        \n",
    "    # Iterate through the depth bins to calculate the climatology for each depth bin\n",
    "        for dbin in depth_bins:\n",
    "            # Get the pressure range to bin from\n",
    "            pmin, pmax = dbin[0], dbin[1]\n",
    "\n",
    "            # Select the data from the pressure range\n",
    "            bin_data = ds.where((ds[zinp] >= pmin) & (ds[zinp] <= pmax), drop=True)\n",
    "\n",
    "            # sort based on time and make sure we have a monotonic dataset\n",
    "            bin_data = bin_data.sortby('time')\n",
    "            _, index = np.unique(bin_data['time'], return_index=True)\n",
    "            bin_data = bin_data.isel(time=index)\n",
    "\n",
    "            # Filter out the data outside the sensor range\n",
    "            m = (bin_data[param] > sensor_range[0]) & (bin_data[param] < sensor_range[1]) & (~np.isnan(bin_data[param]))\n",
    "            bin_data = bin_data.where(m, drop=True)\n",
    "            param_data = bin_data[param]\n",
    "\n",
    "            # Fit the climatology for the selected data\n",
    "            try:\n",
    "                climatology = Climatology()\n",
    "                climatology.fit(param_data)\n",
    "\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Calculate the climatology data\n",
    "                vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "                vmin = np.floor(vmin*100000)/100000\n",
    "                for vind in vmin.index:\n",
    "                    if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                        vmin[vind] = sensor_range[0]\n",
    "                vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "                vmax = np.floor(vmax*100000)/100000\n",
    "                for vind in vmax.index:\n",
    "                    if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                        vmax[vind] = sensor_range[1]\n",
    "                vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "                vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "                # Build the climatology dataframe\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            except:\n",
    "                # Here is where to create nans if insufficient data to fit\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Create a series filled with nans\n",
    "                vals = []\n",
    "                for i in np.arange(len(tspan)):\n",
    "                    vals.append([np.nan, np.nan])\n",
    "                vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "                # Add to the data\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            del bin_data, vspan, tspan, zspan\n",
    "            gc.collect()\n",
    "    \n",
    "    return climatologyTable#, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62c9ba",
   "metadata": {},
   "source": [
    "**Get the depth bins and filter based on max depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b10427",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bins = woa_standard_bins()\n",
    "pmax = data[\"depth\"].max().values\n",
    "pmin = data[\"depth\"].min().values\n",
    "mask = (depth_bins[:, 0] < pmax) | ((depth_bins[:, 0] < pmax) & (depth_bins[:, 1] > pmax)) | (depth_bins[:, 1] < pmin)\n",
    "depth_bins = depth_bins[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d73fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the climatology lookup table\n",
    "climatologyLookup = pd.DataFrame()\n",
    "\n",
    "# Setup the Table Header\n",
    "TBL_HEADER = [\"[1,1]\",\"[2,2]\",\"[3,3]\",\"[4,4]\",\"[5,5]\",\"[6,6]\",\"[7,7]\",\"[8,8]\",\"[9,9]\",\"[10,10]\",\"[11,11]\",\"[12,12]\"]\n",
    "\n",
    "# Set the subsite-node-sensor\n",
    "subsite, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "# Set the parameters\n",
    "param = \"parad_k_par\"\n",
    "\n",
    "# ----------------- Depth tables ---------------------\n",
    "# Get the sensor range of the parameter to test\n",
    "print(f\"##### Calculating climatology for {param} #####\")\n",
    "sensor_range = [0, 5000]\n",
    "        \n",
    "# Generate the climatology table with the depth bins\n",
    "climatologyTable = make_climatology_table(ed, param, \"time\", \"depth\", sensor_range, depth_bins)\n",
    "\n",
    "# Create the tableName\n",
    "tableName = f\"{refdes}-{param}.csv\"\n",
    "\n",
    "# Save the results\n",
    "climatologyTable.to_csv(f\"../results/climatology/climatology_tables/{tableName}\", header=TBL_HEADER)\n",
    "        \n",
    "# ------------------ Lookup tables ------------------\n",
    "# Check which streams have the param in it\n",
    "streams = np.unique(datastreams[\"stream\"])\n",
    "for stream in streams:\n",
    "    qc_dict = {\n",
    "        \"subsite\": subsite,\n",
    "        \"node\": node,\n",
    "        \"sensor\": sensor,\n",
    "        \"stream\": stream,\n",
    "        \"parameters\": {\n",
    "            \"inp\": param,\n",
    "            \"tinp\": \"time\",\n",
    "            \"zinp\": \"depth\",\n",
    "        },\n",
    "        \"climatologyTable\": f\"climatology_tables/{refdes}-{param}.csv\",\n",
    "        \"source\": \"Climatology values are calculated from and applicable to standard depth bins.\",\n",
    "        \"notes\": \"User range modeled from data collected by the NOAA VIIRS satellite and estimates of \\\n",
    "                  clear sky irradiance from the pysolar package.\"\n",
    "    }\n",
    "    # Append to the lookup table\n",
    "    climatologyLookup = climatologyLookup.append(qc_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59255052",
   "metadata": {},
   "source": [
    "**Check the last climatologyTable for reasonableness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93553fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11da9c8",
   "metadata": {},
   "source": [
    "**Check the climatologyLookup table that all the entries made it in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d41498",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in climatologyLookup[\"parameters\"]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in climatologyLookup[\"climatologyTable\"]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd2d8e",
   "metadata": {},
   "source": [
    "**Save the climatologyLookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f564e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup.to_csv(f\"../results/climatology/{refdes}.csv\", index=False, columns=CLM_HEADER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
