{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cfadf9",
   "metadata": {},
   "source": [
    "# NUTNR\n",
    "\n",
    "### Purpose\n",
    "The purpose of this notebook is to calculate the values needed to populate the QARTOD value tables for the Gross Range and Climatology tests as implemented by OOI. The ```NUTNR``` is the sensor name given to the instruments deployed by OOI for measuring seawater nitrate. That includes both the In-Situ Ultraviolet Spectrophotometer (ISUS) and the SUbmersible Ultraviolet Nitrate Analyzer (SUNA) instruments. The ISUS and SUNA instruments were deployed on fixed-depth assets on both coastal and global arrays. The ISUS was phased out in 2018 due to design and data quality issues in favor of the SUNA. Consequently, only the SUNA data is utilized to calculate the QARTOD table inputs.\n",
    "\n",
    "\n",
    "\n",
    "### ISUS v SUNA\n",
    "The following is **Table 3.1** from the **\"OOI Biogeochemical Sensor Data Best Practices and User Guide\" Chapter 3 - Nitrate** which describes which instrument was deployed where and for how long:\n",
    "\n",
    "| Array | Platforms | Sensors | OOI Class-Series |\n",
    "| ----- | --------- | ------- | ---------------- |\n",
    "| Global Argentine Basin Array | Global Profiling Glider | SUNA (2016 - 2017) | NUTNR-M |\n",
    "|                              | NSIF<br>Subsurface Buoy | ISUS (2015 - 2018) | NUTNR-B |\n",
    "| Global Irminger Sea Array | GLOBAL Profiling Glider | SUNA (2014 - Present | NUTNR-M |\n",
    "|                           | NSIF<br>Subsurface Buoy | ISUS (Sep 2014 - Jun 2018)<br>SUNA V2 (Jun 2018 - present) | NUTNR-B |\n",
    "| Global Southern Ocean Array | Global Profiling Glider | SUNA (2014 - Present) | NUTNR-M |\n",
    "|                           | NSIF<br>Subsurface Buoy | ISUS (Sep 2014 - Jun 2018)<br>SUNA V2 (Jun 2018 - present) | NUTNR-B |\n",
    "| Global Station Papa Array | Global Profiling Glider | SUNA V2 (2013 - Present) | NUTNR-M |\n",
    "| Regional Cabled Array | Shallow Profiler Mooring | Deep SUNA (2014 - Present) | NUTNR-A |\n",
    "| Coastal Endurance Array | Surface Piercing Profiler | SUNA V2 (2014 - Present) | NUTNR-J |\n",
    "|                           | NSIF<br>Subsurface Buoy | ISUS (2014 - 2018)<br>SUNA V2 (2018 - present) | NUTNR-B |\n",
    "| Coastal Pioneer Array | Coastal AUV | SUNA (2015 - Present) | NUTNR-N |\n",
    "|                       | Surface Piercing Profiler | SUNA V2 (2014 - 2016) | NUTNR-J |\n",
    "|                       | Coastal Profiling Glider | SUNA (2015 - Present) | NUTNR-M |\n",
    "|                       | NSIF<br>Subsurface Buoy | ISUS (2013 - Mar 2018)<br>SUNA V2 (Mar 2018 - present) | NUTNR-B |\n",
    "\n",
    "### Test Parameters\n",
    "\n",
    "| Dataset Name | OOINet Name | Range |\n",
    "| ------------ | ----------- | ----- |\n",
    "| corrected_nitrate_concentrations | salinity_corrected_nitrate | -1.5 - 3000 $\\mu$mol/kg |\n",
    "| nitrate_concentration | nitrate_concentration | -2 - 3000 $\\mu$mol/kg |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03d2ee",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d43f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys, datetime, pytz, re\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ddc8",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox\n",
    "This toolbox is publicly available at https://github.com/reedan88/OOINet. It should be cloned onto your machine and the setup instructions followed before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OOINet M2M tool\n",
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M\n",
    "from ooinet.utils import convert_time, ntp_seconds_to_datetime, unix_epoch_time\n",
    "from ooinet.Instrument.common import process_file, add_annotation_qc_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e1244",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox\n",
    "This toolbox is publicly available at https://github.com/oceanobservatories/ooi-data-explorations. Similarly to the ```ooinet``` toolbox above, it should be installed onto your machine following the setup instructions before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.common import get_annotations, get_vocabulary, load_gc_thredds\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_nutnr import suna_datalogger, suna_instrument\n",
    "from ooi_data_explorations.qartod.qc_processing import identify_blocks, create_annotations, process_gross_range, \\\n",
    "    process_climatology, parse_qc, inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1060077",
   "metadata": {},
   "source": [
    "#### Import plotting and visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67501483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5d48f",
   "metadata": {},
   "source": [
    "---\n",
    "## Identify Data Streams\n",
    "This section is necessary to identify all of the data stream associated with a specific instrument. This can be done by querying UFrame and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time.\n",
    "\n",
    "First, set the instrument to search for using OOI terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = \"NUTNR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb21299",
   "metadata": {},
   "source": [
    "### Query OOINet for Data Streams <br>\n",
    "If the data streams for a given instrument have not yet been identified from OOINet, then want to query OOINet for the data sets and save them to the local memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b295fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = pd.read_csv(\"../data/NUTNR_datasets.csv\")\n",
    "except:\n",
    "    datasets = M2M.search_datasets(instrument=\"NUTNR\", English_names=True)\n",
    "    datasets.to_csv(f\"../data/NUTNR_datasets.csv\", index=False)\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9de6fa",
   "metadata": {},
   "source": [
    "Separate out the CGSN datasets from the EA and RCA datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628005a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgsn = datasets[\"array\"].apply(lambda x: True if x.startswith((\"CP\",\"GA\",\"GI\",\"GP\",\"GS\")) else False)\n",
    "datasets = datasets[cgsn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b79f1",
   "metadata": {},
   "source": [
    "For the Argentine Basin, I will need the Global Profiling Glider SUNA data because we lack any SUNA information from that location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "argentine_moas = datasets[\"array\"].apply(lambda x: True if \"GA05MOAS\" in x else False)\n",
    "argentine_gliders = datasets[argentine_moas]\n",
    "argentine_gliders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc62116",
   "metadata": {},
   "source": [
    "Otherwise, remove the ```NUTNRs``` mounted on gliders and AUVs (\"MOAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "moas = datasets[\"array\"].apply(lambda x: True if \"MOAS\" in x else False)\n",
    "datasets = datasets[~moas]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0bac",
   "metadata": {},
   "source": [
    "---\n",
    "## Single Reference Designator\n",
    "The reference designator acts as a key for an instrument located at a specific location. First, select a reference designator (refdes) to request data from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_designators = sorted(datasets[\"refdes\"])\n",
    "print(\"Number of reference designators: \" + str(len(reference_designators)))\n",
    "for refdes in reference_designators:\n",
    "    print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44012ae4",
   "metadata": {},
   "source": [
    "Select a single reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb66114",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "refdes = reference_designators[k]\n",
    "print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa03f4",
   "metadata": {},
   "source": [
    "#### Sensor Vocab\n",
    "The vocab provides information about the instrument model and type, its location (with descriptive names), depth, and manufacturer. Get the vocab for the given reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4386f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f248210",
   "metadata": {},
   "source": [
    "#### Sensor Deployments\n",
    "Download the deployment information for the selected reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf043193",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d996fb",
   "metadata": {},
   "source": [
    "#### Sensor Data Streams\n",
    "Next, select the specific data streams for the given reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51398c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a762e",
   "metadata": {},
   "source": [
    "#### Filter datastreams for just SUNA streams\n",
    "Midway through the program OOI switched from the ISUS to the SUNA. The ISUS data is generally poor quality and we want to completely exclude it from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = datastreams[\"stream\"].apply(lambda x: True if \"suna\" in x else False)\n",
    "datastreams[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22822d3",
   "metadata": {},
   "source": [
    "---\n",
    "### Argentine Basin\n",
    "For the Argentine Basin, we do not have any non-ISUS data. Will take a look at the Glider SUNA data as a fill-in for the mooring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gliders(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c56850",
   "metadata": {},
   "outputs": [],
   "source": [
    "gliders_data = {}\n",
    "gliders_annotations = {}\n",
    "\n",
    "for glider_refdes in argentine_gliders[\"refdes\"]:\n",
    "    print(f\"####### Getting data for {glider_refdes} #######\")\n",
    "    \n",
    "    # First, get the associated deployments\n",
    "    glider_deployments = M2M.get_deployments(glider_refdes)\n",
    "    \n",
    "    # Next, get the associated metadata\n",
    "    glider_metadata = M2M.get_metadata(glider_refdes)\n",
    "    \n",
    "    # Filter for the relevant science parameters\n",
    "    science_vars = filter_science_parameters(glider_metadata)\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    \n",
    "    # Get the datastreams\n",
    "    glider_datastreams = M2M.get_datastreams(glider_refdes)\n",
    "    \n",
    "    # Get the associated annotations\n",
    "    refdes_annotations = M2M.get_annotations(glider_refdes)\n",
    "    gliders_annotations.update({glider_refdes: refdes_annotations})\n",
    "\n",
    "    # Get the available datasets\n",
    "    for index in glider_datastreams.index:\n",
    "        # Get the method and stream\n",
    "        method = glider_datastreams.loc[index][\"method\"]\n",
    "        stream = glider_datastreams.loc[index][\"stream\"]\n",
    "\n",
    "        # Get the URL - first try the goldCopy thredds server\n",
    "        thredds_url = M2M.get_thredds_url(glider_refdes, method, stream, goldCopy=True)\n",
    "\n",
    "        # Get the catalog\n",
    "        catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "        # Clean the catalog\n",
    "        catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "\n",
    "        # Get the links to the THREDDs server and load the data\n",
    "        dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "        if method == \"telemetered\":\n",
    "            tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "            print(f\"----- Load {method}-{stream} data -----\")\n",
    "            with ProgressBar():\n",
    "                tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_gliders, parallel=True)\n",
    "        elif method == \"recovered_host\":\n",
    "            host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "            print(f\"----- Load {method}-{stream} data -----\")\n",
    "            with ProgressBar():\n",
    "                host_data = xr.open_mfdataset(host_files, preprocess=preprocess_gliders, parallel=True)\n",
    "        elif method == \"recovered_inst\":\n",
    "            inst_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "            print(f\"----- Load {method}-{stream} data -----\")\n",
    "            with ProgressBar():\n",
    "                inst_data = xr.open_mfdataset(inst_files, preprocess=preprocess_gliders, parallel=True)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "                \n",
    "    # Merge all the datasets together\n",
    "    methods = glider_datastreams[\"method\"].unique()\n",
    "    if \"telemetered\" not in methods:\n",
    "        tele_data = None\n",
    "    if \"recovered_host\" not in methods:\n",
    "        host_data = None\n",
    "    if \"recovered_inst\" not in methods:\n",
    "        inst_data = None\n",
    "    refdes_data = combine_datasets(tele_data, host_data, inst_data, None)\n",
    "    \n",
    "    # Add in the annotations\n",
    "    refdes_data = add_annotation_qc_flag(refdes_data, refdes_annotations)\n",
    "    \n",
    "    # Save the results\n",
    "    gliders_data.update({glider_refdes: refdes_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a3c2e",
   "metadata": {},
   "source": [
    "Combine all of the glider datasets together into a single dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e063566",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gliders = None\n",
    "for refdes in gliders_data.keys():\n",
    "    if merged_gliders is None:\n",
    "        merged_gliders = gliders_data[refdes]\n",
    "    else:\n",
    "        merged_gliders = xr.concat([merged_gliders, gliders_data[refdes]], dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeff3bb",
   "metadata": {},
   "source": [
    "Sort the merged data based on time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af948d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gliders = merged_gliders.sortby(\"time\")\n",
    "merged_gliders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6c3ad",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Metadata \n",
    "The metadata contains the following important key pieces of data for each reference designator: **method**, **stream**, **particleKey**, and **count**. The method and stream are necessary for identifying and loading the relevant dataset. The particleKey tells us which data variables in the dataset we should be calculating the QARTOD parameters for. The count lets us know which dataset (the recovered instrument, recovered host, or telemetered) contains the most data and likely has the best record to use to calculate the QARTOD tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a4e3e",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters for calculating the relevant QARTOD test limits. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we through several steps to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    science_vars = filter_science_parameters(metadata)\n",
    "    # Next, eliminate the optode temperature from the stream\n",
    "    mask = science_vars[\"particleKey\"].apply(lambda x: False if \"temp\" in x else True)\n",
    "    science_vars = science_vars[mask]\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    science_vars = science_vars.explode(column=\"particleKey\")\n",
    "    return science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12987f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vars = filter_science_parameters(metadata)\n",
    "science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "science_vars = science_vars.reset_index()\n",
    "science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6831f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982d635",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data\n",
    "When calculating the QARTOD data tables, we only want to utilize the most complete data record available for a given reference designator. We do this by getting all the available data streams, loading the data, and then combining them into a single dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_overlaps(ds, deployments):\n",
    "    \"\"\"Trim overlapping deployment data (necessary to use xr.open_mfdataset)\"\"\"\n",
    "    # --------------------------------\n",
    "    # Second, get the deployment times\n",
    "    deployments = deployments.sort_values(by=\"deploymentNumber\")\n",
    "    deployments = deployments.set_index(keys=\"deploymentNumber\")\n",
    "    # Shift the start times by (-1) \n",
    "    deployEnd = deployments[\"deployStart\"].shift(-1)\n",
    "    # Find where the deployEnd times are earlier than the deployStart times\n",
    "    mask = deployments[\"deployEnd\"] > deployEnd\n",
    "    # Wherever the deployEnd times occur after the shifted deployStart times, replace those deployEnd times\n",
    "    deployments[\"deployEnd\"][mask] = deployEnd[mask]\n",
    "    deployments[\"deployEnd\"] = deployments[\"deployEnd\"].apply(lambda x: pd.to_datetime(x))\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # With the deployments info, can write a preprocess function to filter \n",
    "    # the data based on the deployment number\n",
    "    depNum = np.unique(ds[\"deployment\"])\n",
    "    deployInfo = deployments.loc[depNum]\n",
    "    deployStart = deployInfo[\"deployStart\"].values[0]\n",
    "    deployEnd = deployInfo[\"deployEnd\"].values[0]\n",
    "    \n",
    "    # Select the dataset data which falls within the specified time range\n",
    "    ds = ds.sel(time=slice(deployStart, deployEnd))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def preprocess_datalogger(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = suna_datalogger(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    gc.collect()\n",
    "    return ds\n",
    "\n",
    "def preprocess_instrument(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = suna_instrument(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c65a1",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data\n",
    "To access data, there are two applicable methods. The first is to download the data and save the netCDF files locally. The second is to access and process the files remotely on the THREDDS server, without having to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87004051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available datasets\n",
    "for index in datastreams[mask].index:\n",
    "    # Get the method and stream\n",
    "    method = datastreams.loc[index][\"method\"]\n",
    "    stream = datastreams.loc[index][\"stream\"]\n",
    "\n",
    "    # Get the URL - first try the goldCopy thredds server\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "    # Clean the catalog\n",
    "    catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Get the links to the THREDDs server and load the data\n",
    "    dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "    if method == \"telemetered\":\n",
    "        tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_host\":\n",
    "        host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            host_data = xr.open_mfdataset(host_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_inst\":\n",
    "        inst_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            inst_data = xr.open_mfdataset(inst_files, preprocess=preprocess_instrument, parallel=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1fda0",
   "metadata": {},
   "source": [
    "**Combine the data into a single file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_datasets(tele_data, host_data, inst_data, None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6328a71",
   "metadata": {},
   "source": [
    "**Clean up workspace variables and free up memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data.close()\n",
    "tele_data.close()\n",
    "inst_data.close()\n",
    "del tele_data, host_data, inst_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb97ed3",
   "metadata": {},
   "source": [
    "---\n",
    "## Human-in-the-loop review\n",
    "Next, visualize and inspect the available time series and data streams that are to be tested in order to identify instrument failures or data issues that are known failure modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db8547",
   "metadata": {},
   "source": [
    "#### Annotations\n",
    "The annotations associated with a specific reference designator may contain relevant information on the performance or reliability of the data for a given dataset. The annotations are downloaded from OOINet as a json and processed into a pandas dataframe. Each annotation may apply to the entire dataset, to a specific stream, or to a specific variable. With the downloaed annotations, we can use the information contained in the ```qcFlag``` column to translate the annotations into QC flags, which can then be used to filter out bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496eb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_dates(x):\n",
    "    if pd.isna(x):\n",
    "        x = datetime.datetime.now()\n",
    "    else:\n",
    "        x = convert_time(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = M2M.get_annotations(refdes)\n",
    "\n",
    "# convert the times to human-readable-dates\n",
    "annotations_with_dates = annotations\n",
    "annotations_with_dates[\"beginDT\"] = annotations_with_dates[\"beginDT\"].apply(lambda x: human_dates(x))\n",
    "annotations_with_dates[\"endDT\"] = annotations_with_dates[\"endDT\"].apply(lambda x: human_dates(x))\n",
    "annotations_with_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d99d27",
   "metadata": {},
   "source": [
    "Look at specific annotations to see what they say:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b75009",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_with_dates.loc[4][\"annotation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc88908",
   "metadata": {},
   "source": [
    "Add the annotations to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b217c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = M2M.get_annotations(refdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_annotation_qc_flag(data, annotations)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5caba3",
   "metadata": {},
   "source": [
    "**Plot some of the variables/data for visual inspection of the time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_variable(data, 'corrected_nitrate_concentration')\n",
    "\n",
    "ax.plot(data.where(data.rollup_annotations_qc_results > 2, drop=True)[\"time\"], data.where(data.rollup_annotations_qc_results > 2, drop=True)['corrected_nitrate_concentration'],\n",
    "        marker='o', linestyle=\"\", color=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f80155",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_variable(data, 'nitrate_concentration')\n",
    "\n",
    "ax.plot(data.where(data.rollup_annotations_qc_results > 2, drop=True)[\"time\"], data.where(data.rollup_annotations_qc_results > 2, drop=True)['nitrate_concentration'],\n",
    "        marker='o', linestyle=\"\", color=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_variable(data, \"sea_water_practical_salinity\")\n",
    "ax.plot(data.where(data.rollup_annotations_qc_results > 2, drop=True)[\"time\"], data.where(data.rollup_annotations_qc_results > 2, drop=True)['sea_water_practical_salinity'],\n",
    "        marker='o', linestyle=\"\", color=\"yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a597f34",
   "metadata": {},
   "source": [
    "Mask out identified bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98757c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CP03ISSM-RID26-07-NUTNRB000\n",
    "#mask = (data.time >= pd.to_datetime(\"2018-07-25 12:00:00\")) & (data.time <= pd.to_datetime(\"2018-10-26 19:52:00\"))\n",
    "#data[\"rollup_annotations_qc_results\"][mask] = 4\n",
    "\n",
    "#mask = (data.time >= pd.to_datetime(\"2022-08-17 00:00:00\")) & (data.time <= pd.to_datetime(\"2022-11-13 13:55:00\"))\n",
    "#data[\"rollup_annotations_qc_results\"][mask] = 4\n",
    "\n",
    "# # CP04OSSM-RID26-07-NUTNRB000\n",
    "#mask = (data.time >= pd.to_datetime(\"2020-07-25\")) & (data.time <= pd.to_datetime(\"2020-11-08 13:13:00\"))\n",
    "#data[\"rollup_annotations_qc_results\"][mask] = 4\n",
    "\n",
    "# # GI01SUMO-RID16-07-NUTNRB000\n",
    "#mask = (data.time >= pd.to_datetime(\"2019-02-07\")) & (data.time <= pd.to_datetime(\"2019-08-09 08:04:00\"))\n",
    "#data[\"rollup_annotations_qc_results\"][mask] = 4\n",
    "\n",
    "#mask = (data.sea_water_practical_salinity < 34.0)\n",
    "#data[\"rollup_annotations_qc_results\"][mask] = 4\n",
    "\n",
    "# # GI01SUMO-SBD11-08-NUTNRB000\n",
    "#mask = (data.corrected_nitrate_concentration > 40)\n",
    "#data[\"rollup_annotations_qc_results\"][mask] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef819f",
   "metadata": {},
   "source": [
    "Select just the \"good\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7facbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data = data.where(data.rollup_annotations_qc_results <= 3, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebde49",
   "metadata": {},
   "source": [
    "Lastly, clean out any duplicate time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = np.unique(good_data['time'], return_index=True)\n",
    "good_data = good_data.isel(time=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf619b",
   "metadata": {},
   "source": [
    "---\n",
    "## Gross Range\n",
    "The Gross Range QARTOD test consists of two parameters: a fail range which indicates when the data is bad, and a suspect range which indicates when data is either questionable or interesting. The fail range values are set based upon the instrument/measurement and associated calibration. For example, the conductivity sensors are calibration for measurements between 0 (freshwater) and 9 (highly-saline waters). The suspect range values are calculated based on the mean of the available data $\\pm$3$\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.gross_range import GrossRange\n",
    "from ooi_data_explorations.qartod.plotting import *\n",
    "from ooi_data_explorations.qartod.qc_processing import format_gross_range, format_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e082c06",
   "metadata": {},
   "source": [
    "#### Test Parameters & Sensor Ranges\n",
    "Map out the data variables in the data set to the data stream inputs and the associated sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fa238",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters = {\n",
    "    \"nitrate_concentration\": [-2, 3000],\n",
    "    \"corrected_nitrate_concentration\": [-1.5, 3000],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0002b",
   "metadata": {},
   "source": [
    "**Argentine Basin**: Need to subset the glider data to only get the observations that are at comparable depths to the mooring instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84282160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the glider_data\n",
    "good_data = merged_gliders.where((merged_gliders.depth < 5) & (merged_gliders.depth > 0), drop=True)\n",
    "good_data[\"nitrate_concentration\"] = good_data.sci_suna_nitrate_um\n",
    "good_data[\"corrected_nitrate_concentration\"] = good_data.sci_suna_nitrate_um"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb761e",
   "metadata": {},
   "source": [
    "**Calculate the Gross Range Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094f167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "site, node, sensor = refdes.split(\"-\", 2)\n",
    "gross_range_table = pd.DataFrame()\n",
    "\n",
    "\n",
    "for param in test_parameters:\n",
    "    sensor_range = test_parameters.get(param)\n",
    "    if param == \"corrected_nitrate_concentration\":\n",
    "        inp = \"salinity_corrected_nitrate\"\n",
    "    else:\n",
    "        inp = param\n",
    "    \n",
    "    if param in good_data.variables:\n",
    "        print(f\"##### Calculating gross range for {param} #####\")\n",
    "        # Check if there is enough data\n",
    "        if len(good_data[param].dropna(dim=\"time\")) < 100:\n",
    "            user_range = [np.nan, np.nan]\n",
    "            source = \"Not enough data to calculate user range.\"\n",
    "        else:\n",
    "            gross_range = GrossRange(sensor_range[0], sensor_range[1])\n",
    "            gross_range.fit(good_data, param, check_normality=True)\n",
    "            user_range = [gross_range.suspect_min, gross_range.suspect_max]\n",
    "            source = gross_range.source\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            qc_dict = format_gross_range(inp, sensor_range, user_range, site, node, sensor, stream, source)\n",
    "            gross_range_table = gross_range_table.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # Plot the result\n",
    "        try:\n",
    "            fig, ax = plot_gross_range(good_data, param, gross_range)\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            if ymin < sensor_range[0]:\n",
    "                ymin = sensor_range[0]\n",
    "            if ymax > sensor_range[1]:\n",
    "                ymax = sensor_range[1]\n",
    "            ax.set_ylim((ymin, ymax))\n",
    "            #ax.set_ylim((200, 600))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1408aab",
   "metadata": {},
   "source": [
    "**Add the stream name and the source comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table['notes'] = ('User range based on data collected through {}.'.format(\"2021-01-01\"))\n",
    "gross_range_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961583f",
   "metadata": {},
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588287f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in gross_range_table.index:\n",
    "    print(gross_range_table.loc[ind][\"qcConfig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c26c92",
   "metadata": {},
   "source": [
    "**Save the gross range table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table.to_csv(f\"../results/gross_range/{refdes}.csv\", index=False, columns=GR_HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22804cf5",
   "metadata": {},
   "source": [
    "---\n",
    "## Climatology\n",
    "For the climatology QARTOD test, First, we bin the data by month and take the mean. The binned-montly means are then fit with a 2 cycle harmonic via Ordinary-Least-Squares (OLS) regression. Ranges are calculated based on the 3$\\sigma$ calculated from the OLS-fitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_climatology_table(ds, param, tinp, zinp, sensor_range, depth_bins):\n",
    "    \"\"\"Function which calculates the climatology table based on the \"\"\"\n",
    "    \n",
    "    climatologyTable = pd.DataFrame()\n",
    "    \n",
    "    if depth_bins is None:\n",
    "        # Filter out the data outside the sensor range\n",
    "        m = (ds[param] > sensor_range[0]) & (ds[param] < sensor_range[1]) & (~np.isnan(ds[param]))\n",
    "        param_data = ds[param][m]\n",
    "        \n",
    "        # Fit the climatology for the selected data\n",
    "        pmin, pmax = [0, 0]\n",
    "        \n",
    "        try:\n",
    "            climatology = Climatology()\n",
    "            climatology.fit(param_data)\n",
    "\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Calculate the climatology data\n",
    "            vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "            vmin = np.floor(vmin*10000)/10000\n",
    "            for vind in vmin.index:\n",
    "                if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                    vmin[vind] = sensor_range[0]\n",
    "            vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "            for vind in vmax.index:\n",
    "                if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                    vmax[vind] = sensor_range[1]\n",
    "            vmax = np.floor(vmax*10000)/10000\n",
    "            vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "            vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "            # Build the climatology dataframe\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "        except:\n",
    "            # Here is where to create nans if insufficient data to fit\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Create a series filled with nans\n",
    "            vals = []\n",
    "            for i in np.arange(len(tspan)):\n",
    "                vals.append([np.nan, np.nan])\n",
    "            vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "            # Add to the data\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "            \n",
    "        del ds, vspan, tspan, zspan\n",
    "        gc.collect()\n",
    "        \n",
    "    else:        \n",
    "    # Iterate through the depth bins to calculate the climatology for each depth bin\n",
    "        for dbin in depth_bins:\n",
    "            # Get the pressure range to bin from\n",
    "            pmin, pmax = dbin[0], dbin[1]\n",
    "\n",
    "            # Select the data from the pressure range\n",
    "            bin_data = data.where((data[zinp] >= pmin) & (data[zinp] <= pmax), drop=True)\n",
    "\n",
    "            # sort based on time and make sure we have a monotonic dataset\n",
    "            bin_data = bin_data.sortby('time')\n",
    "            _, index = np.unique(bin_data['time'], return_index=True)\n",
    "            bin_data = bin_data.isel(time=index)\n",
    "\n",
    "            # Filter out the data outside the sensor range\n",
    "            m = (bin_data[param] > sensor_range[0]) & (bin_data[param] < sensor_range[1]) & (~np.isnan(bin_data[param]))\n",
    "            param_data = bin_data[param][m]\n",
    "\n",
    "            # Fit the climatology for the selected data\n",
    "            try:\n",
    "                climatology = Climatology()\n",
    "                climatology.fit(param_data)\n",
    "\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Calculate the climatology data\n",
    "                vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "                vmin = np.floor(vmin*10000)/10000\n",
    "                for vind in vmin.index:\n",
    "                    if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                        vmin[vind] = sensor_range[0]\n",
    "                vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "                vmax = np.floor(vmax*10000)/10000\n",
    "                for vind in vmax.index:\n",
    "                    if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                        vmax[vind] = sensor_range[1]\n",
    "                vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "                vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "                # Build the climatology dataframe\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            except:\n",
    "                # Here is where to create nans if insufficient data to fit\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Create a series filled with nans\n",
    "                vals = []\n",
    "                for i in np.arange(len(tspan)):\n",
    "                    vals.append([np.nan, np.nan])\n",
    "                vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "                # Add to the data\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            del climatology, bin_data, vspan, tspan, zspan\n",
    "            gc.collect()\n",
    "    \n",
    "    return climatologyTable, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b5289",
   "metadata": {},
   "source": [
    "**Get the depth bins and filter based on max depth.** <br>\n",
    "For the ```SUNAs``` which are only deployed on Surface Moorings, there is no depth bins needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c427519",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bins = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ef241",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the climatology lookup table\n",
    "climatologyLookup = pd.DataFrame()\n",
    "\n",
    "# Setup the Table Header\n",
    "TBL_HEADER = [\"[1,1]\",\"[2,2]\",\"[3,3]\",\"[4,4]\",\"[5,5]\",\"[6,6]\",\"[7,7]\",\"[8,8]\",\"[9,9]\",\"[10,10]\",\"[11,11]\",\"[12,12]\"]\n",
    "\n",
    "# Set the subsite-node-sensor\n",
    "subsite, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "# Iterate through the parameters\n",
    "for param in test_parameters:\n",
    "    if param in good_data.variables:\n",
    "        if param == \"corrected_nitrate_concentration\":\n",
    "            inp = \"salinity_corrected_nitrate\"\n",
    "        else:\n",
    "            inp = param\n",
    "        \n",
    "        # ----------------- Depth tables ---------------------\n",
    "        # Get the sensor range of the parameter to test\n",
    "        print(f\"##### Calculating climatology for {param} #####\")\n",
    "        sensor_range = test_parameters.get(param)\n",
    "        # Set the sensor range to something more reasonable, i.e. \n",
    "        if param == 'nitrate_concentration':\n",
    "            sensor_range = (-2.0, 3000)\n",
    "        else:\n",
    "            sensor_range = (-1.5, 3000)\n",
    "        \n",
    "        # Generate the climatology table with the depth bins\n",
    "        climatologyTable, climatology = make_climatology_table(good_data, param, \"time\", \"depth\", sensor_range, depth_bins)\n",
    "        \n",
    "        # Get the variance\n",
    "        try:\n",
    "            variance = float(np.round(climatology.regression['variance_explained']*100, 1))\n",
    "        except:\n",
    "            variance = 0.0\n",
    "\n",
    "        # Create the tableName\n",
    "        tableName = f\"{refdes}-{param}.csv\"\n",
    "        \n",
    "        # Save the results\n",
    "        climatologyTable.to_csv(f\"../results/climatology/climatology_tables/{tableName}\", header=TBL_HEADER)\n",
    "        \n",
    "        # ------------------ Lookup tables ------------------\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            if stream == \"metbk_hourly\":\n",
    "                pass\n",
    "            else:\n",
    "                qc_dict = {\n",
    "                    \"subsite\": subsite,\n",
    "                    \"node\": node,\n",
    "                    \"sensor\": sensor,\n",
    "                    \"stream\": stream,\n",
    "                    \"parameters\": {\n",
    "                        \"inp\": inp,\n",
    "                        \"tinp\": \"time\",\n",
    "                        \"zinp\": \"depth\",\n",
    "                    },\n",
    "                    \"climatologyTable\": f\"climatology_tables/{refdes}-{param}.csv\",\n",
    "                    \"source\": f\"The variance explained by the climatology model is {variance}%.\",\n",
    "                    \"notes\": \"Climatology based on available data through 2021-01-01.\"\n",
    "                }\n",
    "                # Append to the lookup table\n",
    "                climatologyLookup = climatologyLookup.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the climatology ------------------\n",
    "        if good_data[param].time.size > 100000:\n",
    "            try:\n",
    "                subset = sorted(np.random.choice(good_data.time, 100000, replace=False))\n",
    "                subset_data = good_data.sel(time=subset)\n",
    "                fig, ax = plot_climatology(subset_data, param, climatology)\n",
    "                #ax.set_ylim((sensor_range[0], sensor_range[1]))\n",
    "                del subset, subset_data\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                fig, ax = plot_climatology(good_data, param, climatology)\n",
    "                #ax.set_ylim((sensor_range[0], sensor_range[1]))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399e7c4",
   "metadata": {},
   "source": [
    "**Check the last climatologyTable for reasonableness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be46c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5305a",
   "metadata": {},
   "source": [
    "**Check the climatologyLookup table that all the entries made it in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c659d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a061455",
   "metadata": {},
   "source": [
    "**Save the climatologyLookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lookup table results\n",
    "climatologyLookup.to_csv(f\"../results/climatology/{refdes}.csv\", index=False, columns=CLM_HEADER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
