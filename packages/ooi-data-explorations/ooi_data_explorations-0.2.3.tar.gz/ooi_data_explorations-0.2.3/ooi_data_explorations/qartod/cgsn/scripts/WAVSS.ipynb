{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cfadf9",
   "metadata": {},
   "source": [
    "# WAVSS\n",
    "\n",
    "### Purpose\n",
    "The purpose of this notebook is to calculate the gross range user min/max values for populating QARTOD parameter tables for data streams for OOI - CGSN data streams for the TRIAXYS Nex Wave (WAVSS) sensor. The WAVSS measures three-dimensional acceleration, rotation rate, and orientation in the Earth's magnetic field. \n",
    "\n",
    "The WAVSS datasets are split into five types of datastreams: statistics, motion, fourier, non-directional, and mean directional. The motion datastreams contain the buoy displacement values (north, east, heave). The fourier datastreams contain the fourier coefficients computed for both the directional and non-directional wave spectra, along with associated frequencies, frequency spacing, number of bands, etc. The non-directional datastreams contain the PSDs and associated frequencies used to compute the bulk wave statistics. The mean directional datastreams contain the parameters for mean direction of the wave field, the spread direction, and the PSDs with associated frequencies used to compute those parameters. The statistics datastreams contain the bulk wave parameters: significant wave height; peak wave period; mean wave direction; wave spread; and other associated parameters. \n",
    "\n",
    "It is the _bulk_wave_statistics_ datastream that we are interested in testing, since that has the science-relevant parameters.\n",
    "  \n",
    "### Test Parameters\n",
    "\n",
    "| Variable | Description | Range | Motion or Spectral Product |\n",
    "|----------|-------|------------|----------|\n",
    "| average_wave_height | Average zero down-crossing wave height (m) | [0, 40] | Motion |\n",
    "| max_wave_height | Maximum zero down-crossing wave height (trough to peak) (m) | [0, 40] | Motion |\n",
    "| mean_direction | mean direction of wave field against true north | [0, 360] | Directional |\n",
    "| mean_spectral_period | | [1.5, 33] | Directional |\n",
    "| mean_spread | Mean directional spread of wave field (units: degrees) | [0, 90] | Directional |\n",
    "| mean_wave_period | average wave period | [1.5, 33] | Motion |\n",
    "| number_zero_crossings | Number of zero crossings in the underlying displacement data, used primarily for verification and QC. | NA | Motion |\n",
    "| peak_wave_period | Peak wave period; period where wave spectrum has its maximum | [1.5, 33] | Directional |\n",
    "| significant_wave_period | Significant wave period, average period of the Hsig waves. | [1.5, 33] | Motion |\n",
    "| significant_wave_height | Significant wave height, average height of the highest third of the waves (Hsig) | [0, 40] | Motion |\n",
    "| wave_height_10 | Average height of the highest tenth (H10) of the waves | [0, 40] | Motion |\n",
    "| wave_height_hmo | Significant wave height estimated by an alternate method based on spectral moments (zeroth moment method) | [0, 40] | Directional |\n",
    "| wave_period_10 | Average period of the H10 waves | [1.5, 33] | Motion |\n",
    "| wave_period_tp5 | Peak wave period computed via Read method; an alternative to TP | [1.5, 33] | Directional |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666742bd",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d43f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, datetime, pytz, re, yaml\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cea47",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox\n",
    "This toolbox is publicly available at https://github.com/reedan88/OOINet. It should be cloned onto your machine and the setup instructions followed before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OOINet M2M tool\n",
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M\n",
    "from ooinet.utils import convert_time, ntp_seconds_to_datetime, unix_epoch_time\n",
    "from ooinet.Instrument.common import process_file, add_annotation_qc_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2047e42",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox\n",
    "This toolbox is publicly available at https://github.com/oceanobservatories/ooi-data-explorations. Similarly to the ```ooinet``` toolbox above, it should be installed onto your machine following the setup instructions before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.common import get_annotations, get_vocabulary, load_gc_thredds\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_wavss import wavss_datalogger\n",
    "from ooi_data_explorations.qartod.qc_processing import identify_blocks, create_annotations, process_gross_range, \\\n",
    "    process_climatology, parse_qc, inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e9f7",
   "metadata": {},
   "source": [
    "#### Import plotting and visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67501483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4cbb4",
   "metadata": {},
   "source": [
    "**Import the test parameters with ranges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sensor_ranges.yaml\") as stream:\n",
    "    sensor_ranges = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ecadd",
   "metadata": {},
   "source": [
    "---\n",
    "## Identify Data Streams\n",
    "This section is necessary to identify all of the data stream associated with a specific instrument. This can be done by querying UFrame and iteratively walking through all of the API endpoints. The results are saved into a csv file so this step doesn't have to be repeated each time.\n",
    "\n",
    "First, set the instrument to search for using OOI terminology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = \"WAVSS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6eedd",
   "metadata": {},
   "source": [
    "### Query OOINet for Data Streams <br>\n",
    "First check if the datasets have already been downloaded; if not, use the ```M2M.search_datasets``` tool to search the OOINet API and return a table of all of the available datasets for the given instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b295fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = pd.read_csv(\"../data/WAVSS_datasets.csv\")\n",
    "except:\n",
    "    datasets = M2M.search_datasets(instrument=\"WAVSS\", English_names=True)\n",
    "    # Save the datasets\n",
    "    datasets.to_csv(\"../data/WAVSS_datasets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de8866",
   "metadata": {},
   "source": [
    "Separate out the CGSN datasets from the EA datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cgsn = datasets[\"array\"].apply(lambda x: True if x.startswith((\"CP\",\"GA\",\"GI\",\"GP\",\"GS\")) else False)\n",
    "datasets = datasets[cgsn]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0bac",
   "metadata": {},
   "source": [
    "---\n",
    "## Single Reference Designator\n",
    "The reference designator acts as a key for an instrument located at a specific location. First, select a reference designator (refdes) to request data from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_designators = sorted(datasets[\"refdes\"])\n",
    "print(\"Number of reference designators: \" + str(len(reference_designators)))\n",
    "for refdes in reference_designators:\n",
    "    print(refdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb66114",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "refdes = reference_designators[k]\n",
    "print(refdes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa03f4",
   "metadata": {},
   "source": [
    "#### Sensor Vocab\n",
    "The vocab provides information about the instrument model and type, its location (with descriptive names), depth, and manufacturer. Get the vocab for the given reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4386f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f248210",
   "metadata": {},
   "source": [
    "#### Sensor Deployments\n",
    "Download the deployment information for the selected reference designator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf043193",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d996fb",
   "metadata": {},
   "source": [
    "#### Sensor Data Streams\n",
    "Next, select the specific data streams for the given reference designator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51398c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6c3ad",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata \n",
    "The metadata contains the following important key pieces of data for each reference designator: **method**, **stream**, **particleKey**, and **count**. The method and stream are necessary for identifying and loading the relevant dataset. The particleKey tells us which data variables in the dataset we should be calculating the QARTOD parameters for. The count lets us know which dataset (the recovered instrument, recovered host, or telemetered) contains the most data and likely has the best record to use to calculate the QARTOD tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a489e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a4e3e",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters for calculating the relevant QARTOD test limits. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we through several steps to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_science_parameters(metadata):\n",
    "    \"\"\"This function returns the science parameters for each datastream\"\"\"\n",
    "    \n",
    "    def filter_parameter_ids(pdId, pid_dict):\n",
    "        data_level = pid_dict.get(pdId)\n",
    "        if data_level is not None:\n",
    "            if data_level > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Filter the parameters for processed science parameters\n",
    "    data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "    mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "    metadata = metadata[mask]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def filter_metadata(metadata):\n",
    "    science_vars = filter_science_parameters(metadata)\n",
    "    # Next, eliminate the optode temperature from the stream\n",
    "    mask = science_vars[\"particleKey\"].apply(lambda x: False if \"temp\" in x else True)\n",
    "    science_vars = science_vars[mask]\n",
    "    science_vars = science_vars.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "    science_vars = science_vars.reset_index()\n",
    "    science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "    science_vars = science_vars.explode(column=\"particleKey\")\n",
    "    return science_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12987f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#science_vars = filter_science_parameters(metadata)\n",
    "science_vars = metadata.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "science_vars = science_vars.reset_index()\n",
    "science_vars = science_vars.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251d157",
   "metadata": {},
   "source": [
    "**Filter data streams for bulk wave statistics**<br>\n",
    "The WAVSS returns a lot of different data products, including spectral products, fourier series, and raw motion data. We are interested in testing _only_ the bulk wave statistics datastreams, which have the relevant science wave parameters such as signficant wave height, wave period, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = science_vars[\"stream\"].apply(lambda x: True if \"stat\" in x else False)\n",
    "science_vars = science_vars[mask]\n",
    "science_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16045e17",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data\n",
    "When calculating the QARTOD data tables, we only want to utilize the most complete data record available for a given reference designator. We do this by getting all the available data streams, loading the data, and then combining them into a single dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_overlaps(ds, deployments):\n",
    "    \"\"\"Trim overlapping deployment data (necessary to use xr.open_mfdataset)\"\"\"\n",
    "    # --------------------------------\n",
    "    # Second, get the deployment times\n",
    "    deployments = deployments.sort_values(by=\"deploymentNumber\")\n",
    "    deployments = deployments.set_index(keys=\"deploymentNumber\")\n",
    "    # Shift the start times by (-1) \n",
    "    deployEnd = deployments[\"deployStart\"].shift(-1)\n",
    "    # Find where the deployEnd times are earlier than the deployStart times\n",
    "    mask = deployments[\"deployEnd\"] > deployEnd\n",
    "    # Wherever the deployEnd times occur after the shifted deployStart times, replace those deployEnd times\n",
    "    deployments[\"deployEnd\"][mask] = deployEnd[mask]\n",
    "    deployments[\"deployEnd\"] = deployments[\"deployEnd\"].apply(lambda x: pd.to_datetime(x))\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # With the deployments info, can write a preprocess function to filter \n",
    "    # the data based on the deployment number\n",
    "    depNum = np.unique(ds[\"deployment\"])\n",
    "    deployInfo = deployments.loc[depNum]\n",
    "    deployStart = deployInfo[\"deployStart\"].values[0]\n",
    "    deployEnd = deployInfo[\"deployEnd\"].values[0]\n",
    "    \n",
    "    # Select the dataset data which falls within the specified time range\n",
    "    ds = ds.sel(time=slice(deployStart, deployEnd))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def preprocess(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = wavss_datalogger(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3a62f",
   "metadata": {},
   "source": [
    "#### Identify the bulk wave statistics datastreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd29f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = datastreams[\"stream\"].apply(lambda x: True if \"stat\" in x else False)\n",
    "datastreams[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the available datasets\n",
    "for index in datastreams[mask].index:\n",
    "    # Get the method and stream\n",
    "    method = datastreams.loc[index][\"method\"]\n",
    "    stream = datastreams.loc[index][\"stream\"]\n",
    "\n",
    "    # Get the URL - first try the goldCopy thredds server\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "    # Clean the catalog\n",
    "    catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Get the links to the THREDDs server and load the data\n",
    "    dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "    if method == \"telemetered\":\n",
    "        tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess, parallel=True)\n",
    "    elif method == \"recovered_host\":\n",
    "        host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            host_data = xr.open_mfdataset(host_files, preprocess=preprocess, parallel=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d902c1",
   "metadata": {},
   "source": [
    "**Combine the datasets into a single dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6169975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_datasets(tele_data, host_data, None, None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9586fac",
   "metadata": {},
   "source": [
    "---\n",
    "## Process the WAVSS\n",
    "The WAVSS data needs reprocessing before we can generate the qartod tables. This includes removing data identified by annotations as bad and filtering out data that, based on HITL review, is outside of the expected instrument calibration ranges. Once these steps are done, then the cleaned up datasets can be used to generate more-accurate QARTOD tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630bcf8",
   "metadata": {},
   "source": [
    "#### Annotations\n",
    "The annotations associated with a specific reference designator may contain relevant information on the performance or reliability of the data for a given dataset. The annotations are downloaded from OOINet as a json and processed into a pandas dataframe. Each annotation may apply to the entire dataset, to a specific stream, or to a specific variable. With the downloaed annotations, we can use the information contained in the ```qcFlag``` column to translate the annotations into QC flags, which can then be used to filter out bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = M2M.get_annotations(refdes)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751f313",
   "metadata": {},
   "source": [
    "Look at some individual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.loc[2][\"annotation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a0b36",
   "metadata": {},
   "source": [
    "Pass in the annotations and the dataset to add the annotation ```qcFlag``` values to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_annotation_qc_flag(data, annotations)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f23d23",
   "metadata": {},
   "source": [
    "Plot some of the data with the annotations on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf13a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable(ds, param, add_deployments=True):\n",
    "    \"\"\"Function to plot the timeseries with deployment info.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: (xarray.Dataset)\n",
    "        An xarray dataset downloaded from OOINet\n",
    "    param: (str)\n",
    "        The parameter name of the data variable in the OOI\n",
    "        dataset to plot\n",
    "    add_deployments: (boolean)\n",
    "        Also plot deployment information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig, ax: (matplotlib figs)\n",
    "        Figure and axis handles for the matplotlib image\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate the figure bounds\n",
    "    yavg = ds[param].mean()\n",
    "    ystd = ds[param].std()\n",
    "    ymed = np.nanmedian(ds[param])\n",
    "    # Need to check for way out-of-bounds values by comparison with median\n",
    "    if ystd > ymed:\n",
    "        yavg = ymed\n",
    "        ystd = ymed*0.2\n",
    "    ymin = yavg - 4*ystd\n",
    "    ymax = yavg + 4*ystd\n",
    "    \n",
    "    # Generate the plot figure\n",
    "    if add_deployments:\n",
    "        s = ds.plot.scatter(\"time\", param, ax=ax, hue=\"deployment\", hue_style=\"discrete\")\n",
    "    else:\n",
    "        s = ds.plot.scatter(\"time\", param, ax=ax)\n",
    "        \n",
    "    # Add in limits and labels\n",
    "    #ax.set_ylim((ymin, ymax))\n",
    "    xlabel = ax.get_xlabel()\n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ds[param].attrs[\"long_name\"], fontsize=14)\n",
    "    ax.set_title(ds.attrs[\"id\"], fontsize=16)\n",
    "    ax.grid()\n",
    "    \n",
    "    # Add in legend if deployments added\n",
    "    if add_deployments:\n",
    "        ax.legend(edgecolor=\"black\", loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=12, title=\"Deployments\")\n",
    "        deployments = np.unique(ds[\"deployment\"])\n",
    "        for depNum in deployments:\n",
    "            dt = ds.where(ds[\"deployment\"] == depNum, drop=True)[\"time\"].min()\n",
    "            ax.vlines(dt.values, yavg-4*ystd, yavg+4*ystd)\n",
    "            ax.text(dt.values, yavg-3*ystd, str(int(depNum)), fontsize=14, weight=\"bold\")\n",
    "            \n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = \"wave_height_hmo\"\n",
    "\n",
    "fig, ax = plot_variable(data, param)\n",
    "\n",
    "# Plot the annotations\n",
    "pqc = param + \"_qc_summary_flag\"\n",
    "qc3 = (data[pqc] == 3)\n",
    "qc4 = (data[pqc] > 3)\n",
    "ax.plot(data.time[qc3], data[param][qc3], marker=\".\", linestyle=\"\", color=\"yellow\")\n",
    "ax.plot(data.time[qc4], data[param][qc4], marker=\".\", linestyle=\"\", color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa1ba5",
   "metadata": {},
   "source": [
    "Use the added ```rollup_annotations_qc_results``` values to filter out bad or suspect (```rollup_annotations_qc_results``` value of 3, 4, or 9) data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7facbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.where(data.rollup_annotations_qc_results <= 3, drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf621caa",
   "metadata": {},
   "source": [
    "#### Limit the data to data collected before 2021-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a675547",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = np.unique(data['time'], return_index=True)\n",
    "data = data.isel(time=index)\n",
    "data = data.sel(time=slice('2014-01-01T00:00:00', \"2023-01-01T00:00:00\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf619b",
   "metadata": {},
   "source": [
    "---\n",
    "## Gross Range\n",
    "The Gross Range QARTOD test consists of two parameters: a fail range which indicates when the data is bad, and a suspect range which indicates when data is either questionable or interesting. The fail range values are set based upon the instrument/measurement and associated calibration. For example, the conductivity sensors are calibration for measurements between 0 (freshwater) and 9 (highly-saline waters). The suspect range values are calculated based on the mean of the available data $\\pm$3$\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.gross_range import GrossRange\n",
    "from ooi_data_explorations.qartod.plotting import *\n",
    "from ooi_data_explorations.qartod.qc_processing import format_gross_range, format_climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e082c06",
   "metadata": {},
   "source": [
    "#### Test Parameters & Sensor Ranges\n",
    "The sensor ranges are stored in a yaml file. Load those and get the test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ea5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters = sensor_ranges[\"test_parameters\"]\n",
    "test_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb761e",
   "metadata": {},
   "source": [
    "**Calculate the Gross Range Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094f167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "site, node, sensor = refdes.split(\"-\", 2)\n",
    "gross_range_table = pd.DataFrame()\n",
    "\n",
    "for param in test_parameters:\n",
    "    sensor_range = test_parameters.get(param)\n",
    "    \n",
    "    # Set the input parameter name\n",
    "    inp = param\n",
    "    \n",
    "    # Clean up the parameter based on the QC flag\n",
    "    param_qc = param + \"_qc_summary_flag\"\n",
    "    \n",
    "    if param in data.variables:\n",
    "        print(f\"##### Calculating gross range for {param} #####\")\n",
    "        param_data = data.where((data[param_qc] < 3), drop=True)\n",
    "        # Check if there is enough data\n",
    "        if len(param_data[param].dropna(dim=\"time\")) < 100:\n",
    "            user_range = [np.nan, np.nan]\n",
    "            source = \"Not enough data to calculate user range.\"\n",
    "        else:\n",
    "            gross_range = GrossRange(sensor_range[0], sensor_range[1])\n",
    "            gross_range.fit(param_data, param, check_normality=True)\n",
    "            user_range = [gross_range.suspect_min, gross_range.suspect_max]\n",
    "            source = gross_range.source\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            qc_dict = format_gross_range(inp, sensor_range, user_range, site, node, sensor, stream, source)\n",
    "            gross_range_table = gross_range_table.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # Plot the result\n",
    "        try:\n",
    "            fig, ax = plot_gross_range(param_data, param, gross_range)\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            if ymin < sensor_range[0]:\n",
    "                ymin = sensor_range[0]\n",
    "            if ymax > sensor_range[1]:\n",
    "                ymax = sensor_range[1]\n",
    "            ax.set_ylim((ymin, ymax))\n",
    "            #ax.set_ylim((200, 600))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbb1db",
   "metadata": {},
   "source": [
    "**Add the stream name and the source comments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_range_table['notes'] = ('User range based on data collected through {}.'.format(\"2021-01-01\"))\n",
    "gross_range_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7110bf",
   "metadata": {},
   "source": [
    "**Check the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588287f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in gross_range_table.index:\n",
    "    print(gross_range_table.loc[ind][\"qcConfig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b6973",
   "metadata": {},
   "source": [
    "**Save the gross range table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "gross_range_table.to_csv(f\"../results/gross_range/{refdes}.csv\", index=False, columns=GR_HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e013d60",
   "metadata": {},
   "source": [
    "---\n",
    "## Climatology\n",
    "For the climatology QARTOD test, First, we bin the data by month and take the mean. The binned-montly means are then fit with a 2 cycle harmonic via Ordinary-Least-Squares (OLS) regression. Ranges are calculated based on the 3$\\sigma$ calculated from the OLS-fitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ooi_data_explorations.qartod.climatology import Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_climatology_table(ds, param, tinp, zinp, sensor_range, depth_bins):\n",
    "    \"\"\"Function which calculates the climatology table based on the \"\"\"\n",
    "    \n",
    "    climatologyTable = pd.DataFrame()\n",
    "    \n",
    "    if depth_bins is None:\n",
    "        # Filter out the data outside the sensor range\n",
    "        m = (ds[param] > sensor_range[0]) & (ds[param] < sensor_range[1]) & (~np.isnan(ds[param]))\n",
    "        param_data = ds[param][m]\n",
    "        \n",
    "        # Fit the climatology for the selected data\n",
    "        pmin, pmax = [0, 0]\n",
    "        \n",
    "        try:\n",
    "            climatology = Climatology()\n",
    "            climatology.fit(param_data)\n",
    "\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Calculate the climatology data\n",
    "            vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "            vmin = np.floor(vmin*10000)/10000\n",
    "            for vind in vmin.index:\n",
    "                if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                    vmin[vind] = sensor_range[0]\n",
    "            vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "            for vind in vmax.index:\n",
    "                if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                    vmax[vind] = sensor_range[1]\n",
    "            vmax = np.floor(vmax*10000)/10000\n",
    "            vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "            vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "            # Build the climatology dataframe\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "        except:\n",
    "            # Here is where to create nans if insufficient data to fit\n",
    "            # Create the depth index\n",
    "            zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "            # Create the monthly bins\n",
    "            tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "            # Create a series filled with nans\n",
    "            vals = []\n",
    "            for i in np.arange(len(tspan)):\n",
    "                vals.append([np.nan, np.nan])\n",
    "            vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "            # Add to the data\n",
    "            climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "            \n",
    "        del ds, vspan, tspan, zspan\n",
    "        gc.collect()\n",
    "        \n",
    "    else:        \n",
    "    # Iterate through the depth bins to calculate the climatology for each depth bin\n",
    "        for dbin in depth_bins:\n",
    "            # Get the pressure range to bin from\n",
    "            pmin, pmax = dbin[0], dbin[1]\n",
    "\n",
    "            # Select the data from the pressure range\n",
    "            bin_data = data.where((data[zinp] >= pmin) & (data[zinp] <= pmax), drop=True)\n",
    "\n",
    "            # sort based on time and make sure we have a monotonic dataset\n",
    "            bin_data = bin_data.sortby('time')\n",
    "            _, index = np.unique(bin_data['time'], return_index=True)\n",
    "            bin_data = bin_data.isel(time=index)\n",
    "\n",
    "            # Filter out the data outside the sensor range\n",
    "            m = (bin_data[param] > sensor_range[0]) & (bin_data[param] < sensor_range[1]) & (~np.isnan(bin_data[param]))\n",
    "            param_data = bin_data[param][m]\n",
    "\n",
    "            # Fit the climatology for the selected data\n",
    "            try:\n",
    "                climatology = Climatology()\n",
    "                climatology.fit(param_data)\n",
    "\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Calculate the climatology data\n",
    "                vmin = climatology.monthly_fit - climatology.monthly_std*3\n",
    "                vmin = np.floor(vmin*10000)/10000\n",
    "                for vind in vmin.index:\n",
    "                    if vmin[vind] < sensor_range[0] or vmin[vind] > sensor_range[1]:\n",
    "                        vmin[vind] = sensor_range[0]\n",
    "                vmax = climatology.monthly_fit + climatology.monthly_std*3\n",
    "                vmax = np.floor(vmax*10000)/10000\n",
    "                for vind in vmax.index:\n",
    "                    if vmax[vind] < sensor_range[0] or vmax[vind] > sensor_range[1]:\n",
    "                        vmax[vind] = sensor_range[1]\n",
    "                vdata = pd.Series(data=zip(vmin, vmax), index=vmin.index).apply(lambda x: [v for v in x])\n",
    "                vspan = vdata.values.reshape(1,-1)\n",
    "\n",
    "                # Build the climatology dataframe\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            except:\n",
    "                # Here is where to create nans if insufficient data to fit\n",
    "                # Create the depth index\n",
    "                zspan = pd.interval_range(start=pmin, end=pmax, periods=1, closed=\"both\")\n",
    "\n",
    "                # Create the monthly bins\n",
    "                tspan = pd.interval_range(0, 12, closed=\"both\")\n",
    "\n",
    "                # Create a series filled with nans\n",
    "                vals = []\n",
    "                for i in np.arange(len(tspan)):\n",
    "                    vals.append([np.nan, np.nan])\n",
    "                vspan = pd.Series(data=vals, index=tspan).values.reshape(1,-1)\n",
    "\n",
    "                # Add to the data\n",
    "                climatologyTable = climatologyTable.append(pd.DataFrame(data=vspan, columns=tspan, index=zspan))\n",
    "\n",
    "            del climatology, bin_data, vspan, tspan, zspan\n",
    "            gc.collect()\n",
    "    \n",
    "    return climatologyTable, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b5289",
   "metadata": {},
   "source": [
    "**Get the depth bins and filter based on max depth.** <br>\n",
    "For the ```WAVSS``` which are only deployed on Surface Moorings, there is no depth bins needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c427519",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_bins = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ef241",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the climatology lookup table\n",
    "climatologyLookup = pd.DataFrame()\n",
    "\n",
    "# Setup the Table Header\n",
    "TBL_HEADER = [\"[1,1]\",\"[2,2]\",\"[3,3]\",\"[4,4]\",\"[5,5]\",\"[6,6]\",\"[7,7]\",\"[8,8]\",\"[9,9]\",\"[10,10]\",\"[11,11]\",\"[12,12]\"]\n",
    "\n",
    "# Set the subsite-node-sensor\n",
    "subsite, node, sensor = refdes.split(\"-\", 2)\n",
    "\n",
    "# Iterate through the parameters\n",
    "for param in test_parameters:\n",
    "    # ----------------- Drop bad data from the parameter ---------------------\n",
    "    param_qc = param + \"_qc_summary_flag\"\n",
    "    param_data = data.where(data[param_qc] < 3, drop=True)\n",
    "    \n",
    "    if param in data.variables:\n",
    "        inp = param\n",
    "        # ----------------- Depth tables ---------------------\n",
    "        # Get the sensor range of the parameter to test\n",
    "        print(f\"##### Calculating climatology for {param} #####\")\n",
    "        sensor_range = test_parameters.get(param)\n",
    "        \n",
    "        # Generate the climatology table with the depth bins\n",
    "        climatologyTable, climatology = make_climatology_table(param_data, param, \"time\", \"depth\", sensor_range, depth_bins)\n",
    "        \n",
    "        # Get the variance\n",
    "        try:\n",
    "            variance = float(np.round(climatology.regression['variance_explained']*100, 1))\n",
    "        except:\n",
    "            variance = 0.0\n",
    "\n",
    "        # Create the tableName\n",
    "        tableName = f\"{refdes}-{param}.csv\"\n",
    "        \n",
    "        # Save the results\n",
    "        climatologyTable.to_csv(f\"../results/climatology/climatology_tables/{tableName}\", header=TBL_HEADER)\n",
    "        \n",
    "        # ------------------ Lookup tables ------------------\n",
    "        # Check which streams have the param in it\n",
    "        streams = metadata[metadata[\"particleKey\"] == inp][\"stream\"].unique()\n",
    "        for stream in streams:\n",
    "            qc_dict = {\n",
    "                \"subsite\": subsite,\n",
    "                \"node\": node,\n",
    "                \"sensor\": sensor,\n",
    "                \"stream\": stream,\n",
    "                \"parameters\": {\n",
    "                    \"inp\": inp,\n",
    "                    \"tinp\": \"time\",\n",
    "                    \"zinp\": \"depth\",\n",
    "                },\n",
    "                \"climatologyTable\": f\"climatology_tables/{refdes}-{param}.csv\",\n",
    "                \"source\": f\"The variance explained by the climatology model is {variance}%.\",\n",
    "                \"notes\": \"Climatology based on available data through 2021-01-01.\"\n",
    "            }\n",
    "            # Append to the lookup table\n",
    "            climatologyLookup = climatologyLookup.append(qc_dict, ignore_index=True)\n",
    "            \n",
    "        # ------------------ Plot the climatology ------------------\n",
    "        if param_data[param].time.size > 100000:\n",
    "            try:\n",
    "                subset = sorted(np.random.choice(param_data.time, 100000, replace=False))\n",
    "                subset_data = param_data.sel(time=subset)\n",
    "                fig, ax = plot_climatology(subset_data, param, climatology)\n",
    "                #ax.set_ylim((sensor_range[0], sensor_range[1]))\n",
    "                del subset, subset_data\n",
    "                gc.collect()\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                fig, ax = plot_climatology(param_data, param, climatology)\n",
    "                #ax.set_ylim((sensor_range[0], sensor_range[1]))\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399e7c4",
   "metadata": {},
   "source": [
    "**Check the last climatologyTable for reasonableness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5305a",
   "metadata": {},
   "source": [
    "**Check the climatologyLookup table that all the entries made it in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b34d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "climatologyLookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a061455",
   "metadata": {},
   "source": [
    "**Save the climatologyLookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e94f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lookup table results\n",
    "climatologyLookup.to_csv(f\"../results/climatology/{refdes}.csv\", index=False, columns=CLM_HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fc2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
