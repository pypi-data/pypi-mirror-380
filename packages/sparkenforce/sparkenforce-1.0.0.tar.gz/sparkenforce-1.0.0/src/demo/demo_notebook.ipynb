{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "from sparkenforce import validate, infer_dataframe_annotation, DataFrameValidationError\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", date(1991, 4, 1), 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", date(2000, 5, 19), 4000),\n",
    "    (\"Robert\", \"\", \"Williams\", date(1978, 9, 5), 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", date(1967, 12, 1), 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", date(1980, 2, 17), -1),\n",
    "]\n",
    "\n",
    "schema = \"firstname string, middlename string, lastname string, dob date, salary int\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+\n",
      "|firstname|middlename|lastname|       dob|salary|\n",
      "+---------+----------+--------+----------+------+\n",
      "|    James|          |   Smith|1991-04-01|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|    -1|\n",
      "+---------+----------+--------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 0: Get an annotation from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[\"firstname\": str, \"middlename\": str, \"lastname\": str, \"dob\": date, \"salary\": int]\n"
     ]
    }
   ],
   "source": [
    "print(infer_dataframe_annotation(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Successful return value validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation successful!\n",
      "+-------+------+\n",
      "|   name|length|\n",
      "+-------+------+\n",
      "|  James|     5|\n",
      "|Michael|     7|\n",
      "| Robert|     6|\n",
      "|  Maria|     5|\n",
      "|    Jen|     3|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "\n",
    "@validate\n",
    "def transform_data(df: DataFrame[\"firstname\":str, ...]) -> DataFrame[\"name\":str, \"length\":int]:\n",
    "    \"\"\"Function that validates both input and output.\"\"\"\n",
    "    return df.select(df.firstname.alias(\"name\"), fn.length(df.firstname).alias(\"length\"))\n",
    "\n",
    "\n",
    "# This should work correctly\n",
    "result = transform_data(df)\n",
    "print(\"✅ Validation successful!\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Error due to incorrect schema in return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expected error in return validation:\n"
     ]
    },
    {
     "ename": "DataFrameValidationError",
     "evalue": "return value columns mismatch. missing required columns: {'length', 'name'}, unexpected columns: {'lastname', 'firstname'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataFrameValidationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DataFrameValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Expected error in return validation:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# This should fail\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     result = \u001b[43mincorrect_return_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m❌ Should not reach here\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DataFrameValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:106\u001b[39m, in \u001b[36mvalidate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m return_annotation = hints.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    103\u001b[39m     return_annotation,\n\u001b[32m    104\u001b[39m     _TypedDataFrame,\n\u001b[32m    105\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43m_validate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_annotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn value\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:152\u001b[39m, in \u001b[36m_validate_dataframe\u001b[39m\u001b[34m(value, hint, argument_name)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m extra:\n\u001b[32m    151\u001b[39m             msg_parts.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munexpected columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m DataFrameValidationError(\n\u001b[32m    153\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns mismatch. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(msg_parts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    154\u001b[39m         )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    156\u001b[39m     missing = required_columns - columns\n",
      "\u001b[31mDataFrameValidationError\u001b[39m: return value columns mismatch. missing required columns: {'length', 'name'}, unexpected columns: {'lastname', 'firstname'}"
     ]
    }
   ],
   "source": [
    "@validate\n",
    "def incorrect_return_schema(df: DataFrame[\"firstname\":str, ...]) -> DataFrame[\"name\":str, \"length\":int]:\n",
    "    \"\"\"Function that returns an incorrect schema.\"\"\"\n",
    "    return df.select(\"firstname\", \"lastname\")  # Incorrect columns\n",
    "\n",
    "\n",
    "# This should fail\n",
    "try:\n",
    "    result = incorrect_return_schema(df)\n",
    "    print(\"❌ Should not reach here\")\n",
    "except DataFrameValidationError as e:\n",
    "    print(\"✅ Expected error in return validation:\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Error due to returning incorrect type (not DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expected error - not DataFrame:\n"
     ]
    },
    {
     "ename": "DataFrameValidationError",
     "evalue": "return value must be a PySpark DataFrame, got <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataFrameValidationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DataFrameValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Expected error - not DataFrame:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# This should fail\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     result = \u001b[43mnon_dataframe_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m❌ Should not reach here\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DataFrameValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:106\u001b[39m, in \u001b[36mvalidate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m return_annotation = hints.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    103\u001b[39m     return_annotation,\n\u001b[32m    104\u001b[39m     _TypedDataFrame,\n\u001b[32m    105\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43m_validate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_annotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn value\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:127\u001b[39m, in \u001b[36m_validate_dataframe\u001b[39m\u001b[34m(value, hint, argument_name)\u001b[39m\n\u001b[32m    125\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidating \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m with hint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataFrameValidationError(\n\u001b[32m    128\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be a PySpark DataFrame, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    129\u001b[39m     )\n\u001b[32m    131\u001b[39m columns = \u001b[38;5;28mset\u001b[39m(value.columns)\n\u001b[32m    132\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mDataFrameValidationError\u001b[39m: return value must be a PySpark DataFrame, got <class 'str'>"
     ]
    }
   ],
   "source": [
    "@validate\n",
    "def non_dataframe_return(df: DataFrame[\"firstname\":str, ...]) -> DataFrame[\"result\":str]:\n",
    "    \"\"\"Function that returns something that is not a DataFrame.\"\"\"\n",
    "    return \"Not a DataFrame\"\n",
    "\n",
    "\n",
    "# This should fail\n",
    "try:\n",
    "    result = non_dataframe_return(df)\n",
    "    print(\"❌ Should not reach here\")\n",
    "except DataFrameValidationError as e:\n",
    "    print(\"✅ Expected error - not DataFrame:\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: No return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No annotation: Can return anything\n",
      "✅ With explicit None: 42\n"
     ]
    }
   ],
   "source": [
    "@validate\n",
    "def no_return_annotation(df: DataFrame[\"firstname\":str, ...]):\n",
    "    \"\"\"Function without return annotation - return is not validated.\"\"\"\n",
    "    return \"Can return anything\"\n",
    "\n",
    "\n",
    "@validate\n",
    "def explicit_none_return(df: DataFrame[\"firstname\":str, ...]) -> None:\n",
    "    \"\"\"Function with explicit None return - not validated.\"\"\"\n",
    "    return 42\n",
    "\n",
    "\n",
    "# Both should work without return validation\n",
    "result1 = no_return_annotation(df)\n",
    "result2 = explicit_none_return(df)\n",
    "print(f\"✅ No annotation: {result1}\")\n",
    "print(f\"✅ With explicit None: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Validation with ellipsis (minimum columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ellipsis validation successful!\n",
      "+---------+---------+--------+\n",
      "|firstname|  summary|lastname|\n",
      "+---------+---------+--------+\n",
      "|    James|processed|   Smith|\n",
      "|  Michael|processed|        |\n",
      "|   Robert|processed|Williams|\n",
      "+---------+---------+--------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "@validate\n",
    "def ellipsis_return_example(df: DataFrame[\"firstname\":str, ...]) -> DataFrame[\"firstname\":str, \"summary\":str, ...]:\n",
    "    \"\"\"Function that allows additional columns in the return.\"\"\"\n",
    "    # Add additional columns (allowed with ellipsis)\n",
    "    return df.select(\n",
    "        \"firstname\",\n",
    "        fn.lit(\"processed\").alias(\"summary\"),\n",
    "        \"lastname\",  # Additional column allowed\n",
    "        \"salary\",  # Another additional column allowed\n",
    "    )\n",
    "\n",
    "\n",
    "# This should work correctly\n",
    "result = ellipsis_return_example(df)\n",
    "print(\"✅ Ellipsis validation successful!\")\n",
    "result.select(\"firstname\", \"summary\", \"lastname\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: Validation with both Python and Spark types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Date (Python) validation successful!\n",
      "✅ Date (Spark) validation successful!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "@validate\n",
    "def validation_py_date(df: DataFrame[\"firstname\":str, \"dob\":date, ...]) -> DataFrame[\"firstname\":str, \"age\":int]:\n",
    "    \"\"\"Function that validates date and calculates age.\"\"\"\n",
    "    return df.select(\n",
    "        \"firstname\",\n",
    "        (fn.year(fn.now()) - fn.year(df.dob)).alias(\"age\"),\n",
    "    )\n",
    "\n",
    "\n",
    "result = validation_py_date(df)\n",
    "print(\"✅ Date (Python) validation successful!\")\n",
    "\n",
    "\n",
    "@validate\n",
    "def validation_spark_date(\n",
    "    df: DataFrame[\"firstname\":str, \"dob\" : DateType(), ...],\n",
    ") -> DataFrame[\"firstname\":str, \"age\":int]:\n",
    "    \"\"\"Function that validates Spark date type and calculates age.\"\"\"\n",
    "    return df.select(\n",
    "        \"firstname\",\n",
    "        (fn.year(fn.current_date()) - fn.year(df.dob)).alias(\"age\"),\n",
    "    )\n",
    "\n",
    "\n",
    "result = validation_spark_date(df)\n",
    "print(\"✅ Date (Spark) validation successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7: using custom types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Name class to string validation successful!\n",
      "✅ DataFrameValidationError correctly raised for custom class with wrong type!\n"
     ]
    },
    {
     "ename": "DataFrameValidationError",
     "evalue": "return value column 'name' has incorrect type. Expected StringType(), got DateType()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataFrameValidationError\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DataFrameValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ DataFrameValidationError correctly raised for custom class with wrong type!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.select(\n\u001b[32m     28\u001b[39m         df.dob.alias(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     result = \u001b[43mcustom_class_example_nok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m❌ Should not reach here\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DataFrameValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:106\u001b[39m, in \u001b[36mvalidate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m return_annotation = hints.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    103\u001b[39m     return_annotation,\n\u001b[32m    104\u001b[39m     _TypedDataFrame,\n\u001b[32m    105\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43m_validate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_annotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn value\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:165\u001b[39m, in \u001b[36m_validate_dataframe\u001b[39m\u001b[34m(value, hint, argument_name)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hint.dtypes:\n\u001b[32m    164\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidating dtypes for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint.dtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[43m_validate_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/src/sparkenforce/__init__.py:231\u001b[39m, in \u001b[36m_validate_dtypes\u001b[39m\u001b[34m(df, expected_dtypes, argument_name)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for DataFrame column \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    227\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected a Spark DataType or a supported Python type.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    228\u001b[39m     )\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _types_compatible(actual_spark_type, expected_spark_type):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataFrameValidationError(\n\u001b[32m    232\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m column \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has incorrect type. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    233\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_spark_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_spark_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    234\u001b[39m     )\n",
      "\u001b[31mDataFrameValidationError\u001b[39m: return value column 'name' has incorrect type. Expected StringType(), got DateType()"
     ]
    }
   ],
   "source": [
    "from sparkenforce import register_type_mapping\n",
    "from dataclasses import dataclass\n",
    "from pyspark.sql import types as spark_types\n",
    "\n",
    "\n",
    "class Name: ...\n",
    "\n",
    "\n",
    "register_type_mapping(Name, spark_types.StringType())\n",
    "\n",
    "\n",
    "@validate\n",
    "def custom_class_example_ok(df) -> DataFrame[\"name\":Name]:\n",
    "    \"\"\"Function that uses dataclass to specify schema.\"\"\"\n",
    "    return df.select(\n",
    "        df.firstname.alias(\"name\"),\n",
    "    )\n",
    "\n",
    "\n",
    "result = custom_class_example_ok(df)\n",
    "print(\"✅ Name class to string validation successful!\")\n",
    "\n",
    "\n",
    "@validate\n",
    "def custom_class_example_nok(df) -> DataFrame[\"name\":Name]:\n",
    "    \"\"\"Function that uses dataclass to specify schema.\"\"\"\n",
    "    return df.select(\n",
    "        df.dob.alias(\"name\"),\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    result = custom_class_example_nok(df)\n",
    "    print(\"❌ Should not reach here\")\n",
    "except DataFrameValidationError as e:\n",
    "    print(\"✅ DataFrameValidationError correctly raised for custom class with wrong type!\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8: using custom types for structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataclass validation successful!\n",
      "+------------------+\n",
      "|name              |\n",
      "+------------------+\n",
      "|{James, Smith}    |\n",
      "|{Michael, }       |\n",
      "|{Robert, Williams}|\n",
      "|{Maria, Jones}    |\n",
      "|{Jen, Brown}      |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparkenforce import register_type_mapping\n",
    "from pyspark.sql import types as spark_types\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Name:\n",
    "    firstname: str\n",
    "    middlename: str = \"\"\n",
    "\n",
    "\n",
    "struct_type = spark_types.StructType(\n",
    "    [\n",
    "        spark_types.StructField(\"forename\", spark_types.StringType(), True),\n",
    "        spark_types.StructField(\"surname\", spark_types.StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "register_type_mapping(Name, struct_type)\n",
    "\n",
    "\n",
    "@validate\n",
    "def dataclass_example(df) -> DataFrame[\"name\":Name]:\n",
    "    \"\"\"Function that uses dataclass to specify schema.\"\"\"\n",
    "    return df.select(\n",
    "        fn.struct(\n",
    "            df.firstname.alias(\"forename\"),\n",
    "            df.lastname.alias(\"surname\"),\n",
    "        ).alias(\"name\"),\n",
    "    )\n",
    "\n",
    "\n",
    "result = dataclass_example(df)\n",
    "print(\"✅ Dataclass validation successful!\")\n",
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparkenforce (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
