import logging;
import traceback;
import from typing { Optional }
import from logging { Logger }
import from jivas.agent.action.action { Action }

import from jivas.agent.modules.data.serialization { convert_str_to_json }

import from jivas.agent.core.graph_node { GraphNode }

node ModelAction(Action) {
    # Generic model action node archetype for abstracted, standardized LLM interfacing

    # set up logger
    static has logger: Logger = logging.getLogger(__name__);

    has api_key: str = "";
    has model_name: str = "gpt-4o";
    has model_temperature: float = 0.7;
    has model_max_tokens: int = 2048;

    # abstract invoke ability for the model meant to be overidden with specifics of model call
    def invoke(prompt_messages:list, prompt_variables:dict, **kwargs:dict) -> Optional[ModelActionResult] abs;

    def call_model(
        prompt_messages: list,
        prompt_variables: dict,
        interaction_node: Optional[Interaction]=None,
        logging: bool=False,
        streaming: bool=False,
        **kwargs:dict
    ) -> Optional[ModelActionResult] {

        # add streaming flag in kwargs
        kwargs.update({"streaming":streaming});

        if (model_action_result := self.invoke(
            prompt_messages,
            prompt_variables,
        	interaction_node=interaction_node,
            **kwargs
        )) {
            if (interaction_node) {
                # add exported model action result to context data
                interaction_node.data_append(
                    key="ModelActionResult",
                    value=model_action_result
                );
                # update token tally
                interaction_node.add_tokens(
                    model_action_result.get_tokens()
                );
            }
            if (logging) {
                self.logger.info(model_action_result.get_result());
            }

            return model_action_result;
        }

        return None;
    }

    def healthcheck() -> Union[bool, dict] {
        if (self.model_name == "" or self.api_key == "") {
            return False;
        }

        test_prompt_messages = [{"system": "Output the result of 2 + 2"}];

        try  {
            if (model_action_result := self.call_model(
                prompt_messages=test_prompt_messages,
                prompt_variables={},
                model_name=self.model_name,
                model_temperature=self.model_temperature,
                model_max_tokens=self.model_max_tokens
            )) { # set the interaction message+
                interaction_message = model_action_result.get_result();
                if not interaction_message {
                    return False;
                } else {
                    return True;
                }
            }
            return False;
        } except Exception as e {
            self.logger.error(
                f"An exception occurred in {self.label}:\n{traceback.format_exc()}\n"
            );
            return False;
        }
    }

}

obj ModelActionResult(GraphNode) {

    # encapsulates a language model prompt, result and related tokens
    # this is meant to be instantiated with attr values and serve as the standard when interfacing with llms asyncronously

    has prompt: str = "";
    has functions: str = "";
    has result: str = "";
    has tokens: int = 0;
    has temperature: float = 0;
    has model_name: str = "";
    has max_tokens: int = 0;

    def get_prompt() {
        return self.prompt;
    }

    def get_functions() {
        return self.functions;
    }

    def get_result() {

        if (self.result) {
            return self.result;
        } else {
            return None;
        }

    }

    def get_json_result() {
        #  convert result to json and return it
        if (json_result := convert_str_to_json(self.result)) {
            return json_result;
        } else {
            return {};
        }
    }

    def get_tokens() {
        return self.tokens;
    }

    def get_max_tokens() {
        return self.max_tokens;
    }

    def get_temperature() {
        return self.temperature;
    }

    def get_model_name() {
        return self.model_name;
    }
}
