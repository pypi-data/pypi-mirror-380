"""SLM runtime for inference and benchmarking.

This module provides runtime support for Small Language Models,
including loading, inference, and performance benchmarking.
"""

import asyncio
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from ..core.config import Config
from .benchmark import SLMBenchmarker
from .loaders import SLMModelLoaderFactory
from .model import SLMModel

logger = logging.getLogger(__name__)


class SLMRuntime:
    """Runtime for Small Language Models."""

    def __init__(self, config: Config):
        """Initialize SLM runtime."""
        self.config = config
        self.loaded_models: Dict[str, SLMModel] = {}
        self.inference_cache: Dict[str, Any] = {}
        self.benchmarker = SLMBenchmarker()

    async def load_model(self, model_path: Union[str, Path]) -> SLMModel:
        """Load an SLM model from disk.

        Args:
            model_path: Path to the SLM model directory or file

        Returns:
            Loaded SLM model instance
        """
        model_path = Path(model_path)
        model_key = str(model_path.absolute())

        # Check if already loaded
        if model_key in self.loaded_models:
            logger.info(f"Using cached model: {model_path}")
            return self.loaded_models[model_key]

        logger.info(f"Loading SLM model from: {model_path}")

        # Get appropriate loader and load model
        loader = SLMModelLoaderFactory.get_loader(model_path)
        model = await loader.load_model(model_path)

        # Cache the loaded model
        self.loaded_models[model_key] = model

        logger.info(f"Successfully loaded SLM: {model.model_id}")
        return model

    async def generate(
        self,
        model: SLMModel,
        prompt: str,
        max_length: int = 100,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> str:
        """Generate text using the SLM model.

        Args:
            model: Loaded SLM model
            prompt: Input prompt
            max_length: Maximum generated length
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter

        Returns:
            Generated text
        """
        # Check cache first
        cache_key = f"{model.model_id}:{hash(prompt)}:{max_length}:{temperature}:{top_p}"
        if cache_key in self.inference_cache:
            logger.debug("Using cached inference result")
            cached_result = self.inference_cache[cache_key]
            return str(cached_result) if cached_result is not None else ""  # type: ignore[no-any-return]

        # Simulate text generation (placeholder implementation)
        start_time = time.time()

        # Mock generation based on model characteristics
        response = await self._mock_generation(model, prompt, max_length, temperature)

        end_time = time.time()
        generation_time = end_time - start_time

        logger.debug(f"Generated {len(response)} characters in {generation_time:.3f}s")

        # Cache the result
        self.inference_cache[cache_key] = response

        return response

    async def _mock_generation(
        self, model: SLMModel, prompt: str, max_length: int, temperature: float
    ) -> str:
        """Mock text generation for demonstration."""
        # Simulate processing time based on model size
        processing_time = max(0.1, model.parameters / 1e10)  # Scale with parameters
        await asyncio.sleep(min(processing_time, 2.0))  # Cap at 2 seconds

        # Generate mock response
        words = prompt.split()
        response_words = []

        # Simple word continuation based on compression ratio
        compression_ratio = model.metadata.get("slm_model", {}).get("compression_ratio", 0.5)
        quality_factor = compression_ratio  # Higher compression = lower quality

        for i in range(min(max_length // 5, 20)):  # Approximate word count
            if i < len(words):
                response_words.append(words[i])
            else:
                # Generate based on quality factor
                if quality_factor > 0.7:
                    response_words.append("word")
                else:
                    response_words.append("generated")

        return " ".join(response_words) + f" [Generated by SLM {model.model_id}]"

    async def benchmark(
        self, model: SLMModel, test_prompts: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Benchmark an SLM model's performance.

        Args:
            model: Loaded SLM model
            test_prompts: List of test prompts (uses default if None)

        Returns:
            Benchmark results
        """
        logger.info(f"Benchmarking model: {model.model_id}")
        return await self.benchmarker.benchmark(model, self.generate, test_prompts)

    def _estimate_memory_usage(self, model: SLMModel) -> Dict[str, int]:
        """Estimate memory usage for the model."""
        # Rough estimation based on model size and parameters
        model_size = model.size
        runtime_overhead = model_size * 0.2  # 20% overhead estimate

        return {
            "model_size_bytes": model_size,
            "runtime_overhead_bytes": int(runtime_overhead),
            "total_estimated_bytes": int(model_size + runtime_overhead),
            "total_estimated_mb": int((model_size + runtime_overhead) / (1024 * 1024)),
        }

    async def compare_models(self, models: List[SLMModel]) -> Dict[str, Any]:
        """Compare multiple SLM models."""
        logger.info(f"Comparing {len(models)} models")

        comparison_results: Dict[str, Any] = {"models": [], "comparison_metrics": {}}
        models_list: List[Dict[str, Any]] = comparison_results["models"]

        # Benchmark each model
        for model in models:
            benchmark_result = await self.benchmark(model)
            models_list.append(
                {"model_id": model.model_id, "benchmark": benchmark_result}
            )  # type: ignore[attr-defined]

        # Calculate comparison metrics
        if len(models) > 1:
            latencies = [
                result["benchmark"]["performance_metrics"]["avg_latency"] for result in models_list
            ]
            throughputs = [
                result["benchmark"]["performance_metrics"]["avg_throughput"]
                for result in models_list
            ]
            sizes = [model.size for model in models]

            comparison_results["comparison_metrics"] = {
                "fastest_model": models[latencies.index(min(latencies))].model_id,
                "highest_throughput": models[throughputs.index(max(throughputs))].model_id,
                "smallest_model": models[sizes.index(min(sizes))].model_id,
                "largest_model": models[sizes.index(max(sizes))].model_id,
                "latency_range": {"min": min(latencies), "max": max(latencies)},
                "throughput_range": {"min": min(throughputs), "max": max(throughputs)},
                "size_range": {"min": min(sizes), "max": max(sizes)},
            }

        return comparison_results

    def clear_cache(self) -> None:
        """Clear inference cache."""
        self.inference_cache.clear()
        logger.info("Inference cache cleared")

    def unload_model(self, model_path: Union[str, Path]) -> bool:
        """Unload a model from memory."""
        model_key = str(Path(model_path).absolute())

        if model_key in self.loaded_models:
            del self.loaded_models[model_key]
            logger.info(f"Unloaded model: {model_path}")
            return True

        return False

    def list_loaded_models(self) -> List[Dict[str, Any]]:
        """List all currently loaded models."""
        return [
            {
                "path": path,
                "model_id": model.model_id,
                "parameters": model.parameters,
                "loaded_time": model.loaded_time,
            }
            for path, model in self.loaded_models.items()
        ]
