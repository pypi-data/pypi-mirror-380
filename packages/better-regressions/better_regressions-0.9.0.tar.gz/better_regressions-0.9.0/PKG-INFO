Metadata-Version: 2.4
Name: better-regressions
Version: 0.9.0
Summary: Advanced regression methods with sklearn-like interface
License: MIT
License-File: LICENSE
Requires-Python: >=3.10
Requires-Dist: beartype>=0.10.0
Requires-Dist: build>=1.2.2.post1
Requires-Dist: dcor>=0.6
Requires-Dist: factor-analyzer>=0.5.1
Requires-Dist: ipykernel>=6.29.5
Requires-Dist: ipywidgets>=8.1.7
Requires-Dist: jaxtyping>=0.2.11
Requires-Dist: lightgbm>=4.6.0
Requires-Dist: loguru>=0.7.3
Requires-Dist: matplotlib>=3.10.1
Requires-Dist: networkx>=3.4.2
Requires-Dist: numpy>=1.20.0
Requires-Dist: pandas>=2.2.3
Requires-Dist: plotly>=6.1.2
Requires-Dist: rich>=14.0.0
Requires-Dist: scikit-learn>=1.0.0
Requires-Dist: scipy>=1.7.0
Requires-Dist: seaborn>=0.13.2
Requires-Dist: setuptools>=78.1.0
Requires-Dist: supersmoother>=0.4
Requires-Dist: tqdm>=4.67.1
Requires-Dist: twine>=6.1.0
Requires-Dist: xgboost>=3.0.0
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: isort>=5.12.0; extra == 'dev'
Description-Content-Type: text/markdown

# Better Regressions

Advanced regression methods with an sklearn-like interface.

## Current Features

- `Linear`:
  - Configurable regularization: Ridge with given `alpha` / BayesianRidge / ARD
  - "Better bias" option to properly regularize the intercept term
- `AdaptiveLinear`: Ridge regression with automatic shrinkage of features (like `ARDRegression`, but works in a different way and works better with correlated features)
- `Scaler`:
  - Configurable preprocessing: Standard scaling (by second moment) / Quantile transformation with uniform/normal output / Power transformation
  - `AutoScaler` to automatically select the best scaling method based on validation split
- `Smooth`: Boosting-based regression using smooth functions for features
  - `SuperSmoother`: Adaptive-span smoother for arbitrary complex functions.
  - `Angle`: Bagging of piecewise-linear functions, it's less flexible but because of that it's more robust to overfitting.
- `Soft`: Mixture of regressors based on quantile classification
- `Stabilize`: Robust scaling & clipping transformation for features/targets
- `AutoClassifier`: Classification with automatic model selection (LogisticRegression or XGBoost, with auto depth selection)
- `BinnedRegression`: Bins features and target, then trains a classifier. This way it can learn non-linear relationships and it also models the target distribution (not only its mean).
- `EDA`: Exploratory Data Analysis utilities
  - `plot_distribution`: Visualize sample distributions with fitted t-distribution parameters
  - `plot_trend`: Automatically detect and visualize relationships between variables + Pearson/Spearman correlation
    - For discrete features: Shows violin plots with distribution at each value
    - For continuous features: Fits trend lines with variance estimation and confidence intervals

## Installation

```bash
pip install better-regressions
```

## Basic Usage

```python
from better_regressions import auto_angle, auto_linear, Linear, Scaler, AutoClassifier
from better_regressions.eda import plot_distribution, plot_trend
from sklearn.datasets import make_regression, make_moons
import numpy as np

X, y = make_regression(n_samples=100, n_features=5, noise=0.1)
model = auto_angle(n_breakpoints=2)
model.fit(X, y)
y_pred = model.predict(X)
print(repr(model))

# Classification example
dataset = make_moons(n_samples=200, noise=0.3)
Xc, yc = dataset
clf = AutoClassifier(depth="auto")
clf.fit(Xc, yc)
yc_pred = clf.predict(Xc)

# EDA example
plot_distribution(y, name="Target Distribution")
plot_trend(X[:, 0], y, name="Feature 0 vs Target")
```

## Building new verison
1. Update `__version__` in `better_regressions/__init__.py` and `pyproject.toml`
2. `python -m build`
3. `python -m twine upload dist/*`