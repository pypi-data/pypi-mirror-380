import os
import numpy as np

# Assume CASA 6, CASA 5 is no longer built
from casatools import image
from casatasks import casalog
ia = image()

from collections import namedtuple
StopCodes = namedtuple('StopCodes', ['major', 'minor'])

class ImagingDict():
    """
    Class that constructs the tclean return dictionary - implemented
    specifically for the niter=0 case, to reproduce the dictionary generated in
    C++ without modifying the C++ code.

    Attributes:
    -----------
    residname       : Name of the input residual image (required)
    modelname       : Name of the input model image (optional)
    summaryminor    : The dictionary containing summaryminor
    retrec          : The full return dictionary

    Methods:
    --------

    image_dimensions(residname)
        Use the image() tool to query for the number of Stokes planes and
        frequency channels in the input image.

    fill_summary_minor(residname, modelname, channo, stokes, stokes_axis, freq_axis, fullsummary):
        Given the input image name, and the corresponding field, channel number, and
        Stokes plane, extract the relevant information from the image to generate a
        summaryminor dict as defined

    construct_summary_minor(self, paramList):
        Constructs and populates a nested dictionary containing the summaryMinor()
        information required by tclean.
        This is a duplicate of the existing summaryMinor() class in C++, but is
        being re-written here to avoid issues with modifying iterControl.

    construct_residual_dict(paramList):
        Construct the residual dictionary given the input image name and associated parameters. This residual dictionary is meant to
        duplicate that generated by imager.getSummary(fullsummary) for the special
    """

    def __init__(self) -> None:
        self._returndict = {}
        self._summaryminor = {}

        # These three parameters cannot be assigned, only accessed.
        # They reflect the values in the summaryminor key in the return dict.

        self._nfield = 0
        self._nstokes = 0
        self._nchan = 0

        self.residname = ''
        self.modelname = ''
        self.maskname = ''

        # namedtuple works better than a dict here
        # can do self.stopcode[0] or self.stopcode['major']
        # or self.stopcode.major
        self.stopcode = StopCodes(major=0, minor=0)

        self._initialize()

        self._summaryminor_keys = ['iterDone', 'peakRes', 'modelFlux', 'cycleThresh',
                                   'cycleStartIters', 'startIterDone',
                                   'startPeakRes', 'startModelFlux',
                                   'startPeakResNM', 'peakResNM', 'masksum',
                                   'mpiServer', 'stopCode']

    def __str__(self) -> str:
        """
        Pretty print the return dictionary.
        """

        retstr = ''
        for key, val in self._returndict.items():
            if key == 'summaryminor':
                retstr += 'summaryminor:\n'
                for field in val.keys():
                    for freq in val[field].keys():
                        for stokes in val[field][freq].keys():
                            for key2, val2 in val[field][freq][stokes].items():
                                retstr += f'\t Field: {field: 3d}, Chan: {freq: 3d}, Stokes: {stokes :2d}, {key2}, {val2}\n'
            else:
                retstr += f'{key}: {val}\n'
        return retstr

    @property
    def returndict(self) -> dict:
        return self._returndict

    @returndict.setter
    def returndict(self, inpdict: dict) -> None:
        self._returndict = inpdict

    @returndict.deleter
    def returndict(self) -> None:
        del self._returndict

    @property
    def nfield(self) -> int:
        self._nfield = len(self._returndict['summaryminor'].keys())
        return self._nfield

    @nfield.deleter
    def nfield(self) -> None:
        del self._nfield

    # XXX : This assumes that the number of channels is the same for all fields
    @property
    def nchan(self) -> int:
        self._nchan = 0
        for field in self._returndict['summaryminor'].keys():
            self._nchan = len(self._returndict['summaryminor'][field].keys())
            break

        return self._nchan

    @nchan.deleter
    def nchan(self) -> None:
        del self._nchan

    @property
    def nstokes(self) -> int:
        self._nstokes = 0
        for field in self._returndict['summaryminor'].keys():
            for freq in self._returndict['summaryminor'][field].keys():
                self._nstokes = len(self._returndict['summaryminor'][field][freq].keys())
                break

        return self._nstokes

    @nstokes.deleter
    def nstokes(self) -> None:
        del self._nstokes

    def _initialize(self) -> None:
        """
        Initialize the return dictionary with dummy values. This sets up the
        dictionary structure and keys to expect, and will be filled in with
        real values when running tclean or deconvolve.
        """

        # Initialize the values that don't need to inspect any images
        self._returndict['cleanstate'] = ''
        self._returndict['cyclefactor'] = 1.0
        self._returndict['cycleiterdone'] = 0
        self._returndict['cycleniter'] = 0
        self._returndict['cyclethreshold'] = 0
        self._returndict['interactiveiterdone'] = 0
        self._returndict['interactivemode'] = False
        self._returndict['interactiveniter'] = 0
        self._returndict['interactivethreshold'] = 0

        self._returndict['iterdone'] = 0
        self._returndict['loopgain'] = 0
        self._returndict['maxpsffraction'] = 0
        self._returndict['maxpsfsidelobe'] = 0
        self._returndict['minpsffraction'] = 0

        self._returndict['niter'] = 0
        self._returndict['nmajordone'] = 0
        self._returndict['nsigma'] = 0.0
        self._returndict['stopcode'] = 0

        self._returndict['summarymajor'] = np.array([])
        # Summary minor is nested as {field{freq{stokes}}
        # TODO : Figure out a better way to initialize the empty summary minor
        # without explicitly setting a field/stokes/channel number.
        self._returndict['summaryminor'] = {}
        self._returndict['summaryminor'][0] = {}
        self._returndict['summaryminor'][0][0] = {}
        self._returndict['summaryminor'][0][0][0] = self._initialize_summary_minor()

        self._returndict['threshold'] = 0.0
        self._returndict['stopDescription'] = 'Zero iterations performed'


    def _initialize_summary_minor(self) -> dict:
        """
        Initialize the summary minor dictionary with dummy values. This sets up the
        dictionary structure and keys to expect, and will be filled in with
        real values when running tclean or deconvolve.
        """

        self._summaryminor['iterDone'] = []
        self._summaryminor['peakRes'] = []
        self._summaryminor['modelFlux'] = []
        self._summaryminor['cycleThresh'] = []
        self._summaryminor['cycleStartIters'] = []
        self._summaryminor['startIterDone'] = []
        self._summaryminor['startPeakRes'] = []
        self._summaryminor['startModelFlux'] = []
        self._summaryminor['startPeakResNM'] = []
        self._summaryminor['peakResNM'] = []
        self._summaryminor['masksum'] = []
        self._summaryminor['mpiServer'] = []
        self._summaryminor['stopCode'] = []

        return self._summaryminor



    def get_key(self, key:str, field:int=0, chan:int=0, stokes:int=0) -> list:
        """
        Return the list of values for the specified key. If requesting a key
        within "summaryminor", also specify the field, channel and stokes plane.

        Inputs:
        key         The key to return. str
        field       The field to return. int
        chan        The channel to return. int
        stokes      The stokes plane to return. int

        Returns:
        The list of values for the specified key. list
        """

        try:
            if key in self._summaryminor_keys:
                return self._returndict['summaryminor'][field][chan][stokes][key]
            else:
                return self._returndict[key]
        except KeyError:
            print(f'WARNING : Key {key} not found in return dictionary.')
            return []


    @staticmethod
    def get_summaryminor_stopdesc(stopcode:int=None) -> str:
        """
        Given the summaryminor stopcode, return the stop description.

        Inputs:
        stopcode        The stopcode to return the description for. int

        Returns:
        stopdesc        The stop description. str
        """

        stopdesc = {0:'Skipped this channel/polarization. Zero mask.',
                    1:'Reached cycleniter',
                    2:'Reached cyclethreshold',
                    3:'Zero iterations performed',
                    4:'Possible divergence. Peak residual increased by 10% from minimum',
                    5:'Exited deconvolver minor cycle without reaching any stopping criterion',
                    6:'Reached n-sigma threshold'}

        if stopcode is None:
            return stopdesc

        return stopdesc[stopcode]



    def merge(self, tclean_dict: dict, deconv_dict: dict, filter_nonzero=True) -> dict:
        """
        Merge the return dictionaries from tclean and deconvolve.

        Inputs:
        tclean_dict     The return dictionary from deconvolve. dict
        deconv_dict     The return dictionary from tclean. dict
        filter_nonzero  If True, only update the non-zero keys from deconv_dict, default True

        Returns:
        merge_dict
        """

        merge_dict = tclean_dict.copy()

        for key, val in deconv_dict.items():
            if filter_nonzero:
                # Only copy the keys that are non-zero in deconvolve
                if isinstance(val, (int, float, complex)) and val == 0:
                    continue
                elif isinstance(val, list) and all(v == 0 for v in val):
                    continue
                elif isinstance(val, np.ndarray) and np.all(val == 0):
                    continue
            
            if key == 'summaryminor':
                # Copy summaryminor from deconvolve always
                merge_dict[key] = val
                # Copy all the iteration control keys from deconvolve
            elif key in ['iterdone', 'cycleiterdone', 'cycleniter', 'cyclethreshold',
                         'maxpsffraction', 'maxpsfsidelobe', 'minpsffraction', 'nsigma']:
                merge_dict[key] = val

        return merge_dict


    @staticmethod
    def concat_multifield_deconv(dict1:dict, dict2:dict) -> dict:
        """
        Concatenate the two input deconvolve dictionaries. Each input
        dictionary must be from an independent field ID, and must
        have non-overlapping field IDs. If the field IDs overlap exactly,
        then use ImagingDict.concat() instead.

        Inputs:
        dict1     The initial dictionary, dict
        dict2     The input dictionary to append. dict

        Returns:
        appendix  The concatenated dictionary. dict
        """

        appendix = dict1.copy()
        # All unique keys
        dict_keys = list(dict1.keys()) + [key for key in dict2.keys() if key not in dict1.keys()]

        for key in dict_keys:
            if key in ['iterdone', 'nmajordone', 'interactiveiterdone']:
                appendix[key] = dict1[key] + dict2[key]
            elif 'summaryminor' in key:
                # Concatenate the summaryminor dictionaries
                for field in dict2[key].keys():
                    if field not in appendix[key].keys():
                        appendix[key][field] = dict2[key][field]
            else:
                # Just use the key from dict2
                try:
                    appendix[key] = dict2[key]
                except KeyError:
                    # Key is only in dict1 and not dict2
                    appendix[key] = dict1[key]


        return appendix


    def concat(self, dict1:dict, dict2:dict) -> dict:
        """
        Concatenate the two input dictionaries. The input dictionary must be a
        fully formed return dictionary from either `tclean` or `deconvolve`.

        The keys within summaryminor are always concatenated.
        The following keys are incremented :
            iterdone
            nmajordone

        The rest of the keys are left unchanged, and default to the values in dict2.

        Inputs:
        dict1     The initial dictionary, dict
        dict2     The input dictionary to append. dict

        Returns:
        appendix  The concatenated dictionary. dict
        """

        appendix = dict1.copy()

        # All unique keys
        dict_keys = list(dict1.keys()) + [key for key in dict2.keys() if key not in dict1.keys()]

        for key in dict_keys:
            if key == 'stopcode':
                # Stopcode is a special case, so handled separately
                # Stopcode can either be a named tuple or an int.
                # If it's an int, it comes from deconvolve.
                # Update the minor key of the tuple, and merge that in
                if isinstance(dict1[key], StopCodes):
                    # If dict1[key] is a StopCodes named tuple, then update the minor key
                    # with the value from dict2[key]
                    appendix[key] = StopCodes(major=dict1[key].major, minor=dict2[key])
                elif isinstance(dict2[key], StopCodes):
                    # If dict2[key] is a StopCodes named tuple, then update the minor key
                    # with the value from dict1[key]
                    appendix[key] = StopCodes(major=dict2[key].major, minor=dict1[key])
                continue

            if key not in dict1.keys() and key not in dict2.keys():
                print("Internal Error : KEY mismatch : ", key)
                continue
            elif key in dict1 and key not in dict2:
                # Key is only in dict1 and not dict2
                appendix[key] = dict1[key]
                continue
            elif key in dict2 and key not in dict1:
                # Key is only in dict2 and not dict1
                appendix[key] = dict2[key]
                continue


            if key in ['iterdone', 'nmajordone']:
                appendix[key] = dict1[key] + dict2[key]
            elif isinstance(dict1[key], dict) and isinstance(dict2[key], dict):
                # If they are dicts, recurse
                appendix[key] = self.concat(dict1[key], dict2[key])
            elif isinstance(dict1[key], list) and isinstance(dict2[key], list):
                appendix[key] = dict1[key] + dict2[key]
            elif isinstance(dict1[key], np.ndarray) and isinstance(dict2[key], np.ndarray):
                # Using a list allows either key to be completely empty
                # numpy.concatenate breaks if one or both are length 0
                temparr = []
                temparr.extend(dict1[key])
                temparr.extend(dict2[key])
                appendix[key] = np.array(temparr)
            elif isinstance(dict1[key], (int, float, complex)) and isinstance(dict2[key], (int, float, complex)):
                appendix[key] = dict2[key]
            elif isinstance(dict1[key], str) and isinstance(dict2[key], str):
                # Just use the dict2 key here, these are keys like "cleanstate" etc.
                # That do not need to be concatenated.
                appendix[key] = dict2[key]
            else:
                # This should never happen
                raise KeyError(f"Key {key} not found in either dictionary.")

        return appendix



    def check_masksum(self, field:int=0, major_index:int=-1) -> int:
        """
        Checks if the masksum key in summaryminor is uniformly zero across all
        channels and Stokes planes.

        By default it checks the last major cycle.
        """

        masksum = 0
        for chan in range(self.nchan):
            for stokes in range(self.nstokes):
                masksum += self.get_key('masksum', field, chan, stokes)[major_index]

        return masksum


    def get_peakres(self, field:int=0, major_index:int=-1) -> float:
        """
        Calculates the peak residual across all channels and Stokes planes.

        By default it calculates the peak residual over the last major cycle.
        major_index directly indexes into the major cycle to query, so
        major_index=-2 will query the penultimate major cycle etc.

        Inputs:
        field               The field index over which to calculate the peak residual, int
        major_index         The major cycle index to query, int

        Returns:
        peakres             The peak residual, float
        """

        peakres = 0.0

        for chan in range(self.nchan):
            for stokes in range(self.nstokes):
                masksum = self.get_key('masksum', field, chan, stokes)[major_index]
                if masksum > 0:
                    peakres = max(peakres, self.get_key('peakRes', field, chan, stokes)[major_index])

        return peakres


    def check_convergence(self, niterleft=0, threshold=0, nmajorleft=0, cycleniter=None, imstats=None, peakres=None):
        """
        Check stopping criteria for convergence, based on the criteria specified here -
        https://casadocs.readthedocs.io/en/stable/notebooks/synthesis_imaging.html#Returned-Dictionary

        If peakres is provided, then prefer that over what is in the return
        dict. This is to account for the usage case where the user has modified
        the mask in between major cycles, and we need to update values of the
        peak residual in order to determine if we want to continue
        deconvolution.

        Note : The stopcode is a tuple of (stopcode_major, stopcode_minor)
        which is a difference from the regular tclean stopcodes. This was done
        to deal with the degeneracy in stopcode numbers, while still using the
        same definitions.
        """

        # Floating point tolerance, to maintain compatibility with CAS-11278 (and SIIterBot.cc)
        tol = 0.01

        # TODO : Implement StopCode 8
        # Note : For InteractiveClean, stopcode 3 is handled by the GUI for now
        # Stopcode 8 requires some work to figure out what nsigmathreshold is
        # and how it is calculated, so leaving that out for the moment.
        # NSigma is not exposed in the GUI as of 2023-12-15.
        # TODO : nsigmathreshold is now exposed to the GUI - 2025-06-15


        stopcode_maj_per_field = [0 for ff in range(self.nfield)]
        stopcode_min_per_field = [0 for ff in range(self.nfield)]
        stopdesc_per_field = ['' for ff in range(self.nfield)]

        #hasit = False # Has converged - global
        self.stopDescription = '' # Description of the stop code - global

        nmajordone = self.get_key('nmajordone')

        for fieldidx in range(self.nfield):
            stopcode_maj = 0
            stopcode_min = 0

            if imstats is not None:
                _masksum = imstats[fieldidx]['masksum']
                _peakres = imstats[fieldidx]['peakres']
            else:
                _masksum = self.check_masksum(field=fieldidx)
                _peakres = self.get_peakres(field=fieldidx)

            # Only stop on cycleniter if it is set
            stop_on_cycleniter = False
            if cycleniter != None:
                stop_on_cycleniter = True

            # Minor cycle stop codes
            if stop_on_cycleniter and cycleniter <= 0:
                stopcode_min = 1
                self.stopDescription = 'Reached the cycle iteration limit'

            # Major cycle stopcodes
            if niterleft<=0:
                stopcode_maj = 1
                self.stopDescription = 'Reached the iteration limit'
            elif (nmajorleft != -1 and nmajorleft==0):
                stopcode_maj = 9
                self.stopDescription = 'Reached the major cycle limit (nmajor)'
            #elif all([_masksum == 0 for _masksum in masksum_per_field]): # The ordering of masksum before peakres is significant
            elif _masksum == 0: # The ordering of masksum before peakres is significant
                # NOTE : Only stop if _all_ fields are zero mask
                stopcode_maj = 7
                self.stopDescription = 'Zero mask'
            #elif all([_peakres <= threshold for _peakres in peakres_per_field if _peakres is not None]):
            elif threshold > 0 and (_peakres <= threshold or np.abs(_peakres - threshold)/threshold < tol):
                # NOTE : Only stop if _all_ fields are below threshold
                stopcode_maj = 2
                self.stopDescription = 'Reached global stopping threshold (within mask)'
            # Stopcode 3 : Force Stop is handled by InteractiveClean GUI for now
            #elif nmajordone > 2 and np.allclose(peakres1, peakres2):
            #    stopcode_maj = 4
            #    self.stopDescription = 'No change in peak residual across consecutive major cycles'
            #elif peakres1 > 3*peakres2 and peakres2 != 0:
            #    stopcode = 5
            #    stopDescription = 'Peak residual increased by more than 3x across consecutive major cycles'
            #elif peakres1 > 3*min_peakres:
            #    stopcode = 6
            #    stopDescription = 'Peak residual increased by more than 3x from the minimum reached'

            stopcode_maj_per_field[fieldidx] = stopcode_maj
            stopcode_min_per_field[fieldidx] = stopcode_min
            stopdesc_per_field[fieldidx] = self.stopDescription

        # Cast as namedtuple
        # We want the minimum stopcode for the global values
        # since we want major cycles to continue if a single field
        # reaches convergence, but the others do not.
        # The per-field convergence should stop the converged
        # fields from continuing on.
        maj_idx = np.argmin(stopcode_maj_per_field)
        min_idx = np.argmin(stopcode_min_per_field)
        self.stopcode = StopCodes(major=stopcode_maj_per_field[maj_idx],
                                       minor=stopcode_min_per_field[min_idx])
        stopDescription = stopdesc_per_field[maj_idx]

        return  self.stopcode, stopDescription, stopcode_maj_per_field, stopcode_min_per_field, stopdesc_per_field


        ##if imstats is not None:
        ##    for field in imstats:
        ##        _masksum = field['masksum']
        ##        _peakres = field['peakres']

        ##        masksum_per_field.append(_masksum)
        ##        peakres_per_field.append(_peakres)
        ##else:
        ##    masksum_per_field = [self.check_masksum(field=ff) for ff in range(self.nfield)]
        ##    peakres_per_field = [self.get_peakres(field=ff) for ff in range(self.nfield)]

        ##    _peakres = peakres
        ##else:
        ##    _peakres = self.get_peakres()
        ##if masksum !=None:
        ##    use_masksum = masksum      ## If the mask has been zero'd out and iterations have been skipped (i.e. no summaryminor).
        ##else:
        ##    use_masksum = self.check_masksum()    ## If iterations have happened, latest info will be in the summaryminor.

        ## Only stop on cycleniter if it is set
        #stop_on_cycleniter = False
        #if cycleniter != None:
        #    stop_on_cycleniter = True

        ## Minor cycle stop codes
        #if stop_on_cycleniter and cycleniter <= 0:
        #    stopcode_min = 1
        #    stopDescription = 'Reached the cycle iteration limit'

        ## Major cycle stopcodes
        #if niterleft<=0:
        #    stopcode_maj = 1
        #    stopDescription = 'Reached the iteration limit'
        #elif (nmajorleft != -1 and nmajorleft==0):
        #    stopcode_maj = 9
        #    stopDescription = 'Reached the major cycle limit (nmajor)'
        #elif all([_masksum == 0 for _masksum in masksum_per_field]): # The ordering of masksum before peakres is significant
        #    # NOTE : Only stop if _all_ fields are zero mask
        #    stopcode_maj = 7
        #    stopDescription = 'Zero mask'
        #elif all([_peakres <= threshold for _peakres in peakres_per_field if _peakres is not None]):
        #    # NOTE : Only stop if _all_ fields are below threshold
        #    stopcode_maj = 2
        #    stopDescription = 'Reached global stopping threshold (within mask)'

        ## Stopcode 3 : Force Stop is handled by InteractiveClean GUI for now
        ## TODO : If this is used to run iteration control for non-interactive clean, implement this
        ##elif nmajordone > 2 and np.allclose(peakres1, peakres2):
        ##    stopcode = 4
        ##    stopDescription = 'No change in peak residual across consecutive major cycles'
        ##elif peakres1 > 3*peakres2 and peakres2 != 0:
        ##    stopcode = 5
        ##    stopDescription = 'Peak residual increased by more than 3x across consecutive major cycles'
        ##elif peakres1 > 3*min_peakres:
        ##    stopcode = 6
        ##    stopDescription = 'Peak residual increased by more than 3x from the minimum reached'

        ## Cast as namedtuple
        #self.stopcode = StopCodes(major=stopcode_maj, minor=stopcode_min)

        #return  self.stopcode, stopDescription




    def image_dimensions(self):
        """
        Given the input image, uses the ia tool to query for the number of Stokes
        planes and frequency channels.

        Inputs:
        None

        Returns:
        nstokes         Number of Stokes planes in the image
        nfreq           Number of frequency channels in the image
        """
        ia = image()

        # At this point we've already checked residname exists
        ia.open(self.residname)
        csys = ia.coordsys()

        shape = ia.shape()

        #  Figure out which axis is which
        stokes_axis = csys.findaxisbyname('Stokes')
        freq_axis = csys.findaxisbyname('Frequency')

        nstokes = shape[stokes_axis]
        nfreq = shape[freq_axis]

        ia.close()

        return nstokes, nfreq, stokes_axis, freq_axis


    def fill_summary_minor(self, channo, stokes, stokes_axis, freq_axis, fullsummary):
        """
        Given the input image name, and the corresponding field, channel number, and
        Stokes plane, extract the relevant information from the image to generate a
        summaryminor dict as defined
        https://casadocs.readthedocs.io/en/stable/notebooks/synthesis_imaging.html#Minor-Cycle-Summary-Dictionary

        Inputs:
        channo          Channel number to query in the image, int
        stokes          Stokes plane to query in the image, int
        stokes_axis     The axis to index for Stokes, int
        freq_axis       The axis to index for frequency, int
        fullsummary     Construct a full summary, or only a subset, bool

        Returns:
        summaryparams     Dict containing the necessary (key:value) pairs
        """

        ia = image()

        if not os.path.exists(self.residname):
            raise FileNotFoundError(f'Residual image {self.residname} does not exist.')

        ia.open(self.residname)
        shape = ia.shape()

        if stokes_axis == 2 and freq_axis == 3:
            blc = [0, 0, stokes, channo]
            trc = [shape[0], shape[1], stokes, channo]
        elif stokes_axis == 3 and freq_axis == 2:
            blc = [0, 0, channo, stokes]
            trc = [shape[0], shape[1], channo, stokes]

        data = ia.getchunk(blc, trc, dropdeg=True)
        ia.close()

        # Get the mask if it exists
        if len(self.maskname) > 0 and os.path.exists(self.maskname):
            ia.open(self.maskname)
            mask = ia.getchunk(blc, trc, dropdeg=True)
            ia.close()
        else:
            mask = [] # No mask, so everything is unmasked

        # If model image exists, calc model flux, else set to 0
        model_sum = 0
        if os.path.exists(self.modelname):
            ia.open(self.modelname)
            model_data = ia.getchunk(blc, trc, dropdeg=True)
            model_sum = np.sum(model_data)
            ia.close()

        if len(mask) == 0:
            peak_resid = np.amax(data)
        else:
            peak_resid = np.amax(data*mask)

        if fullsummary:
            peak_resid_NM = np.amax(data)
            mask_sum = np.sum(mask)

        summaryparams = dict()
        # This entire function is only invoked in the special case of niter=0
        summaryparams['iterDone'] = [0.0,]
        summaryparams['peakRes'] = [peak_resid,]
        # model flux has to be zero because no iterations were performed
        summaryparams['modelFlux'] = [model_sum,]
        # No threshold because no deconvolution done
        summaryparams['cycleThresh'] = [0.0,]

        if fullsummary:
            summaryparams['cycleStartIters'] = [0.0,]
            summaryparams['startIterDone'] = [0.0,]
            summaryparams['startPeakRes'] = [peak_resid,]
            summaryparams['startModelFlux'] = [model_sum,]
            summaryparams['startPeakResNM'] = [peak_resid_NM,]
            summaryparams['peakResNM'] = [peak_resid_NM,]
            summaryparams['masksum'] = [mask_sum,]
            summaryparams['mpiServer'] = [0.0,]
            summaryparams['stopCode'] = [3,]


        return summaryparams


    def _validate_mask(self):
        """
        Check if maskname is a valid iamge, and if not, set it to an empty string.
        """

        if os.path.exists(self.maskname) and os.path.isdir(self.maskname):
            try:
                ia.open(self.maskname)
                ia.close()
            except RuntimeError:
                self.maskname = ''
        else:
            self.maskname = ''


    def construct_summary_minor(self, paramList):
        """
        Constructs and populates a nested dictionary containing the summaryMinor()
        information required by tclean.

        This is a duplicate of the existing summaryMinor() class in C++, but is
        being re-written here to avoid issues with modifying iterControl.

        Inputs:
        paramList        Object that contains all imaging parameters

        Returns:
        None
        """

        impars = paramList.allimpars
        decpars = paramList.alldecpars

        # Each field is stored as a different key in impars
        nfields = len(impars.keys())

        for ff in range(nfields):
            self.residname=impars[str(ff)]['imagename']+'.residual.tt0' if(os.path.exists(impars[str(ff)]['imagename']+'.residual.tt0')) else impars[str(ff)]['imagename']+'.residual'
            self.modelname=impars[str(ff)]['imagename']+'.model.tt0' if(os.path.exists(impars[str(ff)]['imagename']+'.model.tt0')) else impars[str(ff)]['imagename']+'.model'
            if(os.path.exists(impars[str(ff)]['imagename']+'.mask')):
                self.maskname=impars[str(ff)]['imagename']+'.mask'

            if len(decpars[str(ff)]['mask']) > 0 and os.path.exists(decpars[str(ff)]['mask']):
                self.maskname = decpars[str(ff)]['mask']
            elif os.path.exists(impars[str(ff)]['imagename']+'.mask') and os.path.isdir(impars[str(ff)]['imagename']+'.mask'):
                self.maskname = impars[str(ff)]['imagename']+'.mask'
            else:
                self.maskname = ''

            # Check that the derived mask name corresponds to a real mask on disk
            # Note : This only affects the tclean(niter=0) functionality
            self._validate_mask()

            fullsummary = decpars[str(ff)]['fullsummary']
            nstokes, nfreq, stokes_axis, freq_axis = self.image_dimensions()

            if ff not in self._returndict['summaryminor']:
                self._returndict['summaryminor'][ff] = dict()

            for cc in range(nfreq):
                if cc not in self._returndict['summaryminor'][ff]:
                    self._returndict['summaryminor'][ff][cc] = dict()

                for ss in range(nstokes):
                    if ss not in self._returndict['summaryminor'][ff][cc]:
                        self._returndict['summaryminor'][ff][cc][ss] = dict()

                    self._returndict['summaryminor'][ff][cc][ss] = self.fill_summary_minor(cc, ss, stokes_axis, freq_axis, fullsummary)


    def construct_residual_dict(self, paramList):
        """
        Construct the residual dictionary given the input image name and associated parameters. This residual dictionary is meant to
        duplicate that generated by imager.getSummary(fullsummary) for the special
        case of niter = 0 to avoid initializing the deconvolver.

        Inputs:
        paramList   The tclean inputs, defaults where not specified. dict

        Returns:
        returndict  The return dictionary with imaging statistics. dict
        """

        imagename = paramList.allimpars['0']['imagename']

        self.residname = imagename+'.residual.tt0' if(os.path.exists(imagename +'.residual.tt0')) else imagename +'.residual'
        do_summary_minor = True
        # Only fill summaryminor if the residual image exists
        if not os.path.exists(self.residname):
            do_summary_minor = False
            #raise FileNotFoundError(f'{residname} does not exist on disk. Cannot construct tclean return dictionary.')

        # Initialize the values that don't need to inspect any images
        self._returndict['cleanstate'] = 'running'
        self._returndict['cyclefactor'] = paramList.getAllPars()['cyclefactor']
        self._returndict['cycleiterdone'] = 0
        self._returndict['cycleniter'] = 0
        self._returndict['cyclethreshold'] = 0
        self._returndict['interactiveiterdone'] = 0
        self._returndict['interactivemode'] = paramList.alldecpars['0']['interactive']
        self._returndict['interactiveniter'] = 0
        self._returndict['interactivethreshold'] = 0

        self._returndict['iterdone'] = 0
        self._returndict['loopgain'] = 0
        self._returndict['maxpsffraction'] = 0
        self._returndict['maxpsfsidelobe'] = 0
        self._returndict['minpsffraction'] = 0

        self._returndict['niter'] = 0
        self._returndict['nmajordone'] = 1
        self._returndict['nsigma'] = 0.0
        # stopcode 3 --> Zero iterations performed
        self._returndict['stopcode'] = 3

        self._returndict['summarymajor'] = np.array([0,])

        if do_summary_minor:
            self.construct_summary_minor(paramList)

        self._returndict['threshold'] = paramList.getAllPars()['threshold']
        self._returndict['stopDescription'] = 'Zero iterations performed'


        return self._returndict


class ConvergenceResult:
    """
    Class to hold convergence results for all fields. Also some convenience 
    methods to access the results.

    Attributes:
    -----------

    Methods:
    --------

    """

    def __init__(self, nfields, fieldnames):
        """
        Initialize the convergence result with the correct number of outlier fields specified.
        The structure of the convergence result is a list-of-lists, each element of the outer
        list is a field. 

        Args:
            nfields: int, number of fields to initialize
        """

        # Initialize the convergence result with the correct number of outlier fields specified.
        # The structure of the convergence result is a list-of-lists, each element of the outer
        # list is a field. 
        #
        # The inner list is structured as
        #
        # self._convergence_result = [[None,None,None,None,None,{ 'chan': None, 'major': None }],]
        #                           ^^^^ ^^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^----->>> convergence info
        #                              |    | |     |    +----->>> Number of global iterations remaining for current run (niterleft)
        #                              |    | |     +---------->>> Number of major cycles remaining for current run (nmajorleft)
        #                              |    | +---------------->>> major cycles done for current run (nmajordone)
        #                              |    +------------------>>> tclean stopcode

        self.nfields = nfields
        self.fieldnames = fieldnames

        self._convergence_result = []
        self._initialize_convergence_result()

    def _initialize_convergence_result(self):
        """
        Initialize the convergence result with the correct number of outlier fields specified.
        The structure of the convergence result is a list-of-lists, each element of the outer
        list is a field. 

        The inner list is structured as

        self._convergence_result = [None,None,None,None,None,{'field_name':{'chan': None, 'major': None},}]
                                ^^^^ ^^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^----->>> convergence info - one dict of dicts per field
                                    |    | |     |    +----->>> Number of global iterations remaining for current run (niterleft)
                                    |    | |     +---------->>> Number of major cycles remaining for current run (nmajorleft)
                                    |    | +---------------->>> major cycles done for current run (nmajordone)
                                    |    +------------------>>> tclean stopcode
        """

        self._convergence_result.extend([None, None, None, None, None])

        field_dict = {}
        for nn in range(self.nfields):
            field_dict[self.fieldnames[nn]] = {'chan':None, 'major':None}

        self._convergence_result.append(field_dict)



    def update_params(self, msg, majordone, nmajor, niter):
        """
        Update iteration control parameters for every field in convergence record.

        Inputs:
        msg          : str, message to log
        majordone    : int, number of major cycles done
        nmajor       : int, number of major cycles in total
        niter        : int, number of iterations completed

        Returns:
            None
        """

        for nn in range(len(self._convergence_result)):
            self._convergence_result[nn][0] = msg
            self._convergence_result[nn][2] = majordone
            self._convergence_result[nn][3] = nmajor
            self._convergence_result[nn][4] = niter

    @property 
    def convergence_result(self):
        """
        Return the convergence result.

        Returns:
            list: convergence result, a list of lists, each element of the outer list is a field.
        """

        return self._convergence_result


    @convergence_result.setter
    def convergence_result(self, val):
        """
        Set the convergence result.

        Args:
            val: list, convergence result, a list of lists, each element of the outer list is a field.
        """

        self._convergence_result = val


    def append(self, val):
        """
        Append a new convergence result to the list.

        Args:
            val: list, convergence result, a list of lists, each element of the outer list is a field.
        """

        self._convergence_result.append(val)


    def extend(self, val):
        """
        Extend the convergence result with a new list of convergence results.

        Args:
            val: list, convergence result, a list of lists, each element of the outer list is a field.
        """

        self._convergence_result.extend(val)


    def update_convergence_chan_ret(self, global_imdict):
        """
        Accumulates the per-channel/stokes summaryminor keys across all major cycle calls so far.

        The "iterDone" key will be replaced with "iterations", and for the "iterations" key,
        the value in the returned cummulative record will be a rolling sum of iterations done
        for tclean calls so far, one value per minor cycle.
        For example, if there have been two clean calls, and in the first call channel 0 had
        [1] iteration in 1 minor cycle, and for the second call channel 0 had [6, 10, 9, 1]
        iterations in 4 minor cycles), then the resultant "iterations" key for channel 0 would be:
        [1, 7, 17, 26, 27]
        """

        keys = ['modelFlux', 'iterDone', 'peakRes', 'stopCode', 'cycleThresh']

        # Grab tuples of keys of interest
        outrec = {}
        for ff in range(self.nfields):
            outrec[self.fieldnames[ff]] = {}
            #outrec[ff] = {}
            for nn in range(global_imdict.nchan):
                outrec[self.fieldnames[ff]][nn] = {}
                for ss in range(global_imdict.nstokes):
                    outrec[self.fieldnames[ff]][nn][ss] = {}
                    for key in keys:
                        # Replace iterDone with iterations
                        if key == 'iterDone':
                            # Maintain cumulative sum of iterations per entry
                            outrec[self.fieldnames[ff]][nn][ss]['iterations'] = np.cumsum(global_imdict.get_key(key, field=ff, stokes=ss, chan=nn))
                            # Replace iterDone with iterations
                            #outrec[nn][ss]['iterations'] =  global_imdict.get_key(key, stokes=ss, chan=nn)
                        else:
                            outrec[self.fieldnames[ff]][nn][ss][key] = global_imdict.get_key(key, field=ff, stokes=ss, chan=nn)

        return outrec





