{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Textgrad Notebook\n",
    "\n",
    "You can either edit it via JupyterLab WebUI (run `harbor open textgrad` to access), or by connecting your IDE to the Jupyter server over the URL from `harbor url textgrad`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nvidia capability should be added by\n",
    "# compose.x.textgrad.nvidia.yml if the\n",
    "# Nvidia Container Toolkit is installed.\n",
    "from torch import cuda\n",
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a conversation:\\n\\n<CONVERSATION>{context}</CONVERSATION>\\n\\nThis conversation is potentially part of a larger system. The output is used as {response_desc}\\n\\nHere is the feedback we got for {variable_desc} in the conversation:\\n\\n<FEEDBACK>{feedback}</FEEDBACK>\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextGrad is available\n",
    "from textgrad import prompts\n",
    "prompts.GRADIENT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama Example\n",
    "from openai import OpenAI\n",
    "from textgrad.engine.local_model_openai_api import ChatExternalClient\n",
    "import textgrad as tg\n",
    "\n",
    "# Ollama is one of the default services in Harbor,\n",
    "# unless you changed it, should be available over this URL\n",
    "# You can obtain the URL via `harbor url --internal ollama`\n",
    "client = OpenAI(base_url=\"http://harbor.ollama:11434/v1\", api_key=\"sk-ollama\")\n",
    "engine = ChatExternalClient(client=client, model_string='llama3.1:8b-instruct-q8_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.set_backward_engine(engine, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The base indication of reasoning and cognition can be understood from various perspectives in philosophy, psychology, neuroscience, and artificial intelligence. Here's a comprehensive overview:\\n\\n**Philosophical Perspective:**\\n\\nIn philosophy, reasoning and cognition are often considered to arise from the interaction between perception, attention, memory, and inference. The ancient Greek philosopher Aristotle proposed that reasoning involves the use of syllogisms (logical arguments) to arrive at conclusions based on premises.\\n\\n**Cognitive Science Perspective:**\\n\\nFrom a cognitive science perspective, reasoning and cognition involve complex processes such as:\\n\\n1. **Perception**: processing sensory information from the environment.\\n2. **Attention**: selectively focusing on relevant stimuli or mental representations.\\n3. **Working Memory**: temporarily holding and manipulating information in mind.\\n4. **Long-term Memory**: storing and retrieving knowledge and experiences.\\n5. **Inference**: drawing conclusions based on rules, patterns, and associations.\\n\\n**Neural Perspective:**\\n\\nFrom a neural perspective, reasoning and cognition are thought to be supported by various brain regions and networks, including:\\n\\n1. **Prefrontal Cortex (PFC)**: involved in executive functions, decision-making, and working memory.\\n2. **Temporal Lobes**: play a key role in processing and storing auditory and visual information.\\n3. **Parietal Lobes**: contribute to spatial reasoning, attention, and working memory.\\n4. **Basal Ganglia**: involved in habit formation, motor control, and cognitive flexibility.\\n\\n**Artificial Intelligence Perspective:**\\n\\nIn artificial intelligence (AI), reasoning and cognition are often modeled using computational frameworks such as:\\n\\n1. **Symbolic AI**: representing knowledge using symbols, rules, and logical operations.\\n2. **Connectionist AI**: modeling cognition using neural networks inspired by the brain's structure and function.\\n3. **Hybrid Approaches**: combining symbolic and connectionist methods to leverage their strengths.\\n\\nIn summary, the base indication of reasoning and cognition involves a complex interplay between perception, attention, memory, inference, and various cognitive processes supported by different brain regions and computational frameworks.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.generate('What is the base indication of reasoning and cognition?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrad import Variable\n",
    "from textgrad.loss import TextLoss\n",
    "\n",
    "system_prompt = Variable(\"Evaluate the correctness of this sentence\", role_description=\"The system prompt\")\n",
    "loss = TextLoss(system_prompt, engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrad.optimizer import TextualGradientDescent\n",
    "\n",
    "x = Variable(\"A sntence with a typo\", role_description=\"The input sentence\", requires_grad=True)\n",
    "optimizer = TextualGradientDescent(parameters=[x], engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss(x)\n",
    "l.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following sentence contains a spelling mistake: A sntence with a typo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
