import base64
import json
import re
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Union

from strong_typing.core import JsonType
from strong_typing.schema import json_schema_type, register_schema
from strong_typing.serialization import object_to_json

from .dap_error import ProcessingError

JobID = str
ObjectID = str


@json_schema_type(schema={"type": "string", "format": "uri", "pattern": "^https?://"})  # type: ignore
@dataclass(frozen=True)
class URL:
    """
    A Uniform Resource Locator (URL).

    :param url: The URL string.
    """

    url: str

    @staticmethod
    def from_json(value: JsonType) -> "URL":
        if not isinstance(value, str):
            raise TypeError("Invalid input. The URL must be provided as a string.")

        return URL(value)

    def to_json(self) -> str:
        return self.url

    def __str__(self) -> str:
        return self.url


@json_schema_type(  # type: ignore
    examples=[
        {
            "schema": {
                "type": "object",
                "properties": {
                    "id": {
                        "type": "integer",
                        "format": "int64",
                        "title": "The ID of this version object. Primary key.",
                    },
                    "user_id": {
                        "type": "integer",
                        "format": "int64",
                        "title": "The ID of the submitter. Foreign key to `users.id`.",
                    },
                    "context_id": {
                        "type": "integer",
                        "format": "int64",
                        "title": "The ID of the course this submission belongs to. Foreign key to `courses.id`.",
                    },
                    "context_type": {
                        "type": "string",
                        "enum": ["Course"],
                        "title": "The type of the context object (typically `Course`).",
                    },
                },
                "additionalProperties": False,
                "required": ["id", "context_type"],
                "title": "This table stores information describing previous versions of individual submission objects.",
            },
            "version": 1,
        }
    ]
)
@dataclass(frozen=True)
class VersionedSchema:
    """
    The state of the schema at a specific point in time.

    Schemas are backwards compatible. They receive strictly monotonically increasing version numbers as schema
    evolution takes place.

    :param schema: The JSON Schema object to validate against.
    :param version: The version of the schema.
    """

    schema: Dict[str, JsonType]
    version: int


@json_schema_type
@dataclass(frozen=True)
class Object:
    """
    A reference to a binary or text object persisted in object storage, such as a CSV, JSON, or Parquet file.

    The lifetime of the object depends on the operation that created it but typically lasts for 24 hours.
    Object identifiers can be traded for pre-signed URLs via an authenticated endpoint operation while the object exists.

    :param id: Uniquely identifies the object.
    """

    id: ObjectID


@json_schema_type
@dataclass(frozen=True)
class Resource:
    """
    A pre-signed URL to a binary or text object persisted in object storage, such as a CSV, JSON or Parquet file.

    The lifetime of the pre-signed URL depends on the operation that created it but typically lasts for 15 minutes.
    No authentication is required to fetch the object via the pre-signed URL.

    :param url: URL to the object.
    """

    url: URL


class JobStatus(Enum):
    "Tracks the lifetime of a job from creation to termination (with success or failure)."

    Waiting = "waiting"
    Running = "running"
    Complete = "complete"
    Failed = "failed"

    def isTerminal(self) -> bool:
        "Signals if a job has been terminated (with 'complete' or 'failed' status)."
        return self is JobStatus.Complete or self is JobStatus.Failed


@dataclass(frozen=True)
class TableJob:
    """
    A data access job in progress.

    :param id: Opaque unique identifier of the job.
    :param status: The current status of the job.
    :param expires_at: The time when job will no longer be available.
    """

    id: JobID
    status: JobStatus
    expires_at: Optional[datetime]

    def json(self) -> str:
        return json.dumps(object_to_json(self))


@json_schema_type
@dataclass(frozen=True)
class CompleteJob(TableJob):
    """
    A data access job that has completed with success.

    :param objects: The list of objects generated by the job.
    :param schema_version: Version of the schema that records in the table conform to.
    """

    objects: List[Object]
    schema_version: int


@json_schema_type
@dataclass(frozen=True)
class CompleteSnapshotJob(CompleteJob):
    """
    A snapshot query that has completed with success.

    :param at: Timestamp (in UTC) that identifies the table state. This can be used as a starting point for future incremental queries.
    """

    at: datetime

    def __post_init__(self) -> None:
        if not isinstance(self.at, datetime):
            raise TypeError("Invalid timestamp for `at`. Expected a datetime object. (e.g., 2025-05-23T13:22:47)")
        if self.at.tzinfo is None:
            raise ValueError("Timestamp `at` must include a time zone offset (e.g., +00:00).")


@json_schema_type
@dataclass(frozen=True)
class CompleteIncrementalJob(CompleteJob):
    """
    An incremental query that has completed with success.

    :param since: Start timestamp (in UTC); only those records are returned that have been persisted since the specified date and time.
    :param until: End timestamp (in UTC); only those records are returned that have been persisted before the specified date and time.
        This can be used as a starting point for future incremental queries.
    """

    since: datetime
    until: datetime

    def __post_init__(self) -> None:
        if not isinstance(self.since, datetime):
            raise TypeError("Invalid timestamp for `since`. Expected a datetime object. (e.g., 2025-05-23T13:22:47)")
        if self.since.tzinfo is None:
            raise ValueError("Timestamp `since` must include a time zone offset (e.g., +00:00).")

        if not isinstance(self.until, datetime):
            raise TypeError("Invalid timestamp for `until`. Expected a datetime object. (e.g., 2025-05-23T13:22:47)")
        if self.until.tzinfo is None:
            raise ValueError("Timestamp `until` must include a time zone offset (e.g., +00:00).")


@json_schema_type
@dataclass(frozen=True)
class FailedJob(TableJob):
    """
    A data access job that has terminated with failure.

    :param error: Provides more details on the error that occurred.
    """

    error: ProcessingError


Job = Union[TableJob, CompleteSnapshotJob, CompleteIncrementalJob, FailedJob]
register_schema(Job, name="Job")


@dataclass(frozen=True)
class TableList:
    "A list of tables that exist in the organization domain."

    tables: List[str]


@json_schema_type
class Format(Enum):
    r"""
    Identifies the format of the data returned, e.g. TSV, CSV, JSON Lines, or Parquet.

    Tab-separated values (TSV) is a simple tabular format in which each record (table row) occupies a single line.

    * Output always begins with a header row, which lists all metadata and data field names.
    * Fields (table columns) are delimited by *tab* characters.
    * Non-printable characters and special values are escaped with *backslash* (`\\`).

    Comma-separated values (CSV) output follows [RFC 4180](https://www.ietf.org/rfc/rfc4180.html) with a few extensions:

    * Output always begins with a header row, which lists all metadata and data field names.
    * Strings are quoted with double quotes (`"`) if they contain special characters such as the double quote itself,
      the comma delimiter, a newline, a carriage return, a tab character, etc., or if their string representation would
      be identical to a special value such as NULL.
    * Empty strings are always represented as `""`.
    * NULL values are represented with the unquoted literal string `NULL`.
    * Missing values are presented as an empty string (no characters between delimiters).
    * Each row has the same number of fields.

    When the output data is represented in the [JSON Lines](https://jsonlines.org/) format, each record (table row)
    occupies a single line. Each line is a JSON object, which can be validated against the corresponding JSON schema.

    Properties with `null` values are omitted in JSON.

    Parquet files are compatible with Spark version 3.0 and later.
    """

    TSV = "tsv"
    "Tab-separated values, in compliance with PostgreSQL COPY."

    CSV = "csv"
    "Comma-separated values, as per RFC 4180."

    JSONL = "jsonl"
    "JSON lines format, with a single JSON object occupying each line."

    Parquet = "parquet"
    "Parquet format, as generated by Spark."


@json_schema_type
class Mode(Enum):
    r"""
    Output generation mode controls how nested fixed-cardinality fields are expanded into columns.

    Mode `expanded` lays out nested fixed-cardinality fields into several columns. Consider the following example for TSV:
    ```tsv
    meta.ts               meta.action  key.id  value.plain  value.nested.sub1  value.nested.sub2  value.nested.sub3
    2023-10-23T01:02:03Z  U            1       string       1                  multi-\nline        \N
    ```

    Mode `condensed` keeps nested fields together. Observe how a nested field becomes a single JSON-valued field:
    ```tsv
    meta.ts               meta.action  key.id  value.plain  value.nested
    2023-10-23T01:02:03Z  U            1       string       {"sub1": 1, "sub2": "multi-\\nline"}
    ```

    In case both JSON and the output format (e.g. CSV or TSV) define escaping rules, they are applied consecutively.
    This is why there are multiple backslash characters in the example above: JSON escapes a newline character as
    `\n`, and then TSV escapes the backslash character to make the sequence `\\n`.

    Properties with `null` values are omitted in condensed nested fields, as in JSON.

    If all nested values are *NULL*, the tabular result is empty, not `{}` (empty JSON object). Specifically, TSV
    would write `\N` (*NULL*) and CSV would write no value (blank field).

    Output generation mode does not affect fields `meta` and `key`, which are always expanded. Likewise,
    variable-cardinality fields (e.g. JSON `array` or `object`) are unaffected by `mode`, and are always exported as
    JSON.
    """

    expanded = "expanded"
    "Nested fixed-cardinality fields are expanded into several columns."

    condensed = "condensed"
    "Nested fixed-cardinality fields are exported as embedded JSON."


@dataclass(frozen=True)
class TableQuery:
    """
    Encapsulates a query request to retrieving data from a table.

    :param format: The format of the data to be returned.
    :param mode: Output generation mode.
    """

    format: Format
    mode: Optional[Mode]


@json_schema_type
@dataclass(frozen=True)
class SnapshotQuery(TableQuery):
    """
    Snapshot queries return the present state of the table.

    Snapshot queries help populate an empty database. After the initial snapshot query, you would use incremental
    queries to get the most up-to-date version of the data.
    """

    pass


@json_schema_type
@dataclass(frozen=True)
class IncrementalQuery(TableQuery):
    """
    Incremental queries return consolidated updates to a table, and help update a previous state to the present state.

    If only a *since* timestamp is given (recommended), the operation returns all changes since the specified point in
    time. If multiple updates took place to a record since the specified time, only the most recent version of the
    record is returned.

    If both a *since* and an *until* timestamp is given, the operation returns all records that have changed since
    the start timestamp of the interval but have not been altered after the end timestamp of the interval. Any records
    that have been updated after the *until* timestamp are not included in the query result. This functionality is
    useful to break up larger batches of changes but cannot be reliably used as a means of reconstructing a database
    state in the past (i.e. a point-in-time query or a backup of a previous state).

    The range defined by *since* and *until* is inclusive for the *since* timestamp but exclusive for the *until*
    timestamp.

    You would normally use incremental queries to fetch changes since a snapshot query or a previous incremental query.
    If issued as a follow-up to a snapshot query, the *since* timestamp of the incremental query would be equal to the
    *at* timestamp of the snapshot query. If issued as a follow-up to an incremental query, you would chain the *until*
    timestamp returned by the previous query job with the *since* timestamp of the new query request.

    :param since: Start timestamp (in UTC); only those records are returned that have been persisted since the
        specified date and time. This typically equals `at` returned by a previous snapshot query job, or `until` returned
        by a previous incremental query job.
    :param until: End timestamp (in UTC); only those records are returned that have not been changed after the
        specified date and time. If omitted (recommended), defaults to the commit time of the latest record.
    """

    since: datetime
    until: Optional[datetime]

    def __post_init__(self) -> None:
        if not isinstance(self.since, datetime):
            raise TypeError("Invalid timestamp for `since`. Expected a datetime object. (e.g., 2025-05-23T13:22:47)")
        if self.since.tzinfo is None:
            raise ValueError("Timestamp `since` must include a time zone offset (e.g., +00:00).")

        if self.until is not None:
            if not isinstance(self.until, datetime):
                raise TypeError("Invalid timestamp for `until`. Expected a datetime object. (e.g., 2025-05-23T13:22:47)")
            if self.until.tzinfo is None:
                raise ValueError("Timestamp `until` must include a time zone offset (e.g., +00:00).")


Query = Union[SnapshotQuery, IncrementalQuery]
register_schema(Query, name="Query")


@dataclass(frozen=True)
class ResourceResult:
    """
    Associates object identifiers with pre-signed URLs to output resources.

    :param urls: A dictionary of key-value pairs consisting of an ObjectID and the corresponding resource URL.
    """

    urls: Dict[ObjectID, Resource]


@dataclass(frozen=True)
class Credentials:
    """
    Credentials to be passed to Instructure API Gateway.

    All Instructure Platform Services go through the API Gateway. Access to credentials is managed via the Instructure
    Identity Service.

    :param basic_credentials: Encoded credentials.
    :param client_id: The OAuth Client ID.
    :param client_region: The client's region decoded from the Client ID key.
    """

    basic_credentials: str
    client_id: str
    client_region: str

    @classmethod
    def create(cls, *, client_id: str, client_secret: str) -> "Credentials":
        if not client_id:
            raise ValueError("Missing DAP client ID. Please provide the client ID before proceeding. Obtain it from identity.instructure.com")
        if not client_secret:
            raise ValueError("Missing DAP client secret. Please provide the client secret before proceeding. Obtain it from identity.instructure.com")

        basic_credentials = base64.b64encode(
            f"{client_id}:{client_secret}".encode("utf-8")
        ).decode("ascii")

        # client ID should match the following pattern: region#client-id, e.g. us-west-2#123-...-ef2:Abc...K0E
        m = re.match(r"([\w-]+)[#]([\w-]+)", client_id)
        if m:
            client_region = m.group(1)
        else:
            client_region = "Unknown"  # IDM does not always send region

        return cls(
            client_id=client_id,
            basic_credentials=basic_credentials,
            client_region=client_region,
        )


@dataclass(frozen=True)
class TokenProperties:
    """
    An authentication/authorization token issued by API Gateway.

    :param access_token: A base64-encoded access token string with header, payload and signature parts.
    :param expires_in: Expiry time (in sec) of the access token. This field is informational, the timestamp is also embedded in the access token.
    :param scope: List of services accessible by the client. Informational field, as the scope is also embedded in the access token.
    :param token_type: Type of the access token.
    """

    access_token: str
    expires_in: int
    scope: str
    token_type: str


@dataclass(frozen=True)
class TableDataResult:
    """
    The result of a table query operation.

    :param schema_version: Version of the schema that records in the table conform to.
    :param timestamp: Timestamp (in UTC) that identifies the table state.
    :param job_id: The ID of the executed backend job.
    """

    schema_version: int
    timestamp: datetime
    job_id: JobID

    def __post_init__(self) -> None:
        if not isinstance(self.timestamp, datetime):
            raise TypeError("Invalid timestamp. Expected a datetime object. (e.g., 2025-05-23T13:22:47)")
        if self.timestamp.tzinfo is None:
            raise ValueError("Timestamp must include a time zone offset (e.g., +00:00).")


@dataclass(frozen=True)
class DownloadTableDataResult(TableDataResult):
    """
    The result of downloading the output of a snapshot or an incremental query to the local file system.

    :param downloaded_files: A list of paths to files containing the downloaded table data.
    """

    downloaded_files: List[str]


@dataclass(frozen=True)
class GetTableDataResult(TableDataResult):
    """
    The result of fetching the output of a snapshot or an incremental query.

    :param objects: The list of objects generated by the job, which can be traded for resource URLs.
    """

    objects: List[Object]
