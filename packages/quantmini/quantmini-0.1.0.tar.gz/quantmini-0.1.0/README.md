# High-Performance Data Pipeline for Financial Market Data

A production-ready data pipeline for processing Polygon.io S3 flat files into optimized formats for quantitative analysis and machine learning.

## ğŸ¯ Key Features

- **Adaptive Processing**: Automatically scales from 24GB workstations to 100GB+ servers
- **70%+ Compression**: Optimized Parquet and binary formats
- **Sub-Second Queries**: Partitioned data lake with predicate pushdown
- **Incremental Updates**: Process only new data using watermarks
- **Apple Silicon Optimized**: 2-3x faster on M1/M2/M3 chips
- **Production Ready**: Monitoring, alerting, validation, and error recovery

## ğŸ“Š Performance

| Mode | Memory | Throughput | With Optimizations |
|------|---------|------------|-------------------|
| **Streaming** | < 32GB | 100K rec/s | 500K rec/s |
| **Batch** | 32-64GB | 200K rec/s | 1M rec/s |
| **Parallel** | > 64GB | 500K rec/s | 2M rec/s |

## ğŸš€ Quick Start

### Prerequisites

- macOS (Apple Silicon or Intel) or Linux
- Python 3.10+
- 24GB+ RAM (recommended: 32GB+)
- 1TB+ storage (SSD recommended)
- Polygon.io account with S3 flat files access

### Installation

1. **Install uv package manager**:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. **Clone and setup**:
```bash
git clone <repository-url>
cd quantmini

# Create project structure
./create_structure.sh

# Create and activate virtual environment
uv venv
source .venv/bin/activate  # On macOS/Linux
```

3. **Install dependencies**:
```bash
uv pip install qlib polygon boto3 aioboto3 polars duckdb pyarrow psutil pyyaml
```

4. **Configure credentials**:
```bash
cp config/credentials.yaml.example config/credentials.yaml
# Edit config/credentials.yaml with your Polygon API keys
```

5. **Run system profiler**:
```bash
python -m src.core.system_profiler
# This will create config/system_profile.yaml
```

### First Run

```bash
# Run daily pipeline (processes latest data)
python scripts/run_daily_pipeline.py

# Or backfill historical data
python scripts/run_backfill.py --start-date 2024-01-01 --end-date 2024-12-31
```

## ğŸ“ Project Structure

```
quantmini/
â”œâ”€â”€ config/              # Configuration files
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ core/           # System profiling, memory monitoring
â”‚   â”œâ”€â”€ download/       # S3 downloaders
â”‚   â”œâ”€â”€ ingest/         # Data ingestion (streaming/batch/parallel)
â”‚   â”œâ”€â”€ storage/        # Parquet data lake
â”‚   â”œâ”€â”€ features/       # Feature engineering
â”‚   â”œâ”€â”€ transform/      # Binary format conversion
â”‚   â”œâ”€â”€ query/          # Query engine
â”‚   â””â”€â”€ orchestration/  # Pipeline orchestration
â”œâ”€â”€ data/               # Data storage (not in git)
â”‚   â”œâ”€â”€ lake/          # Parquet data lake
â”‚   â”œâ”€â”€ binary/        # Qlib binary format
â”‚   â””â”€â”€ metadata/      # Watermarks, indexes
â”œâ”€â”€ scripts/           # Command-line scripts
â”œâ”€â”€ tests/             # Test suite
â””â”€â”€ docs/              # Documentation
```

## ğŸ”§ Configuration

Edit `config/pipeline_config.yaml` to customize:

- **Processing mode**: `adaptive`, `streaming`, `batch`, or `parallel`
- **Data types**: Enable/disable stocks, options, daily, minute data
- **Compression**: Choose `snappy` (fast) or `zstd` (better compression)
- **Features**: Configure which features to compute
- **Optimizations**: Enable Apple Silicon, async downloads, etc.

See [CONFIGURATION.md](docs/CONFIGURATION.md) for details.

## ğŸ“š Documentation

- **[Implementation Plan](IMPLEMENTATION_PLAN.md)**: 28-week roadmap
- **[Project Memory](docs/PROJECT_MEMORY.md)**: Design principles and patterns
- **[Project Structure](docs/PROJECT_STRUCTURE.md)**: Complete directory layout
- **[Design Doc](pipeline_design/mac-optimized-pipeline.md)**: Architecture details

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test suite
pytest tests/unit/
pytest tests/integration/
pytest tests/performance/
```

## ğŸ” Monitoring

Access monitoring dashboards:

```bash
# View health status
python scripts/check_health.py

# View performance metrics
cat logs/performance/performance_metrics.json

# Generate report
python scripts/generate_report.py
```

## ğŸ“Š Data Types

The pipeline processes four types of data from Polygon.io:

1. **Stock Daily Aggregates**: Daily OHLCV for all US stocks
2. **Stock Minute Aggregates**: Minute-level data per symbol
3. **Options Daily Aggregates**: Daily options data per underlying
4. **Options Minute Aggregates**: Minute-level options data (all contracts)

## ğŸ¨ Architecture

```
S3 CSV.GZ Files
      â†“
Adaptive Ingestion (Streaming/Batch/Parallel)
      â†“
Parquet Data Lake (Partitioned)
      â†“
Feature Engineering (DuckDB/Polars)
      â†“
Qlib Binary Format (ML-Ready)
```

## ğŸš¦ Pipeline Stages

1. **Download**: Async S3 downloads with connection pooling
2. **Ingest**: Adaptive processing based on available memory
3. **Validate**: Data quality checks
4. **Enrich**: Feature engineering (alpha, returns, etc.)
5. **Convert**: Transform to qlib binary format
6. **Query**: Fast access via DuckDB/Polars

## ğŸ” Security

- **Never commit** `config/credentials.yaml` (in .gitignore)
- Store credentials in environment variables for production
- Use AWS Secrets Manager or similar for cloud deployments
- Rotate API keys regularly

## ğŸ› Troubleshooting

### Memory Errors
```bash
# Reduce memory usage
export MAX_MEMORY_GB=16

# Force streaming mode
export PIPELINE_MODE=streaming
```

### S3 Rate Limits
```bash
# Reduce concurrent downloads
# Edit config/pipeline_config.yaml:
# optimizations.async_downloads.max_concurrent: 4
```

### Slow Performance
```bash
# Enable profiling
# Edit config/pipeline_config.yaml:
# monitoring.profiling.enabled: true

# Run and check logs/performance/
```

See [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for more.

## ğŸ¤ Contributing

See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for development guidelines.

## ğŸ“ˆ Performance Tuning

See [PERFORMANCE_TUNING.md](docs/PERFORMANCE_TUNING.md) for:
- Apple Silicon optimizations
- Memory tuning
- Storage optimization
- Query performance
- Benchmarking

## ğŸ—ºï¸ Roadmap

- [x] Phase 0-4: Core pipeline (Weeks 1-10)
- [ ] Phase 5-8: Features and queries (Weeks 11-18)
- [ ] Phase 9-11: Orchestration and optimization (Weeks 19-24)
- [ ] Phase 12-14: Monitoring and production (Weeks 25-28)

See [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md) for detailed timeline.

## ğŸ“„ License

[Add your license here]

## ğŸ™ Acknowledgments

- **Polygon.io**: S3 flat files data source
- **Qlib**: Quantitative investment framework
- **Polars**: High-performance DataFrame library
- **DuckDB**: Embedded analytical database

## ğŸ“§ Support

- Documentation: [docs/](docs/)
- Issues: [GitHub Issues](https://github.com/your-repo/issues)
- Email: your-email@example.com

---

**Built with**: Python 3.10+, uv, qlib, polygon, polars, duckdb, pyarrow

**Optimized for**: macOS (Apple Silicon M1/M2/M3), 24GB+ RAM, SSD storage
