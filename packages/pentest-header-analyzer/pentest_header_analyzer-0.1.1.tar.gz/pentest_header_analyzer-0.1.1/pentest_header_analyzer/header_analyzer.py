#!/usr/bin/env python3
# header_analyzer.py

import argparse
import io
import contextlib
import os
from datetime import datetime
from .utils import (
    fetch_headers,
    score_headers,
    display_report,
    generate_poc,
    tls_check,
    analyze_cookies,
    SECURITY_HEADERS,
)


def _filter_security_headers(raw_headers):
    sec_keys = {k.lower() for k in SECURITY_HEADERS.keys()}
    filtered = {}
    for k, v in raw_headers.items():
        if k.lower() in sec_keys:
            filtered[k] = v
        elif k.lower().startswith("set-cookie") or k.lower() == "set-cookie":
            filtered[k] = v
    return filtered


def _truncate_value(val, max_len=120):
    if not isinstance(val, str):
        val = str(val)
    if len(val) > max_len:
        return val[:max_len] + "... (truncated)"
    return val


def _build_text_report_header(index: int = None, url: str = None) -> str:
    if index is not None:
        return f"=== [{index}] Scanning: {url} ===\n"
    return f"=== Scanning: {url} ===\n"


def print_save_confirmation(file_path: str):
    """
    Print a confirmation message with file path, human-readable size and timestamp.
    Example: "[+] Results saved to out.txt (12.3 KB) at 2025-09-29 23:08:11"
    """
    try:
        size_bytes = os.path.getsize(file_path)
        if size_bytes >= 1024:
            size_str = f"{size_bytes / 1024:.1f} KB"
        else:
            size_str = f"{size_bytes} bytes"
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[+] Results saved to {file_path} ({size_str}) at {ts}")
    except Exception:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[+] Results saved to {file_path} at {ts}")


def analyze_url(url, txt_out=None, show_raw=True, batch_index=None):
    """
    Analyze a single URL and return a result dict.

    - txt_out: if provided (truthy), analyze_url will capture a human-readable
      'text_report' but will NOT write files or print save confirmations.
    - show_raw: whether to include raw headers in the captured text_report (or when printing).
    - batch_index: optional int used to build the '[i] Scanning' header inside the text_report.
    """
    headers, set_cookie_list, status_code, parsed_cookies = fetch_headers(url)
    if not headers:
        if txt_out:
            return {
                "url": url,
                "status_code": None,
                "headers": {},
                "set_cookie_list": [],
                "parsed_cookies": {},
                "cookies_analysis": [],
                "score": 0,
                "max_score": 0,
                "poc": {},
                "tls": {},
                "text_report": f"{_build_text_report_header(batch_index, url)}[-] Failed to fetch headers for {url}\n",
            }
        else:
            print(f"[-] Failed to fetch headers for {url}")
            return None

    # scoring and metadata
    score, max_score = score_headers(headers)
    poc = generate_poc(headers)
    tls = {}
    if url.startswith("https://"):
        tls = tls_check(url)

    important_headers = _filter_security_headers(headers)

    # Capture human-readable text report (table + raw headers) into a string when requested
    text_report_buf = io.StringIO()
    if txt_out:
        header_line = _build_text_report_header(batch_index, url) if batch_index is not None else _build_text_report_header(None, url)
        text_report_buf.write(header_line)
        # display_report prints to stdout; capture it into the buffer
        with contextlib.redirect_stdout(text_report_buf):
            display_report(url, headers, set_cookie_list, score, max_score, status_code)
        if show_raw:
            text_report_buf.write("\n--- Raw Response Headers ---\n")
            for k, v in headers.items():
                text_report_buf.write(f"{k}: {_truncate_value(v)}\n")
            if set_cookie_list:
                text_report_buf.write("\n--- Set-Cookie Headers ---\n")
                for c in set_cookie_list:
                    text_report_buf.write(_truncate_value(c) + "\n")
        text_report = text_report_buf.getvalue()
    else:
        # interactive: print to terminal
        display_report(url, headers, set_cookie_list, score, max_score, status_code)
        if show_raw:
            print("\n--- Raw Response Headers ---")
            for k, v in headers.items():
                print(f"{k}: {_truncate_value(v)}")
            if set_cookie_list:
                print("\n--- Set-Cookie Headers ---")
                for c in set_cookie_list:
                    print(_truncate_value(c))
        text_report = None

    result = {
        "url": url,
        "status_code": status_code,
        "headers": important_headers,
        "set_cookie_list": set_cookie_list,
        "parsed_cookies": {c: parsed_cookies.get(c) for c in parsed_cookies} if parsed_cookies else {},
        "cookies_analysis": analyze_cookies(set_cookie_list, parsed_cookies),
        "score": score,
        "max_score": max_score,
        "poc": poc,
        "tls": tls,
    }

    if txt_out:
        result["text_report"] = text_report

    return result


def main():
    parser = argparse.ArgumentParser(description="Pentester Header Analyzer - batch-safe")
    parser.add_argument("url", nargs="?", help="Target URL (e.g. https://example.com)")
    parser.add_argument("--batch", help="File with URLs (one per line)")
    parser.add_argument("--txt", help="Write text output to file (single or batch).")
    args = parser.parse_args()

    # Batch mode
    if args.batch:
        analyze_url._batch_mode = True
        results = []
        reports = []
        with open(args.batch, "r") as f:
            for i, line in enumerate(f, 1):
                url = line.strip()
                if not url:
                    continue
                report_header = f"\n=== [{i}] Scanning: {url} ==="
                print(report_header)
                res = analyze_url(url, txt_out=bool(args.txt), show_raw=True, batch_index=i)
                if res:
                    results.append(res)
                    if args.txt:
                        reports.append(res.get("text_report") or (report_header + "\n[no detailed report available]\n"))

        if args.txt:
            try:
                with open(args.txt, "w", encoding="utf-8") as outf:
                    outf.write("\n\n".join(reports))
                print_save_confirmation(args.txt)
            except Exception as e:
                print(f"[-] Failed to save report to {args.txt}: {e}")
        analyze_url._batch_mode = False

    # Single URL
    elif args.url:
        report_header = f"\n=== Scanning: {args.url} ==="
        print(report_header)
        res = analyze_url(args.url, txt_out=bool(args.txt), show_raw=True, batch_index=None)
        if args.txt and res:
            try:
                with open(args.txt, "w", encoding="utf-8") as outf:
                    outf.write(res.get("text_report") or (report_header + "\n[no detailed report available]\n"))
                print_save_confirmation(args.txt)
            except Exception as e:
                print(f"[-] Failed to save report to {args.txt}: {e}")
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
