"""Knowledge Graph Builder - Main orchestrator for knowledge graph construction"""

import json
import os
from typing import Any, Dict, List, Optional, Union
from loguru import logger

from .config import GraphRagConfig, LLMConfig
from .models import Entity, Relationship, KnowledgeGraph, EntityType, RelationType
from .extractors import EntityExtractor, RelationshipExtractor
from .spo_extractor import SPOExtractor
from .schema_generator import SchemaGenerator
from .validators import GraphQualityValidator
from .community import CommunityDetector
from .optimizer import GraphOptimizer


class KnowledgeGraphBuilder:
    """Main orchestrator for knowledge graph construction"""
    
    def __init__(self, config: GraphRagConfig, llm_config: LLMConfig):
        self.config = config
        self.llm_config = llm_config
        
        # Delayed import to avoid circular dependency
        from agenticx.llms import LlmFactory
        llm_client = LlmFactory.create_llm(self.llm_config)
        self.llm_client = llm_client  # ä¿å­˜ä¸ºå®ä¾‹å±æ€§ï¼ˆè½»é‡æ¨¡å‹ï¼‰
        
        # åˆ›å»ºå¼ºæ¨¡å‹å®¢æˆ·ç«¯ï¼ˆç”¨äºæ–‡æ¡£åˆ†æå’ŒSchemaç”Ÿæˆï¼‰
        try:
            # å°è¯•ä»é…ç½®ä¸­è·å–å¼ºæ¨¡å‹é…ç½®
            strong_model_config = getattr(self.config, 'strong_model_config', None)
            if strong_model_config:
                self.strong_llm_client = LlmFactory.create_llm(strong_model_config)
                # logger.info("ğŸš€ å¼ºæ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            else:
                self.strong_llm_client = llm_client  # å›é€€åˆ°é»˜è®¤æ¨¡å‹
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å¼ºæ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        except Exception as e:
            logger.warning(f"âš ï¸ å¼ºæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {e}")
            self.strong_llm_client = llm_client
        
        # Initialize components
        self.entity_extractor = EntityExtractor(
            llm_client=llm_client,
            config=self.config.entity_extraction
        )
        
        self.relationship_extractor = RelationshipExtractor(
            llm_client=llm_client,
            config=self.config.relationship_extraction
        )
        
        # Initialize extraction method
        self.extraction_method = getattr(self.config, 'extraction_method', 'separate')
        
        # Initialize prompt manager - ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•çš„ç›¸å¯¹è·¯å¾„
        prompts_dir = os.path.join(os.getcwd(), 'prompts')
        
        # åŠ¨æ€å¯¼å…¥PromptManagerï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
        try:
            import sys
            sys.path.append(os.getcwd())
            from prompt_manager import PromptManager
            self.prompt_manager = PromptManager(prompts_dir)
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥PromptManager: {e}")
            self.prompt_manager = None
        
        # Initialize schema generator
        base_schema_path = os.path.join(os.getcwd(), 'schema.json')
        self.schema_generator = SchemaGenerator(
            llm_client=llm_client,
            strong_llm_client=self.strong_llm_client,  # ä¼ å…¥å¼ºæ¨¡å‹å®¢æˆ·ç«¯
            prompt_manager=self.prompt_manager,
            base_schema_path=base_schema_path if os.path.exists(base_schema_path) else None
        )
        
        # Initialize SPO extractor (will be configured with custom schema later)
        if self.extraction_method == 'spo':
            self.spo_extractor = None  # Will be initialized with custom schema
            logger.info(f"ä½¿ç”¨ä¸¤é˜¶æ®µSPOæŠ½å–æ–¹æ³•ï¼ˆSchemaç”Ÿæˆ + SPOæŠ½å–ï¼‰")
        else:
            logger.info(f"ä½¿ç”¨ä¼ ç»Ÿåˆ†ç¦»æŠ½å–æ–¹æ³•")
        
        self.quality_validator = GraphQualityValidator(
            config=self.config.quality_validation.to_dict()
        )
        
        community_config = self.config.community_detection.to_dict()
        community_config["llm_client"] = llm_client
        self.community_detector = CommunityDetector(
            algorithm="louvain",
            config=community_config
        )
        
        self.graph_optimizer = GraphOptimizer(
            config=self.config.graph_optimization.to_dict()
        )
    
    async def build_from_texts(
        self, 
        texts: List[str], 
        metadata: Optional[List[Dict[str, Any]]] = None,
        **kwargs
    ) -> KnowledgeGraph:
        """Build knowledge graph from a list of texts"""
        logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œè¾“å…¥æ–‡æœ¬æ•°é‡: {len(texts)}")
        
        # Initialize graph
        logger.debug("åˆå§‹åŒ–çŸ¥è¯†å›¾è°±")
        graph = KnowledgeGraph()
        
        # Stage 1: Generate custom schema if using SPO method
        custom_schema = None
        if self.extraction_method == 'spo':
            logger.info("é˜¶æ®µ1: æ™ºèƒ½Schemaç”Ÿæˆ")
            logger.info(f"æŠ½å–æ–¹æ³•: {self.extraction_method} (ä¸¤é˜¶æ®µSPOæŠ½å–)")
            
            # ç›´æ¥åŸºäºå®Œæ•´æ–‡æ¡£ç”Ÿæˆå®šåˆ¶schemaï¼ˆæ–°æ–¹æ³•ï¼‰
            logger.info("å¼€å§‹åŸºäºå®Œæ•´æ–‡æ¡£ç”Ÿæˆå®šåˆ¶Schema...")
            custom_schema = self.schema_generator.generate_custom_schema_from_documents(texts)
            
            # Save custom schema for reference
            custom_schema_path = os.path.join(os.getcwd(), 'custom_schema.json')
            self.schema_generator.save_custom_schema(custom_schema, custom_schema_path)
            logger.info(f"å®šåˆ¶Schemaå·²ä¿å­˜: {custom_schema_path}")
            
            # Initialize SPO extractor with custom schema
            logger.info("åˆå§‹åŒ–SPOæŠ½å–å™¨...")
            self.spo_extractor = SPOExtractor(
                llm_client=self.llm_client,
                prompt_manager=self.prompt_manager,
                custom_schema=custom_schema,
                config=self.config.entity_extraction.to_dict()
            )
            
            logger.success(f"âœ… é˜¶æ®µ1å®Œæˆ - å®šåˆ¶Schemaç”Ÿæˆï¼Œé¢†åŸŸ: {custom_schema.get('domain_info', {}).get('primary_domain', 'é€šç”¨')}")
        else:
            logger.info(f"æŠ½å–æ–¹æ³•: {self.extraction_method} (ä¼ ç»Ÿåˆ†ç¦»æŠ½å–)")
        
        # Stage 2: Extract entities and relationships
        logger.info("é˜¶æ®µ2: çŸ¥è¯†æŠ½å–")
        
        if self.extraction_method == 'spo':
            # ä½¿ç”¨æ‰¹å¤„ç†SPOæŠ½å–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
            # logger.info("ğŸš€ ä½¿ç”¨æ‰¹å¤„ç†SPOæŠ½å–ï¼Œæ˜¾è‘—æå‡æ€§èƒ½")
            batch_size = getattr(self.config, 'spo_batch_size', 1)  # ä»é…ç½®è·å–æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º1é¿å…ç½‘ç»œé—®é¢˜
            
            try:
                entities, relationships = await self.spo_extractor.extract_batch(
                    texts=texts, 
                    batch_size=batch_size,
                    **kwargs
                )
                
                logger.info(f"æ‰¹å¤„ç†SPOæŠ½å–å®Œæˆ: {len(entities)} ä¸ªå®ä½“, {len(relationships)} ä¸ªå…³ç³»")
                
                # æ‰¹é‡æ·»åŠ å®ä½“åˆ°å›¾è°±
                for entity in entities:
                    graph.add_entity(entity)
                
                # æ‰¹é‡æ·»åŠ å…³ç³»åˆ°å›¾è°±
                for relationship in relationships:
                    try:
                        graph.add_relationship(relationship)
                    except Exception as e:
                        logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥: {e}")
                        
            except Exception as e:
                logger.error(f"âŒ æ‰¹å¤„ç†SPOæŠ½å–å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªå¤„ç†: {e}")
                # å›é€€åˆ°åŸæ¥çš„é€ä¸ªå¤„ç†æ–¹å¼
                for i, text in enumerate(texts):
                    chunk_id = f"chunk_{i}"
                    logger.debug(f"å¤„ç†æ–‡æœ¬å— {i+1}/{len(texts)} (ID: {chunk_id})")
                    
                    entities, relationships = self.spo_extractor.extract(text, chunk_id=chunk_id)
                    
                    for entity in entities:
                        graph.add_entity(entity)
                    
                    for relationship in relationships:
                        try:
                            graph.add_relationship(relationship)
                        except Exception as rel_e:
                            logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥: {rel_e}")
        
        else:
            # ä¼ ç»Ÿåˆ†ç¦»æŠ½å–ï¼ˆé€ä¸ªå¤„ç†ï¼‰
            logger.info("ä½¿ç”¨ä¼ ç»Ÿåˆ†ç¦»æŠ½å–ï¼ˆé€ä¸ªå¤„ç†ï¼‰")
            for i, text in enumerate(texts):
                chunk_id = f"chunk_{i}"
                logger.info(f"å¤„ç†æ–‡æœ¬å— {i+1}/{len(texts)}: ID={chunk_id}, é•¿åº¦={len(text)}å­—ç¬¦")
                
                # Get metadata for this text chunk if provided
                chunk_metadata = metadata[i] if metadata and i < len(metadata) else {}
                if chunk_metadata:
                    logger.debug(f"ğŸ“‹ æ–‡æœ¬å—å…ƒæ•°æ®: {chunk_metadata}")
                # Use traditional separate extraction
                logger.debug("å¼€å§‹ä¼ ç»Ÿåˆ†ç¦»æŠ½å–")
                
                # Extract entities
                logger.debug("ğŸ‘¥ å¼€å§‹å®ä½“æå–")
                entities = self.entity_extractor.extract(text, chunk_id=chunk_id)
                logger.debug(f"ğŸ‘¥ æå–åˆ° {len(entities)} ä¸ªå®ä½“")
                
                for entity in entities:
                    graph.add_entity(entity)
                    logger.trace(f"â• æ·»åŠ å®ä½“: {entity.name} ({entity.entity_type})")
                
                # Extract relationships
                logger.debug("ğŸ”— å¼€å§‹å…³ç³»æå–")
                relationships = self.relationship_extractor.extract(
                    text, 
                    entities=entities,
                    chunk_id=chunk_id
                )
                logger.debug(f"ğŸ”— æå–åˆ° {len(relationships)} ä¸ªå…³ç³»")
                
                for relationship in relationships:
                    # æ£€æŸ¥æºå®ä½“å’Œç›®æ ‡å®ä½“æ˜¯å¦å­˜åœ¨ï¼ˆéœ€è¦IDä¿®å¤ï¼‰
                    source_exists = relationship.source_entity_id in graph.entities
                    target_exists = relationship.target_entity_id in graph.entities
                    
                    if not source_exists:
                        # å°è¯•é€šè¿‡åç§°æŸ¥æ‰¾å®ä½“
                        source_entity = self._find_entity_by_name(graph, relationship.source_entity_id)
                        if source_entity:
                            logger.info(f"ğŸ”„ ä¿®å¤æºå®ä½“ID: '{relationship.source_entity_id}' -> '{source_entity.id}'")
                            relationship.source_entity_id = source_entity.id
                        else:
                            logger.warning(f"âš ï¸ è·³è¿‡å…³ç³»ï¼šæºå®ä½“ '{relationship.source_entity_id}' ä¸å­˜åœ¨")
                            continue
                            
                    if not target_exists:
                        # å°è¯•é€šè¿‡åç§°æŸ¥æ‰¾å®ä½“
                        target_entity = self._find_entity_by_name(graph, relationship.target_entity_id)
                        if target_entity:
                            logger.info(f"ğŸ”„ ä¿®å¤ç›®æ ‡å®ä½“ID: '{relationship.target_entity_id}' -> '{target_entity.id}'")
                            relationship.target_entity_id = target_entity.id
                        else:
                            logger.warning(f"âš ï¸ è·³è¿‡å…³ç³»ï¼šç›®æ ‡å®ä½“ '{relationship.target_entity_id}' ä¸å­˜åœ¨")
                            continue
                    
                    try:
                        graph.add_relationship(relationship)
                        logger.trace(f"â• æ·»åŠ å…³ç³»: {relationship.source_entity_id} --[{relationship.relation_type}]--> {relationship.target_entity_id}")
                    except Exception as e:
                        logger.error(f"âŒ æ·»åŠ å…³ç³»å¤±è´¥: {e}")
                        logger.debug(f"   å…³ç³»è¯¦æƒ…: {relationship.source_entity_id} --[{relationship.relation_type}]--> {relationship.target_entity_id}")
        
        # Post-processing
        logger.info("å¼€å§‹åå¤„ç†")
        
        # Merge duplicate entities
        if kwargs.get("merge_entities", True):
            logger.debug("ğŸ”„ åˆå¹¶é‡å¤å®ä½“")
            merged_count = self._merge_duplicate_entities(graph)
            logger.debug(f"âœ… åˆå¹¶äº† {merged_count} ä¸ªé‡å¤å®ä½“")
        
        # Validate quality
        if kwargs.get("validate_quality", True):
            logger.debug("è¿›è¡Œè´¨é‡éªŒè¯")
            quality_report = self.quality_validator.validate(graph)
            logger.info(f"è´¨é‡éªŒè¯ç»“æœ: {quality_report.summary()}")
        
        # Detect communities
        if kwargs.get("detect_communities", False):
            logger.debug("ğŸ‘¥ æ£€æµ‹ç¤¾åŒº")
            self.community_detector.detect_communities(graph)
        
        # Optimize graph
        if kwargs.get("optimize_graph", True):
            logger.debug("âš¡ ä¼˜åŒ–å›¾è°±")
            optimization_stats = self.graph_optimizer.optimize(graph)
            logger.info(f"âš¡ å›¾è°±ä¼˜åŒ–ç»“æœ: {optimization_stats}")
        
        logger.success(f"ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼å®ä½“æ•°é‡: {len(graph.entities)}, å…³ç³»æ•°é‡: {len(graph.relationships)}")
        
        # Auto export to Neo4j if enabled
        if self.config.neo4j.enabled and self.config.neo4j.auto_export:
            logger.info("ğŸ—„ï¸ è‡ªåŠ¨å¯¼å‡ºåˆ°Neo4jæ•°æ®åº“")
            try:
                graph.export_to_neo4j(
                    uri=self.config.neo4j.uri,
                    username=self.config.neo4j.username,
                    password=self.config.neo4j.password,
                    database=self.config.neo4j.database,
                    clear_existing=self.config.neo4j.clear_on_export
                )
                logger.success("âœ… Neo4jå¯¼å‡ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Neo4jå¯¼å‡ºå¤±è´¥: {e}")
                logger.warning("ğŸ’¡ è¯·æ£€æŸ¥Neo4jæœåŠ¡æ˜¯å¦è¿è¡Œï¼Œä»¥åŠè¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®")
        
        return graph
    
    def build_from_documents(
        self, 
        documents: List[Dict[str, Any]], 
        **kwargs
    ) -> KnowledgeGraph:
        """Build knowledge graph from structured documents"""
        texts = [doc.get("content", "") for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        
        return self.build_from_texts(texts, metadata, **kwargs)
    
    def build_incremental(
        self, 
        existing_graph: KnowledgeGraph,
        new_texts: List[str],
        **kwargs
    ) -> KnowledgeGraph:
        """Incrementally build upon existing knowledge graph"""
        logger.info(f"ğŸ”„ å¢é‡æ„å»º: å‘ç°æœ‰å›¾è°±({len(existing_graph.entities)}ä¸ªå®ä½“)æ·»åŠ {len(new_texts)}ä¸ªæ–°æ–‡æœ¬")
        
        # Create new graph from existing one
        new_graph = KnowledgeGraph()
        new_graph.entities = existing_graph.entities.copy()
        new_graph.relationships = existing_graph.relationships.copy()
        new_graph.metadata = existing_graph.metadata.copy()
        
        # Copy NetworkX graph
        new_graph.graph = existing_graph.graph.copy()
        
        # Process new texts
        for i, text in enumerate(new_texts):
            chunk_id = f"incremental_chunk_{i}"
            logger.info(f"å¤„ç†å¢é‡æ–‡æœ¬å— {i+1}/{len(new_texts)}")
            
            # Extract entities
            entities = self.entity_extractor.extract(text, chunk_id=chunk_id)
            for entity in entities:
                new_graph.add_entity(entity)
            
            # Extract relationships
            relationships = self.relationship_extractor.extract(
                text, 
                entities=entities,
                chunk_id=chunk_id
            )
            for relationship in relationships:
                new_graph.add_relationship(relationship)
        
        # Post-processing for incremental build
        if kwargs.get("merge_entities", True):
            self._merge_duplicate_entities(new_graph)
        
        if kwargs.get("validate_quality", True):
            quality_report = self.quality_validator.validate(new_graph)
            logger.info(f"å¢é‡è´¨é‡éªŒè¯: {quality_report.summary()}")
        
        if kwargs.get("optimize_graph", True):
            optimization_stats = self.graph_optimizer.optimize(new_graph)
            logger.info(f"âš¡ å¢é‡å›¾è°±ä¼˜åŒ–: {optimization_stats}")
        
        logger.success(f"âœ… å¢é‡æ„å»ºå®Œæˆ: {len(new_graph.entities)} ä¸ªå®ä½“, {len(new_graph.relationships)} ä¸ªå…³ç³»")
        
        return new_graph
    
    def _find_entity_by_name(self, graph: KnowledgeGraph, name: str) -> Optional[Entity]:
        """é€šè¿‡åç§°æŸ¥æ‰¾å®ä½“"""
        for entity in graph.entities.values():
            if entity.name == name:
                return entity
            # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼å’Œå¤§å°å†™ï¼‰
            if entity.name.strip().lower() == name.strip().lower():
                return entity
        return None
    
    def _merge_duplicate_entities(self, graph: KnowledgeGraph) -> int:
        """Merge duplicate entities based on name similarity"""
        merged_count = 0
        processed_pairs = set()
        
        entity_list = list(graph.entities.values())
        
        for i, entity1 in enumerate(entity_list):
            for j, entity2 in enumerate(entity_list[i+1:], i+1):
                pair_key = tuple(sorted([entity1.id, entity2.id]))
                
                if pair_key in processed_pairs:
                    continue
                
                processed_pairs.add(pair_key)
                
                # Check if entities are similar enough to merge
                if self._should_merge_entities(entity1, entity2):
                    self._merge_two_entities(graph, entity1.id, entity2.id)
                    merged_count += 1
        
        logger.debug(f"ğŸ”„ åˆå¹¶äº† {merged_count} ä¸ªé‡å¤å®ä½“")
        return merged_count
    
    def _should_merge_entities(self, entity1: Entity, entity2: Entity) -> bool:
        """Determine if two entities should be merged"""
        # Check name similarity
        name_similarity = self._calculate_name_similarity(entity1.name, entity2.name)
        
        # Check type compatibility
        type_compatible = entity1.entity_type == entity2.entity_type
        
        # Check if they have similar contexts (simple heuristic)
        context_similarity = self._calculate_context_similarity(entity1, entity2)
        
        # Merge if names are very similar and types are compatible
        return name_similarity >= 0.8 and type_compatible and context_similarity >= 0.5
    
    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """Calculate similarity between two entity names"""
        name1_lower = name1.lower().strip()
        name2_lower = name2.lower().strip()
        
        # Exact match
        if name1_lower == name2_lower:
            return 1.0
        
        # One is substring of another
        if name1_lower in name2_lower or name2_lower in name1_lower:
            return 0.9
        
        # Calculate Jaccard similarity of words
        words1 = set(name1_lower.split())
        words2 = set(name2_lower.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    def _calculate_context_similarity(self, entity1: Entity, entity2: Entity) -> float:
        """Calculate context similarity between entities"""
        # Simple context similarity based on attributes
        attr1 = set(entity1.attributes.keys())
        attr2 = set(entity2.attributes.keys())
        
        if not attr1 or not attr2:
            return 0.5
        
        intersection = len(attr1.intersection(attr2))
        union = len(attr1.union(attr2))
        
        return intersection / union if union > 0 else 0.0
    
    def _merge_two_entities(self, graph: KnowledgeGraph, entity_id1: str, entity_id2: str) -> None:
        """Merge two entities into one"""
        entity1 = graph.get_entity(entity_id1)
        entity2 = graph.get_entity(entity_id2)
        
        if not entity1 or not entity2:
            return
        
        # Keep the entity with higher confidence
        if entity1.confidence >= entity2.confidence:
            keep_entity = entity1
            remove_entity = entity2
            keep_id = entity_id1
            remove_id = entity_id2
        else:
            keep_entity = entity2
            remove_entity = entity1
            keep_id = entity_id2
            remove_id = entity_id1
        
        # Merge attributes
        keep_entity.attributes.update(remove_entity.attributes)
        
        # Merge source chunks
        keep_entity.source_chunks.update(remove_entity.source_chunks)
        
        # Update confidence if merged
        keep_entity.confidence = max(entity1.confidence, entity2.confidence)
        
        # Update relationships
        for rel in graph.relationships.values():
            if rel.source_entity_id == remove_id:
                rel.source_entity_id = keep_id
            if rel.target_entity_id == remove_id:
                rel.target_entity_id = keep_id
        
        # Remove the merged entity
        del graph.entities[remove_id]
        graph.graph.remove_node(remove_id)
    
    def add_metadata(self, graph: KnowledgeGraph, metadata: Dict[str, Any]) -> None:
        """Add metadata to knowledge graph"""
        graph.metadata.update(metadata)
    
    def get_build_statistics(self, graph: KnowledgeGraph) -> Dict[str, Any]:
        """Get statistics about the built knowledge graph"""
        return {
            "num_entities": len(graph.entities),
            "num_relationships": len(graph.relationships),
            "num_entity_types": len(set(entity.entity_type for entity in graph.entities.values())),
            "num_relation_types": len(set(rel.relation_type for rel in graph.relationships.values())),
            "average_entity_confidence": sum(entity.confidence for entity in graph.entities.values()) / len(graph.entities) if graph.entities else 0,
            "average_relationship_confidence": sum(rel.confidence for rel in graph.relationships.values()) / len(graph.relationships) if graph.relationships else 0,
            "num_communities": len([entity for entity in graph.entities.values() if entity.entity_type == EntityType.COMMUNITY]),
            "graph_density": nx.density(graph.graph) if graph.graph.number_of_nodes() > 0 else 0,
            "num_connected_components": nx.number_connected_components(graph.graph) if graph.graph.number_of_nodes() > 0 else 0
        }