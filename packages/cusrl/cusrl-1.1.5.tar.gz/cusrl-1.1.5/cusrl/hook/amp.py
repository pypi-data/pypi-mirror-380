from collections.abc import Callable
from typing import cast

import numpy as np
import torch
from torch import Tensor, nn

from cusrl.module import Module, ModuleFactoryLike, RunningMeanStd
from cusrl.template import ActorCritic, Hook
from cusrl.utils.dict_utils import get_first
from cusrl.utils.typing import Array, Slice

__all__ = ["AdversarialMotionPrior"]


class AdversarialMotionPrior(Hook[ActorCritic]):
    """Implements the Adversarial Motion Priors.

    Described in: "AMP: adversarial motion priors for stylized physics-based
    character control",
    https://dl.acm.org/doi/10.1145/3450626.3459670

    This hook modifies the agent's reward and provides a loss objective to train
    a discriminator network. The discriminator is trained to distinguish between
    transitions generated by the agent and transitions from an expert dataset.
    The agent is then rewarded for generating transitions that the discriminator
    classifies as expert-like. This encourages the agent to imitate the style of
    motion from the expert dataset.

    Args:
        discriminator_factory (ModuleFactoryLike):
            A factory for creating the discriminator model. The discriminator
            may not be recurrent.
        dataset_source (str | Array | Callable[[], Array] | None, optional):
            The source of the expert motion dataset. Can be a file path to a
            ``.npy`` or ``.pt`` file, a ``numpy.ndarray`` / ``Tensor``, or a
            callable that returns a ``numpy.ndarray`` / ``Tensor``. If ``None``,
            the ``environment_spec.demonstration_sampler`` should be provided to
            sample expert transitions.
        state_indices (Slice | None, optional):
            A slice object to extract the relevant parts of the state for AMP.
            If ``None``, it's assumed that the environment provides an
            ``"amp_obs"`` key in the transition dictionary. Defaults to
            ``None``.
        batch_size (int, optional):
            The batch size for training the discriminator. Defaults to ``512``.
        reward_scale (float, optional):
            A scaling factor for the style reward. Defaults to ``1.0``.
        loss_weight (float, optional):
            A weight for the total discriminator loss in the optimization
            objective. Defaults to ``1.0``.
        grad_penalty_weight (float, optional):
            The weight for the gradient penalty term in the discriminator loss.
            Defaults to ``5.0``.
    """

    def __init__(
        self,
        discriminator_factory: ModuleFactoryLike,
        dataset_source: str | Array | Callable[[], Array] | None = None,
        state_indices: Slice | None = None,
        batch_size: int | None = 512,
        reward_scale: float = 1.0,
        loss_weight: float = 1.0,
        grad_penalty_weight: float = 5.0,
    ):
        super().__init__()
        self.discriminator_factory = discriminator_factory
        self.dataset_source = dataset_source
        self.state_indices = state_indices

        # Mutable attributes
        self.batch_size: int | None = batch_size
        self.reward_scale: float = reward_scale
        self.loss_weight: float = loss_weight
        self.grad_penalty_weight: float = grad_penalty_weight
        self.register_mutable("batch_size")
        self.register_mutable("reward_scale")
        self.register_mutable("loss_weight")
        self.register_mutable("grad_penalty_weight")

        # Runtime attributes
        self.dataset: Tensor | None
        self.discriminator: Module
        self.transition_dim: int
        self.transition_rms: RunningMeanStd
        self.criterion: nn.BCEWithLogitsLoss

    def init(self):
        self.dataset = None
        if isinstance(self.dataset_source, str):
            if self.dataset_source.endswith(".npy"):
                self.dataset = torch.as_tensor(np.load(self.dataset_source), device=self.agent.device)
            elif self.dataset_source.endswith(".pt"):
                self.dataset = torch.load(self.dataset_source, map_location=self.agent.device)
            else:
                raise ValueError(f"Unsupported dataset file format: {self.dataset_source}")
        elif isinstance(self.dataset_source, (Tensor, np.ndarray)):
            self.dataset = self.agent.to_tensor(self.dataset_source)
        elif callable(self.dataset_source):
            self.dataset = self.agent.to_tensor(self.dataset_source())
        elif self.dataset_source is not None:
            raise ValueError(f"Unsupported dataset_path type: {type(self.dataset_source)}.")

        self.transition_dim = self._sample_demonstration(1).size(-1)
        self.register_module("discriminator", self.discriminator_factory(self.transition_dim, 1))
        self.register_module("transition_rms", RunningMeanStd(self.transition_dim))
        self.criterion = nn.BCEWithLogitsLoss()

    @torch.no_grad()
    def post_step(self, transition):
        if (agent_transition := cast(Tensor, transition.pop("amp_obs", None))) is None:
            if self.state_indices is None:
                raise ValueError("AMP observation is not provided and indices are not specified.")

            state = cast(Tensor, get_first(transition, "state", "observation"))
            next_state = cast(Tensor, get_first(transition, "next_state", "next_observation"))
            amp_state = state[..., self.state_indices]
            amp_next_state = next_state[..., self.state_indices]
            agent_transition = torch.cat([amp_state, amp_next_state], dim=-1)

        expert_transition = self._sample_demonstration(agent_transition.size(0))
        self.transition_rms.update(agent_transition)
        self.transition_rms.update(expert_transition)
        agent_transition = self.transition_rms.normalize(agent_transition)
        expert_transition = self.transition_rms.normalize(expert_transition)
        transition["agent_transition"] = agent_transition
        transition["expert_transition"] = expert_transition

        with self.agent.autocast():
            logit = self.discriminator(agent_transition)
        style_reward = -torch.log(torch.clamp(1 - 1 / (1 + torch.exp(-logit)), min=1e-4))

        cast(Tensor, transition["reward"]).add_(style_reward)
        self.agent.record(amp_reward=style_reward)

    def objective(self, batch) -> Tensor:
        agent_transition = cast(Tensor, batch["agent_transition"]).flatten(0, -2)
        expert_transition = cast(Tensor, batch["expert_transition"]).flatten(0, -2)
        if self.batch_size is not None:
            batch_size = self.batch_size
            indices = torch.randint(agent_transition.size(0), (batch_size,), device=self.agent.device)
            agent_transition = agent_transition[indices]
            expert_transition = expert_transition[indices]
        else:
            batch_size = agent_transition.size(0)

        with self.agent.autocast():
            expert_transition.requires_grad_(True)
            agent_logit = self.discriminator(agent_transition)
            agent_disc_loss = self.criterion(agent_logit, torch.zeros_like(agent_logit))
            expert_logit = self.discriminator(expert_transition)
            expert_disc_loss = self.criterion(expert_logit, torch.ones_like(expert_logit))
            discrimination_loss = (agent_disc_loss + expert_disc_loss) / 2
            grad_penalty_loss = self._compute_grad_penalty_loss(expert_logit, expert_transition)

        self.agent.record(
            amp_discrimination_loss=discrimination_loss,
            amp_grad_penalty_loss=grad_penalty_loss,
        )
        return (discrimination_loss + grad_penalty_loss * self.grad_penalty_weight) * self.loss_weight

    @staticmethod
    def _compute_grad_penalty_loss(outputs: Tensor, inputs: Tensor) -> Tensor:
        gradients = torch.autograd.grad(
            outputs=outputs,
            inputs=inputs,
            grad_outputs=torch.ones_like(outputs),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]
        gradient_penalty = gradients.square().sum(dim=-1).mean()
        return gradient_penalty

    def _sample_demonstration(self, num_samples: int) -> Tensor:
        if self.dataset is not None:
            dataset_indices = torch.randint(self.dataset.size(0), (num_samples,), device=self.agent.device)
            return self.dataset[dataset_indices]
        if self.agent.environment_spec.demonstration_sampler is None:
            raise ValueError("Either 'dataset_source' or 'environment_spec.demonstration_sampler' should be provided.")
        return self.agent.to_tensor(self.agent.environment_spec.demonstration_sampler(num_samples))
