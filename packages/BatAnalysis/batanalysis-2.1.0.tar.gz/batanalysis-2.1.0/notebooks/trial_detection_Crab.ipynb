{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is meant to accompany the trial_detection_Crab.py file that is included in this directory with additional comments. \n",
    "\n",
    "### Please note that running this whole analysis will produce ~2.5 TB of data. There are ways to reduce this amount but the brute force analysis conducted here does not attempt to save disk space. Also take note of the parallelized mosaic analysis since each parallel process can take ~10 GB of memory, so the user wants to set an appropriate value here such that they dont over load their computer.\n",
    "\n",
    "We will go through the code to produce Figure 3 of the associated BatAnalysis paper. \n",
    "\n",
    "First, we need to import the relevant packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: version mismatch between CFITSIO header (v40500) and linked library (v40400).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import batanalysis as ba\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.io import fits\n",
    "from pathlib import Path\n",
    "import swiftbat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to set our data directory, which will contain all of our BAT survey data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/tparsota/Documents/CRAB_SURVEY_DATA')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdir = Path(\"/Users/tparsota/Documents/CRAB_SURVEY_DATA\")\n",
    "ba.datadir(newdir, mkdir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we query HEASARC for the observation IDs when the coordinates of the Crab was in the BAT FOV during the dates when the data for the 22 month survey was accumulated. We also ensure that the minimum amount of exposure that the Crab had to the BAT detector plane was $> 1000$ cm$^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_name='Crab_Nebula_Pulsar'\n",
    "\n",
    "#use swiftbat to create a bat source object\n",
    "object_location = swiftbat.simbadlocation(\"Crab\")\n",
    "object_batsource = swiftbat.source(ra=object_location[0], dec=object_location[1], name=object_name)\n",
    "table_everything, query = ba.from_heasarc(time_range=Time([\"2004-12-15\",\"2006-10-27\"]), return_query=True)\n",
    "minexposure = 1000     # cm^2 after cos adjust\n",
    "\n",
    "#calculate the exposure with partial coding\n",
    "exposures = u.Quantity(\n",
    "    [object_batsource.exposure(ra=row[\"ra\"],\n",
    "                               dec=row[\"dec\"],\n",
    "                               roll=row[\"roll_angle\"])[0]\n",
    "        for row in table_everything\n",
    "    ])\n",
    "\n",
    "#select the observations that have greater than the minimum desired exposure\n",
    "table_exposed = table_everything[exposures.value > minexposure]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the data, we would then do:\n",
    "```\n",
    "result = ba.download_swiftdata(table_exposed)\n",
    "obs_ids=[i for i in table_exposed['obsid'] if result[i]['success']]\n",
    "```\n",
    "\n",
    "### OR\n",
    "\n",
    "If we were continuing our analysis during some later point we would do:\n",
    "```\n",
    "obs_ids=[i.name for i in sorted(ba.datadir().glob(\"*\")) if i.name.isnumeric()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data downloaded and the observation IDs obtained, we can now analyze the BAT survey data. We specify that we want the BAT images to be cleaned from bright sources which have been detected at SNR=6. We also specify that all sources in the BatAnalysis or custom catalogs that have the column value ```ALWAYS_CLEAN==T``` should be cleaned as well.\n",
    "\n",
    "After we define where the directory with all the pattern maps live in our system. Here, my patern map directory lies outside of where my BAT survey data have been downloaded and I want to include these pattern maps in my analyses for the computation of mosaic images later. Thus, I need to specify where these pattern noise maps live. If I do not then the pattern noise maps will not be included in the analyses and any later mosaic images that are created will suffer from this buildup of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict=dict(cleansnr=6,cleanexpr='ALWAYS_CLEAN==T')\n",
    "noise_map_dir=Path(\"/Users/tparsota/Documents/PATTERN_MAPS/\")\n",
    "batsurvey_obs=ba.parallel.batsurvey_analysis(obs_ids, input_dict=input_dict, patt_noise_dir=noise_map_dir, nprocs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after all of the survey data has been processed, we can do a batch calculation of the pha files, the drm files, and subequently the spectral fitting for each observation and pointing ID. By default the spectrum that I am fitting to the BAT survey spectra is a ```cflux*po``` model from 14-195 keV. If the model parameters are not well constrained or if the Crab is not detected at a level of 3$\\sigma$ above the background noise level then the function automatically tries to place 5$\\sigma$ upper limits on the detection of the source. The photon index of the power law that is fitted to the spectrum to obtain the 5$\\sigma$ upper limits is explicitly set to be 2.15 since we expect this type of spectral index already. \n",
    "\n",
    "In the line below I set ```recalc=True``` which is useful for when the user wants to completely redo an operation using different input parameters. For example, if I want to run the below line using he defaults desribed above I can do so but if I want to change the model that is fitted, the level of detection necessary for automatically calculating upper limits or anything else, I simply set ```recalc=True``` and pass in the appropriate values and things will be updated within the ```batsurvey_obs``` objects appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batsurvey_obs=ba.parallel.batspectrum_analysis(batsurvey_obs, object_name, ul_pl_index=2.15, recalc=True,nprocs=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick glimpse of our results, we can use a convience function to plot the various values of interest for the Crab. \n",
    "\n",
    "The values that can be passed in are dicitonary values associated with the observation that can be accessed from the ```BatSurvey``` objects within the ```batsurvey_obs``` list. Some of these values are shown in the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=ba.plot_survey_lc(batsurvey_obs, id_list=object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we want to do the mosaicing analysis\n",
    "\n",
    "In order to do this step, we first have to group together all the BAT survey observations that we want to include in this step in our analysis. In most cases this is going to be all of our BAT survey observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outventory_file=ba.merge_outventory(batsurvey_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to continue our calculations from a point later on in our analysis, we can simply skip the above line and do:\n",
    "```\n",
    "outventory_file=Path(\"./path/to/outventory_all.fits\")\n",
    "```\n",
    "since the above cell simply creates a fits file with all the BAt survey observations that we want included in the analysis and returns the full path to the file. \n",
    "\n",
    "Next, we need to define the time bins for which we will create mosaic images and analyze them. To test our code, we will use the same 1 month binning as the 22 month survey paper used. We specify the ```end_datetime``` explicitly but do not pass in a ```start_datetime``` value. This is because the ```start_datetime``` value is automatically set to be the first BAT survey observation rounded to the nearest whole ```timedelta``` value (ie the floor function applied to the earliest BAT survey date to the start of that month in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bins=ba.group_outventory(outventory_file, np.timedelta64(1, \"M\"), end_datetime=Time(\"2006-10-27\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually do the mosaicing calculation simply by doing. Where we will end up getting a list of mosaics for each month time bin and the total \"time-integrated\" mosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_list, total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file, time_bins, nprocs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the mosaic images we simply use the same call as we did for the BAT Survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_list=ba.parallel.batspectrum_analysis(mosaic_list, object_name, ul_pl_index=2.15, nprocs=11)\n",
    "total_mosaic=ba.parallel.batspectrum_analysis(total_mosaic, object_name, ul_pl_index=2.15, use_cstat=False, nprocs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to plot our values of interest for each month, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=ba.plot_survey_lc(mosaic_list, id_list=object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see the BAT survey data alongside the mosaic data, we would then do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=ba.plot_survey_lc([batsurvey_obs,mosaic_list], id_list=object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"], same_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To also do the weekly mosaic analysis: we would do:\n",
    "```\n",
    "outventory_file_weekly=ba.merge_outventory(batsurvey_obs, savedir=Path('./weekly_mosaiced_surveyresults/'))\n",
    "time_bins_weekly=ba.group_outventory(outventory_file_weekly, np.timedelta64(1, \"W\"), start_datetime=Time(\"2004-12-01\"), end_datetime=Time(\"2006-10-27\"))\n",
    "weekly_mosaic_list, weekly_total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file_weekly, time_bins_weekly, nprocs=8)\n",
    "\n",
    "weekly_mosaic_list=ba.parallel.batspectrum_analysis(weekly_mosaic_list, object_name, recalc=True, nprocs=11)\n",
    "weekly_total_mosaic=ba.parallel.batspectrum_analysis(weekly_total_mosaic, object_name, recalc=True, use_cstat=False, nprocs=1)\n",
    "\n",
    "```\n",
    "\n",
    "To save disc space, it is possible to only construct the weekly mosaic and then combine the weekly mosaics together to produce the monthly mosaics that we obtained in the prior few cells. This operation is a bit more advanced but it depends on the `merge_mosaics` function.\n",
    "\n",
    "\n",
    "We can now save our survey/mosaic results by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=ba.concatenate_data(batsurvey_obs, object_name, [\"met_time\", \"utc_time\", \"exposure\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "with open('all_data_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "all_data_monthly=ba.concatenate_data(mosaic_list, object_name, [\"user_timebin/met_time\", \"user_timebin/utc_time\", \"user_timebin/met_stop_time\", \"user_timebin/utc_stop_time\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "with open('monthly_mosaic_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data_monthly, f)\n",
    "\n",
    "#If the weekly analysis has been completed, the next few lines can be uncommented:\n",
    "#all_data_weekly=ba.concatenate_data(weekly_mosaic_list, object_name, [\"user_timebin/met_time\", \"user_timebin/utc_time\", \"user_timebin/met_stop_time\", \"user_timebin/utc_stop_time\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "#with open('weekly_mosaic_dictionary.pkl', 'wb') as f:\n",
    "#    pickle.dump(all_data_weekly, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the main Crab Pulsar Nebula plot in the manuscript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_range=None\n",
    "time_unit=\"MET\"\n",
    "values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\"]\n",
    "\n",
    "\n",
    "survey_obsid_list=[\"all_data_dictionary\",\"monthly_mosaic_dictionary\"]\n",
    "\n",
    "#if the weekly mosaic dictionary was saved then the following line can be uncommented\n",
    "#survey_obsid_list=[\"all_data_dictionary\",\"monthly_mosaic_dictionary\", \"weekly_mosaic_dictionary\"]\n",
    "\n",
    "obs_list_count=0\n",
    "for observation_list in survey_obsid_list:\n",
    "\n",
    "    with open(observation_list+\".pkl\", 'rb') as f:\n",
    "        all_data=pickle.load(f)\n",
    "        data=all_data[object_name]\n",
    "\n",
    "    # get the time centers and errors\n",
    "    if \"mosaic\" in observation_list:\n",
    "\n",
    "        if \"MET\" in time_unit:\n",
    "            t0 = TimeDelta(data[\"user_timebin/met_time\"], format='sec')\n",
    "            tf = TimeDelta(data[\"user_timebin/met_stop_time\"], format='sec')\n",
    "        elif \"MJD\" in time_unit:\n",
    "            t0 = Time(data[time_str_start], format='mjd')\n",
    "            tf = Time(data[time_str_end], format='mjd')\n",
    "        else:\n",
    "            t0 = Time(data[\"user_timebin/utc_time\"])\n",
    "            tf = Time(data[\"user_timebin/utc_stop_time\"])\n",
    "    else:\n",
    "        if \"MET\" in time_unit:\n",
    "            t0 = TimeDelta(data[\"met_time\"], format='sec')\n",
    "        elif \"MJD\" in time_unit:\n",
    "            t0 = Time(data[time_str_start], format='mjd')\n",
    "        else:\n",
    "            t0 = Time(data[\"utc_time\"])\n",
    "        tf = t0 + TimeDelta(data[\"exposure\"], format='sec')\n",
    "\n",
    "    dt = tf - t0\n",
    "\n",
    "    if \"MET\" in time_unit:\n",
    "        time_center = 0.5 * (tf + t0).value\n",
    "        time_diff = 0.5 * (tf - t0).value\n",
    "    elif \"MJD\" in time_unit:\n",
    "        time_diff = 0.5 * (tf - t0)\n",
    "        time_center = t0 + time_diff\n",
    "        time_center = time_center.value\n",
    "        time_diff = time_diff.value\n",
    "\n",
    "    else:\n",
    "        time_diff = TimeDelta(0.5 * dt)  # dt.to_value('datetime')\n",
    "        time_center = t0 + time_diff\n",
    "\n",
    "        time_center = np.array([i.to_value('datetime64') for i in time_center])\n",
    "        time_diff = np.array([np.timedelta64(0.5 * i.to_datetime()) for i in dt])\n",
    "\n",
    "    x = time_center\n",
    "    xerr = time_diff\n",
    "\n",
    "    if obs_list_count == 0:\n",
    "        fig, axes = plt.subplots(len(values), sharex=True, figsize=(10,12))\n",
    "\n",
    "    axes_queue = [i for i in range(len(values))]\n",
    "    # plot_value=[i for i in values]\n",
    "\n",
    "    e_range_str = f\"{14}-{195} keV\"\n",
    "    #axes[0].set_title(object_name + '; survey data from ' + e_range_str)\n",
    "\n",
    "    for i in values:\n",
    "        ax = axes[axes_queue[0]]\n",
    "        axes_queue.pop(0)\n",
    "\n",
    "        y = data[i]\n",
    "        yerr = np.zeros(x.size)\n",
    "        y_upperlim = np.zeros(x.size)\n",
    "\n",
    "        label = i\n",
    "\n",
    "        if \"rate\" in i:\n",
    "            yerr = data[i + \"_err\"]\n",
    "            label = \"Count rate (cts/s)\"\n",
    "        elif i + \"_lolim\" in data.keys():\n",
    "            # get the errors\n",
    "            lolim = data[i + \"_lolim\"]\n",
    "            hilim = data[i + \"_hilim\"]\n",
    "\n",
    "            yerr = np.array([lolim, hilim])\n",
    "            y_upperlim = data[i + \"_upperlim\"]\n",
    "\n",
    "            # find where we have upper limits and set the error to 1 since the nan error value isnt\n",
    "            # compatible with upperlimits\n",
    "            yerr[:, y_upperlim] = 0.1 * y[y_upperlim]\n",
    "\n",
    "        if \"mosaic\" in observation_list:\n",
    "            if \"weekly\" in observation_list:\n",
    "                zorder = 9\n",
    "                c = \"blue\"\n",
    "                m = \"o\"\n",
    "                l=\"Weekly Mosaic\"\n",
    "                ms=5\n",
    "                a=0.8\n",
    "            else:\n",
    "                zorder = 9\n",
    "                c='green'\n",
    "                m = \"s\"\n",
    "                l = \"Monthly Mosaic\"\n",
    "                ms=7\n",
    "                a = 1\n",
    "        else:\n",
    "            zorder = 4\n",
    "            c = \"gray\"\n",
    "            m = \".\"\n",
    "            l = \"Survey Snapshot\"\n",
    "            ms=3\n",
    "            a = 0.3\n",
    "\n",
    "        ax.errorbar(x, y, xerr=xerr, yerr=yerr, uplims=y_upperlim, linestyle=\"None\", marker=m, markersize=ms,\n",
    "                    zorder=zorder, color=c, label=l, alpha=a)\n",
    "\n",
    "        if (\"flux\" in i.lower()):\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        if (\"snr\" in i.lower()):\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        ax.set_ylabel(label)\n",
    "\n",
    "    # if T0==0:\n",
    "    if \"MET\" in time_unit:\n",
    "        label_string = 'MET Time (s)'\n",
    "    elif \"MJD\" in time_unit:\n",
    "        label_string = 'MJD Time (s)'\n",
    "    else:\n",
    "        label_string = 'UTC Time (s)'\n",
    "\n",
    "    plt.gca().ticklabel_format(useMathText=True)\n",
    "    axes[-1].set_xlabel(label_string)\n",
    "\n",
    "    obs_list_count += 1\n",
    "\n",
    "#add the UTC times as well\n",
    "met_values=[126230399.334, 157766399.929]#[i.get_position()[0] for i in axes[-1].get_xticklabels()]\n",
    "utc_values=[np.datetime64(sbu.met2datetime(i)) for i in met_values]\n",
    "\n",
    "for i,j in zip(met_values, [2005, 2006]):\n",
    "    for ax in axes:\n",
    "        ax.axvline(i, 0, 1, ls='--', color='k')\n",
    "        if ax==axes[0]:\n",
    "            ax.text(i, ax.get_ylim()[1]*1.01, str(j), fontsize=12, ha='center')\n",
    "\n",
    "axes[0].legend(loc=\"best\")\n",
    "\n",
    "axes[1].set_ylabel(\"SNR\")\n",
    "axes[2].set_ylabel(r\"Flux (erg/s/cm$^2$)\")\n",
    "axes[3].set_ylabel(r\"$\\Gamma$\")\n",
    "\n",
    "for ax, l in zip(axes, [\"a\",\"b\",\"c\",\"d\"]):\n",
    "    ax.text(.99, .95, f\"({l})\", ha='right', va='top', transform=ax.transAxes,  fontsize=12)\n",
    "\n",
    "axes[-1].axhline(2.15, 0, 1)\n",
    "\n",
    "axes[-2].axhline(23342.70e-12, 0, 1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plot_filename = object_name + '_survey_lc.pdf'\n",
    "fig.savefig(plot_filename, bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
