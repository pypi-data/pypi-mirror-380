{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938c75e2",
   "metadata": {},
   "source": [
    "## This notebook is meant to accompany the trial_NGC2992.py file that is included in this directory with additional comments. \n",
    "\n",
    "### This analysis can produce $\\sim 900$ GB of data so be sure that there is enough storage on your computer.\n",
    "\n",
    "In this notebook, we will go through the code to produce Figures 4 & 5 of the associated BAT survey paper. This example outlines how to analyze BAT survey data to obtain a light curve/spectrum for an AGN, such as NGC 2992. \n",
    "\n",
    "First, we need to import our usual python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b801b12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: version mismatch between CFITSIO header (v40500) and linked library (v40400).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import batanalysis as ba\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import numpy as np\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "from pathlib import Path\n",
    "import swiftbat\n",
    "import swiftbat.swutil as sbu\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406db8ab",
   "metadata": {},
   "source": [
    "We will also import pyXspec to be able to easily manipulate our fitted mosaic spectrum and generate the plot of the data versus the fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb49bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xspec import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8058ad",
   "metadata": {},
   "source": [
    "Now we can query HEASARC to download data between Dec 15th 2004 and Dec 16th 2005 associated with BAT having the coordinates of NGC 2992 within its FOV and at least 1000 cm$^2$ of the detector plane is exposed to that point on the sky. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442578d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding everything finds 6172 observations, of which 849 have more than 1000 cm^2 coded\n"
     ]
    }
   ],
   "source": [
    "object_name='NGC2992'\n",
    "object_location = swiftbat.simbadlocation(object_name)\n",
    "object_batsource = swiftbat.source(ra=object_location[0], dec=object_location[1], name=object_name)\n",
    "\n",
    "table_everything, query = ba.from_heasarc(time_range=Time([\"2004-12-15\",\"2005-12-16\"]), return_query=True)\n",
    "\n",
    "minexposure = 1000     # cm^2 after cos adjust\n",
    "exposures = u.Quantity(\n",
    "    [object_batsource.exposure(ra=row[\"ra\"],\n",
    "                               dec=row[\"dec\"],\n",
    "                               roll=row[\"roll_angle\"])[0]\n",
    "        for row in table_everything\n",
    "    ])\n",
    "table_exposed = table_everything[exposures.value > minexposure]\n",
    "print(f\"Finding everything finds {len(table_everything)} observations, of which {len(table_exposed)} have more than {minexposure:0} cm^2 coded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88dc23",
   "metadata": {},
   "source": [
    "To download the data we would then do:\n",
    "```\n",
    "result = ba.download_swiftdata(table_exposed)\n",
    "```\n",
    "\n",
    "And to get the observation IDs for the successfully downloaded data we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c299e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ids = [i for i in table_exposed[\"obsid\"] if result[i][\"success\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0861a",
   "metadata": {},
   "source": [
    "Remember, that when continuing a prior analysis, we do not need to requery the database to get the observation IDs. Instead, we can simply do:\n",
    "```\n",
    "obs_ids=[i.name for i in sorted(ba.datadir().glob(\"*\")) if i.name.isnumeric()]\n",
    "```\n",
    "\n",
    "With the data downloaded and the list of observation IDs obtained, we can now process the survey observations by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b02302",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_map_dir=Path(\"/Users/tparsota/Documents/PATTERN_MAPS/\")\n",
    "batsurvey_obs=ba.parallel.batsurvey_analysis(obs_ids, patt_noise_dir=noise_map_dir, nprocs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50670d6",
   "metadata": {},
   "source": [
    "With the survey dataset analyzed, we can fit the spectra and obtain upper limits for the nondetections. We set the photon index for the power law that is fit to the 5$\\sigma$ upper limit spectrum to be 1.9 since this is what has been observed in the source in the past. \n",
    "\n",
    "Here, we set `nprocs = -2` so it will tell the python function to use 2 less than all the CPUs available on the users laptop. This is a convenient notation used by the python joblib package that can be used anywhere that nprocs is a value that can be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f141dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batsurvey_obs=ba.parallel.batspectrum_analysis(batsurvey_obs, object_name,  ul_pl_index=1.9, use_cstat=True, nprocs=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60c886",
   "metadata": {},
   "source": [
    "Now, we can view th elight curve showing our results by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65513c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=ba.plot_survey_lc(batsurvey_obs, id_list= object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dd209",
   "metadata": {},
   "source": [
    "Here, we see that we only have upper limits on the fluxes and thus do not have any detections. \n",
    "\n",
    "To try to get a detection, we can switch to looking at the AGN in monthly time bins. To do that, we follow the prescription outlined in our analysis of the Crab. We simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "outventory_file=ba.merge_outventory(batsurvey_obs)\n",
    "time_bins=ba.group_outventory(outventory_file, np.timedelta64(1, \"M\"), start_datetime=Time(\"2004-12-15\"), end_datetime=Time(\"2005-12-16\"))\n",
    "\n",
    "mosaic_list, total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file, time_bins, nprocs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab35b9",
   "metadata": {},
   "source": [
    "This produces our montly mosaics which we can then fit with spectra and plot those results on top of our individual survey dataset analysis to see if we get any detections on monthly time scales. The cell below does these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_list=ba.parallel.batspectrum_analysis(mosaic_list, object_name, ul_pl_index=1.9, use_cstat=True, nprocs=5)\n",
    "total_mosaic=ba.parallel.batspectrum_analysis(total_mosaic, object_name, ul_pl_index=1.9, use_cstat=True, nprocs=1)\n",
    "\n",
    "fig, axes=ba.plot_survey_lc([batsurvey_obs,mosaic_list], id_list= object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"], same_figure=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25518e9e",
   "metadata": {},
   "source": [
    "We can seee that the call to `plot_survey_lc` has a list of the batsurvey objects and the mosaics and we tell it to plot the two datasets simutaneously with the `same_figure` variable. \n",
    "\n",
    "Based on this plot, we still do not have a detection of the AGN. We can however take a look at our total time integrated mosaic and we will see that we do have a solid detection of the AGN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ae7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=ba.plot_survey_lc([batsurvey_obs,[total_mosaic]], id_list= object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"], same_figure=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6980879",
   "metadata": {},
   "source": [
    "Now, we will first save our data to create our light curve plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d40e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=ba.concatenate_data(batsurvey_obs, object_name, [\"met_time\", \"utc_time\", \"exposure\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "with open('all_data_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "all_data_monthly=ba.concatenate_data(mosaic_list, object_name, [\"user_timebin/met_time\", \"user_timebin/utc_time\", \"user_timebin/met_stop_time\", \"user_timebin/utc_stop_time\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "with open('monthly_mosaic_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(all_data_monthly, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830b078",
   "metadata": {},
   "source": [
    "To create the light curve plots we now do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_range=None\n",
    "time_unit=\"MET\"\n",
    "values=[\"rate\",\"snr\", \"flux\"]\n",
    "\n",
    "survey_obsid_list=[\"all_data_dictionary\", \"monthly_mosaic_dictionary\"]\n",
    "\n",
    "obs_list_count=0\n",
    "for observation_list in survey_obsid_list:\n",
    "\n",
    "    with open(observation_list+\".pkl\", 'rb') as f:\n",
    "        all_data=pickle.load(f)\n",
    "        data=all_data[object_name]\n",
    "\n",
    "    # get the time centers and errors\n",
    "    if \"mosaic\" in observation_list:\n",
    "\n",
    "        if \"MET\" in time_unit:\n",
    "            t0 = TimeDelta(data[\"user_timebin/met_time\"], format='sec')\n",
    "            tf = TimeDelta(data[\"user_timebin/met_stop_time\"], format='sec')\n",
    "        elif \"MJD\" in time_unit:\n",
    "            t0 = Time(data[time_str_start], format='mjd')\n",
    "            tf = Time(data[time_str_end], format='mjd')\n",
    "        else:\n",
    "            t0 = Time(data[\"user_timebin/utc_time\"])\n",
    "            tf = Time(data[\"user_timebin/utc_stop_time\"])\n",
    "    else:\n",
    "        if \"MET\" in time_unit:\n",
    "            t0 = TimeDelta(data[\"met_time\"], format='sec')\n",
    "        elif \"MJD\" in time_unit:\n",
    "            t0 = Time(data[time_str_start], format='mjd')\n",
    "        else:\n",
    "            t0 = Time(data[\"utc_time\"])\n",
    "        tf = t0 + TimeDelta(data[\"exposure\"], format='sec')\n",
    "\n",
    "    dt = tf - t0\n",
    "\n",
    "    if \"MET\" in time_unit:\n",
    "        time_center = 0.5 * (tf + t0).value\n",
    "        time_diff = 0.5 * (tf - t0).value\n",
    "    elif \"MJD\" in time_unit:\n",
    "        time_diff = 0.5 * (tf - t0)\n",
    "        time_center = t0 + time_diff\n",
    "        time_center = time_center.value\n",
    "        time_diff = time_diff.value\n",
    "\n",
    "    else:\n",
    "        time_diff = TimeDelta(0.5 * dt)  # dt.to_value('datetime')\n",
    "        time_center = t0 + time_diff\n",
    "\n",
    "        time_center = np.array([i.to_value('datetime64') for i in time_center])\n",
    "        time_diff = np.array([np.timedelta64(0.5 * i.to_datetime()) for i in dt])\n",
    "\n",
    "    x = time_center\n",
    "    xerr = time_diff\n",
    "\n",
    "    if obs_list_count == 0:\n",
    "        fig, axes = plt.subplots(len(values), sharex=True) #, figsize=(10,12))\n",
    "\n",
    "    axes_queue = [i for i in range(len(values))]\n",
    "    # plot_value=[i for i in values]\n",
    "\n",
    "    e_range_str = f\"{14}-{195} keV\"\n",
    "    #axes[0].set_title(object_name + '; survey data from ' + e_range_str)\n",
    "\n",
    "    for i in values:\n",
    "        ax = axes[axes_queue[0]]\n",
    "        axes_queue.pop(0)\n",
    "\n",
    "        y = data[i]\n",
    "        yerr = np.zeros(x.size)\n",
    "        y_upperlim = np.zeros(x.size)\n",
    "\n",
    "        label = i\n",
    "\n",
    "        if \"rate\" in i:\n",
    "            yerr = data[i + \"_err\"]\n",
    "            label = \"Count rate (cts/s)\"\n",
    "        elif i + \"_lolim\" in data.keys():\n",
    "            # get the errors\n",
    "            lolim = data[i + \"_lolim\"]\n",
    "            hilim = data[i + \"_hilim\"]\n",
    "\n",
    "            yerr = np.array([lolim, hilim])\n",
    "            y_upperlim = data[i + \"_upperlim\"]\n",
    "\n",
    "            # find where we have upper limits and set the error to 1 since the nan error value isnt\n",
    "            # compatible with upperlimits\n",
    "            yerr[:, y_upperlim] = 0.4 * y[y_upperlim]\n",
    "\n",
    "        if \"mosaic\" in observation_list:\n",
    "            if \"weekly\" in observation_list:\n",
    "                zorder = 9\n",
    "                c = \"blue\"\n",
    "                m = \"o\"\n",
    "                l=\"Weekly Mosaic\"\n",
    "                ms=5\n",
    "                a=0.8\n",
    "            else:\n",
    "                zorder = 9\n",
    "                c='green'\n",
    "                m = \"s\"\n",
    "                l = \"Monthly Mosaic\"\n",
    "                ms=7\n",
    "                a = 1\n",
    "        else:\n",
    "            zorder = 4\n",
    "            c = \"gray\"\n",
    "            m = \".\"\n",
    "            l = \"Survey Snapshot\"\n",
    "            ms=3\n",
    "            a = 0.3\n",
    "\n",
    "        ax.errorbar(x, y, xerr=xerr, yerr=yerr, uplims=y_upperlim, linestyle=\"None\", marker=m, markersize=ms,\n",
    "                    zorder=zorder, color=c, label=l, alpha=a)\n",
    "                    \n",
    "        #plt.gca().ticklabel_format(useMathText=True)\n",
    "        ax.xaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "\n",
    "\n",
    "        if (\"flux\" in i.lower()):\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        if (\"snr\" in i.lower()):\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        ax.set_ylabel(label)\n",
    "\n",
    "    # if T0==0:\n",
    "    if \"MET\" in time_unit:\n",
    "        label_string = 'MET Time (s)'\n",
    "    elif \"MJD\" in time_unit:\n",
    "        label_string = 'MJD Time (s)'\n",
    "    else:\n",
    "        label_string = 'UTC Time (s)'\n",
    "\n",
    "    axes[-1].set_xlabel(label_string)\n",
    "    \n",
    "    obs_list_count += 1\n",
    "\n",
    "\n",
    "#add the UTC times as well\n",
    "utc_time=Time([\"2005-01-01\", \"2006-01-01\"])\n",
    "met_time=[]\n",
    "for i in utc_time:\n",
    "    met_time.append(sbu.datetime2met(i.datetime, correct=True))\n",
    "\n",
    "for i,j in zip(met_time, utc_time.ymdhms):\n",
    "    for ax in axes:\n",
    "        ax.axvline(i, 0, 1, ls='--', color='k')\n",
    "        if ax==axes[0]:\n",
    "            ax.text(i, ax.get_ylim()[1]*1.03, f'{j[\"year\"]}', fontsize=10, ha='center')\n",
    "\n",
    "axes[1].set_ylabel(\"SNR\")\n",
    "axes[2].set_ylabel(r\"Flux (erg/s/cm$^2$)\")\n",
    "\n",
    "axes[1].legend(loc= \"lower center\", ncol=2)\n",
    "\n",
    "for ax, l in zip(axes, [\"a\",\"b\",\"c\",\"d\"]):\n",
    "    ax.text(0.01, .95, f\"({l})\", ha='left', va='top', transform=ax.transAxes,  fontsize=12)\n",
    "\n",
    "fig.tight_layout()\n",
    "plot_filename = object_name + '_survey_lc.pdf'\n",
    "fig.savefig(plot_filename, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52742de0",
   "metadata": {},
   "source": [
    "We can also now make a plot of our year long time-integrated spectrum and the best fit model. Here we use the saved xspec session info that was produced by BatAnalysis to load the xspec information related to the fit. This lets us get the model spectra in each energy bin once it has been folded through the response function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32155c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1)\n",
    "pha_file=total_mosaic.get_pha_filenames(id_list=object_name)[0]\n",
    "emax=np.array(total_mosaic.emax)\n",
    "emin=np.array(total_mosaic.emin)\n",
    "ecen=0.5*(emin+emax)\n",
    "\n",
    "os.chdir(pha_file.parent)\n",
    "\n",
    "with fits.open(pha_file.name) as file:\n",
    "    pha_data=file[1].data\n",
    "    energies=file[-2].data\n",
    "\n",
    "#get the xspec model info\n",
    "mosaic_pointing_info=total_mosaic.get_pointing_info(\"mosaic\", source_id=object_name)\n",
    "xspec_session_name=mosaic_pointing_info['xspec_model'].name\n",
    "flux=10**mosaic_pointing_info[\"model_params\"][\"lg10Flux\"][\"val\"]\n",
    "flux_err=10**np.array([mosaic_pointing_info[\"model_params\"][\"lg10Flux\"][\"lolim\"], mosaic_pointing_info[\"model_params\"][\"lg10Flux\"][\"hilim\"]])\n",
    "flux_diff=np.abs(flux-flux_err)\n",
    "\n",
    "\n",
    "phoindex=mosaic_pointing_info[\"model_params\"][\"PhoIndex\"][\"val\"]\n",
    "phoindex_err=np.array([mosaic_pointing_info[\"model_params\"][\"PhoIndex\"][\"lolim\"], mosaic_pointing_info[\"model_params\"][\"PhoIndex\"][\"hilim\"]])\n",
    "phoindex_diff=np.abs(phoindex-phoindex_err)\n",
    "\n",
    "\n",
    "xsp.Xset.restore(xspec_session_name)\n",
    "xsp.Plot.device = \"/null\"\n",
    "xsp.Plot(\"data resid\")\n",
    "energies = xsp.Plot.x()\n",
    "edeltas = xsp.Plot.xErr()\n",
    "rates = xsp.Plot.y(1,1)\n",
    "errors = xsp.Plot.yErr(1,1)\n",
    "foldedmodel = xsp.Plot.model()\n",
    "dataLabels = xsp.Plot.labels(1)\n",
    "residLabels = xsp.Plot.labels(2)\n",
    "\n",
    "foldedmodel.append(foldedmodel[-1])\n",
    "xspec_energy=total_mosaic.emin.copy()\n",
    "xspec_energy.append(total_mosaic.emax[-1])\n",
    "xspec_energy=np.array(xspec_energy)\n",
    "\n",
    "\n",
    "ax.loglog(emin, pha_data['RATE'], color='k', drawstyle='steps-post')\n",
    "ax.loglog(emax, pha_data['RATE'], color='k', drawstyle='steps-pre')\n",
    "ax.errorbar(ecen, pha_data[\"RATE\"], yerr=pha_data['STAT_ERR'], color='k', marker='None', ls='None', label=object_name+\" 1 Year Mosaic Spectrum\")\n",
    "ax.set_ylabel(\"Count Rate (cts/s)\", fontsize=14)\n",
    "ax.set_xlabel(\"E (keV)\", fontsize=14)\n",
    " \n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "l=f\"Folded Model:\\nFlux={flux/1e-11:-.3}$^{{{flux_diff[1]/1e-11:+.3}}}_{{{-1*flux_diff[0]/1e-11:+.3}}} \\\\times 10^{{-11}}$ erg/s/cm$^2$\"+f\"\\n$\\Gamma$={phoindex:-.3}$^{{{phoindex_diff[1]:+.2}}}_{{{-1*phoindex_diff[0]:+.2}}}$\"\n",
    "ax.loglog(xspec_energy, foldedmodel, color='r', drawstyle='steps-post', label=l)\n",
    "ax.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(object_name+\"_1year_spectrum.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
