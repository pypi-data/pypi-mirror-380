# augllm

[English README is here](./README.en.md)

**augllm** は、Ollama を用いた拡張型（Augmented）大規模言語モデル（LLM）操作のためのラッパーです。  
Function Calling による外部ツール利用のためのインターフェースを提供します。  
ただし、ツール本体の実装は含まず、利用者が必要に応じて外部実装を組み込む形を想定しています。

PyPIに登録されているため、pip install でインストールできます。
https://pypi.org/project/augllm/

## 目次

1. 特長 / 概要  
2. 前提条件  
3. インストール方法  
4. 使い方  
   - サンプルプログラム 
   - Function Calling との連携  
6. ライセンス  

## 1. 特長 / 概要

- Ollama を通じて LLM（ローカルまたはクラウド上）とやり取り  
- Function Calling（関数呼び出し）形式でのツール連携をサポート  
- ツールは抽象インターフェースとして定義されており、具体的な実装（API 呼び出し、ローカルスクリプト実行など）はユーザーが自由に実装可能  
- 拡張性重視：カスタムツール、チェーン処理、プロンプト設計などへの組み込み容易  

## 2. 前提条件

- Python 3.11 以上  
- Ollama コマンドラインや API クライアントが動作可能な環境  

## 3. インストール方法
1. 仮想環境の作成と起動

以下を実行し、仮想環境を作成します。
```
python -m venv env
```

仮想環境を起動します。Macの場合は以下を実行します。
```
source env/bin/activate
```

2. ライブラリのインストール

通常のライブラリとして使う場合は以下を実行します。
```bash
pip install augllm
```

このライブラリ自体の改良を行いたい場合は以下を実行します。
```bash
pip install -e .
```

## 4. 使い方

### サンプルプログラム
リポジトリ内に `test/` ディレクトリがあります。中の2つのファイルを参考にしてください。

### Function Calling との連携

1. プロンプト内で、ツール呼び出しを期待する関数署名を与える  
2. モデルから返ってきた関数呼び出し要求（ツール名 + 引数）を受け取り  
3. 対応ツールインターフェースの `run(...)` を呼び出し、結果を受け取り  
4. 結果をモデルに返して最終応答を得る  

## 5. ライセンス

このプロジェクトは **Apache‑2.0 ライセンス** のもとで公開されています。
詳細は、[LICENSE](https://github.com/ToPo-ToPo-ToPo/augllm/blob/main/LICENSE)ファイルを確認してください。

