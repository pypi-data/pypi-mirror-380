import requests
from .exceptions import (
    OpenAIError,
    AnthropicError,
    GeminiError,
    MistralError,
    DeepSeekError,
)


class OpenAIAPI:
    def __init__(self, api_key: str = "YOUR_API_KEY", model: str = "gpt-5") -> None:
        self._model = model
        self._headers = {"Authorization": f"Bearer {api_key}"}
        self._openai_endpoint = "https://api.openai.com/v1/chat/completions"

    def simple_request(
        self,
        user_prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        temperature: float = 1,
        top_p: float = 1,
        max_completion_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Sends a simple request to the OpenAI API and returns the generated response.
        Args:
            user_prompt (str): The prompt from the user to send to the API.
            system_prompt (str, optional): The system prompt that defines the assistant's behavior.
                Defaults to "You are a helpful assistant.".
            temperature (float, optional): Controls randomness in the output. Higher values mean more random outputs.
                Defaults to 1.
            top_p (float, optional): Controls diversity via nucleus sampling.
                Defaults to 1.
            max_completion_tokens (int, optional): The maximum number of tokens to generate.
                Defaults to 2048.
            **kwargs: Additional keyword arguments to include in the request payload.
        Returns:
            str: The content of the message generated by the OpenAI API.
        Raises:
            OpenAIError: If the API request fails or returns an error status code.
        """
        data = {
            "model": self._model,
            "temperature": temperature,
            "top_p": top_p,
            "max_completion_tokens": max_completion_tokens,
            "messages": [
                {"role": "developer", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            **kwargs,
        }

        response = requests.post(
            self._openai_endpoint, json=data, headers=self._headers
        )
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            try:
                error_msg = response.json()["error"]["message"]
            except Exception:
                error_msg = response.text
            raise OpenAIError(f"Error {response.status_code} - {error_msg}")


class AnthropicAPI:
    def __init__(
        self, api_key: str = "YOUR_API_KEY", model: str = "claude-3-7-sonnet-20250219"
    ) -> None:
        self._model = model
        self._headers = {"x-api-key": f"{api_key}", "anthropic-version": "2023-06-01"}
        self._anthropic_endpoint = "https://api.anthropic.com/v1/messages"

    def simple_request(
        self,
        user_prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        temperature: float = 1,
        max_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Sends a simple request to the Anthropic API and returns the generated response.
        Args:
            user_prompt (str): The prompt from the user to send to the API.
            system_prompt (str, optional): The system prompt that defines the assistant's behavior.
                Defaults to "You are a helpful assistant.".
            temperature (float, optional): Controls randomness in the output. Higher values mean more random outputs.
                Defaults to 1.
            max_tokens (int, optional): The maximum number of tokens to generate.
                Defaults to 2048.
            **kwargs: Additional keyword arguments to include in the request payload.
        Returns:
            str: The content of the message generated by the API.
        Raises:
            AnthropicError: If the API request fails or returns an error status code.
        """
        data = {
            "model": self._model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "system": system_prompt,
            "messages": [{"role": "user", "content": user_prompt}],
            **kwargs,
        }

        response = requests.post(
            self._anthropic_endpoint, json=data, headers=self._headers
        )
        if response.status_code == 200:
            return response.json()["content"][0]["text"]
        else:
            try:
                error_msg = response.json()["error"]["message"]
            except Exception:
                error_msg = response.text
            raise AnthropicError(f"Error {response.status_code} - {error_msg}")


class GeminiAPI:
    def __init__(
        self, api_key: str = "YOUR_API_KEY", model: str = "gemini-2.0-flash"
    ) -> None:
        self._model = model
        self._parameters = {"key": api_key}
        self._gemini_endpoint = f"https://generativelanguage.googleapis.com/v1beta/models/{self._model}:generateContent"

    def simple_request(
        self,
        user_prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        temperature: float = 1,
        top_k: int = 40,
        top_p: float = 0.95,
        max_output_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Sends a simple request to the Gemini API and returns the generated response.
        Args:
            user_prompt (str): The prompt from the user to send to the API.
            system_prompt (str, optional): The system prompt that defines the assistant's behavior.
                Defaults to "You are a helpful assistant.".
            temperature (float, optional): Controls randomness in the output. Higher values mean more random outputs.
                Defaults to 1.
            top_k (int, optional): Limits the token selection to the k most likely tokens.
                Defaults to 40.
            top_p (float, optional): Controls diversity via nucleus sampling.
                Defaults to 0.95.
            max_tokens (int, optional): The maximum number of tokens to generate.
                Defaults to 2048.
            **kwargs: Additional keyword arguments to include in the request payload.
        Returns:
            str: The content of the message generated by the API.
        Raises:
            GeminiError: If the API request fails or returns an error status code.
        """
        self._gemini_endpoint = f"https://generativelanguage.googleapis.com/v1beta/models/{self._model}:generateContent"

        data = {
            "contents": [{"role": "user", "parts": [{"text": user_prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "topK": top_k,
                "topP": top_p,
                "maxOutputTokens": max_output_tokens,
                "responseMimeType": "text/plain",
                **kwargs
            },
        }

        if self._model.startswith("gemma"):
            data["contents"][0]["parts"] = [{"text": system_prompt}] + data["contents"][
                0
            ]["parts"]
        else:
            data["systemInstruction"] = {"parts": [{"text": system_prompt}]}

        response = requests.post(
            self._gemini_endpoint, json=data, params=self._parameters
        )
        if response.status_code == 200:
            return response.json()["candidates"][0]["content"]["parts"][0]["text"]
        else:
            try:
                error_msg = response.json()["error"]["message"]
            except Exception:
                error_msg = response.text
            raise GeminiError(f"Error {response.status_code} - {error_msg}")


class MistralAPI:
    def __init__(
        self, api_key: str = "YOUR_API_KEY", model: str = "mistral-large-latest"
    ) -> None:
        self._model = model
        self._headers = {"Authorization": f"Bearer {api_key}"}
        self._mistral_endpoint = "https://api.mistral.ai/v1/chat/completions"

    def simple_request(
        self,
        user_prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        temperature: float = 0.7,
        top_p: float = 1,
        max_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Sends a simple request to the Mistral API and returns the generated response.
        Args:
            user_prompt (str): The prompt from the user to send to the API.
            system_prompt (str, optional): The system prompt that defines the assistant's behavior.
                Defaults to "You are a helpful assistant.".
            temperature (float, optional): Controls randomness in the output. Higher values mean more random outputs.
                Defaults to 0.7.
            top_p (float, optional): Controls diversity via nucleus sampling.
                Defaults to 1.
            max_tokens (int, optional): The maximum number of tokens to generate.
                Defaults to 2048.
            **kwargs: Additional keyword arguments to include in the request payload.
        Returns:
            str: The content of the message generated by the API.
        Raises:
            MistralError: If the API request fails or returns an error status code.
        """
        data = {
            "model": self._model,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            **kwargs,
        }

        response = requests.post(
            self._mistral_endpoint, json=data, headers=self._headers
        )
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            try:
                error_msg = response.json()["detail"][0]["msg"]
            except Exception:
                error_msg = response.text
            raise MistralError(f"Error {response.status_code} - {error_msg}")


class DeepSeekAPI:
    def __init__(
        self, api_key: str = "YOUR_API_KEY", model: str = "deepseek-chat"
    ) -> None:
        self._model = model
        self._headers = {"Authorization": f"Bearer {api_key}"}
        self._deepseek_endpoint = "https://api.deepseek.com/chat/completions"

    def simple_request(
        self,
        user_prompt: str,
        system_prompt: str = "You are a helpful assistant.",
        temperature: float = 1,
        top_p: float = 1,
        max_tokens: int = 2048,
        **kwargs
    ) -> str:
        """
        Sends a simple request to the DeepSeek API and returns the generated response.
        Args:
            user_prompt (str): The prompt from the user to send to the API.
            system_prompt (str, optional): The system prompt that defines the assistant's behavior.
                Defaults to "You are a helpful assistant.".
            temperature (float, optional): Controls randomness in the output. Higher values mean more random outputs.
                Defaults to 1.
            top_p (float, optional): Controls diversity via nucleus sampling.
                Defaults to 1.
            max_tokens (int, optional): The maximum number of tokens to generate.
                Defaults to 2048.
            **kwargs: Additional keyword arguments to include in the request payload.
        Returns:
            str: The content of the message generated by the API.
        Raises:
            DeepSeekError: If the API request fails or returns an error status code.
        """
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
            **kwargs
        }

        response = requests.post(
            self._deepseek_endpoint, json=data, headers=self._headers
        )
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            try:
                error_msg = response.json()["error"]["message"]
            except Exception:
                error_msg = response.text
            raise DeepSeekError(f"Error {response.status_code} - {error_msg}")
