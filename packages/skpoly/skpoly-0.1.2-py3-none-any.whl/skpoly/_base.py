import itertools

import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted


class _BasePolynomialBasisTransformer(BaseEstimator, TransformerMixin):
    """Base class for polynomial basis transformers.

    Parameters
    ----------
    degree : int, default=3
        Highest degree of the polynomial basis.
    feature_range : tuple of float, default=(0.0, 1.0)
        Interval ``[a, b]`` that bounds the input features.
    include_bias : bool, default=False
        If ``True``, include the first basis function of each feature.
    tensor_product : bool, default=False
        If ``True``, append tensor product features for each pair of columns.

    Attributes
    ----------
    n_features_in_ : int
        Number of features in the input passed to :meth:`fit`.
    feature_range_ : tuple of float
        Validated feature range.
    n_output_features_ : int
        Total number of output features generated by :meth:`transform`.
    """

    def __init__(
        self,
        *,
        degree: int = 3,
        feature_range: tuple[float, float] = (0.0, 1.0),
        include_bias: bool = False,
        tensor_product: bool = False,
    ) -> None:
        self.degree = degree
        self.feature_range = feature_range
        self.include_bias = include_bias
        self.tensor_product = tensor_product

    def _validate_params(self) -> None:
        if not isinstance(self.degree, int) or self.degree < 0:
            raise ValueError("degree must be a non-negative integer")

        if not isinstance(self.feature_range, tuple) or len(self.feature_range) != 2:
            raise ValueError("feature_range must be a tuple of two floats")

        a, b = self.feature_range
        if not np.isfinite(a) or not np.isfinite(b):
            raise ValueError("feature_range values must be finite")

        if b <= a:
            raise ValueError("feature_range must satisfy a < b")

        if not isinstance(self.include_bias, bool):
            raise ValueError("include_bias must be a boolean")

        if not isinstance(self.tensor_product, bool):
            raise ValueError("tensor_product must be a boolean")

    def fit(self, X, y=None):  # noqa: D401
        """Validate parameters and learn the number of features in *X*."""
        self._validate_params()

        X = check_array(
            X,
            accept_sparse=False,
            dtype=np.float64,
            force_all_finite=False,
        )

        if X.ndim != 2:
            raise ValueError("X must be a 2D array")

        self.n_features_in_ = X.shape[1]
        self.feature_range_ = (float(self.feature_range[0]), float(self.feature_range[1]))
        self._setup()
        basis_dim = int(self._basis_dimension())
        if basis_dim <= 0:
            raise ValueError("_basis_dimension must be positive")
        self._basis_dim_ = basis_dim
        n_per_feature = basis_dim - (0 if self.include_bias else 1)
        self._n_basis_per_feature_ = max(n_per_feature, 0)
        single_features = self.n_features_in_ * self._n_basis_per_feature_
        if self.tensor_product and self.n_features_in_ > 1 and basis_dim > 0:
            per_pair = basis_dim * basis_dim - 1
            n_pairs = self.n_features_in_ * (self.n_features_in_ - 1) // 2
            interaction_features = n_pairs * per_pair
        else:
            interaction_features = 0
        self.n_output_features_ = single_features + interaction_features
        return self

    def transform(self, X):  # noqa: D401
        """Transform *X* into the polynomial basis representation."""
        check_is_fitted(self, "n_features_in_")
        X = check_array(
            X,
            accept_sparse=False,
            dtype=np.float64,
            force_all_finite=False,
        )

        if X.shape[1] != self.n_features_in_:
            raise ValueError("X has a different number of features than seen during fit")

        lower, upper = self.feature_range_
        nan_mask = np.isnan(X)
        X_filled = np.where(nan_mask, lower, X)
        X_clipped = np.clip(X_filled, lower, upper)
        scaled = self._scale_to_basis(X_clipped)
        basis_full = self._evaluate_basis(scaled)
        basis_full = np.where(nan_mask[..., np.newaxis], 0.0, basis_full)

        if not self.include_bias:
            basis = basis_full[..., 1:]
        else:
            basis = basis_full

        features = basis.reshape(X.shape[0], -1)

        if self.tensor_product and self.n_features_in_ > 1 and basis_full.shape[-1] > 0:
            interaction_features = []
            for i, j in itertools.combinations(range(self.n_features_in_), 2):
                a = basis_full[:, i, :]
                b = basis_full[:, j, :]
                product = np.einsum("si,sj->sij", a, b, optimize=True)
                product = product.reshape(X.shape[0], -1)
                if product.shape[1] > 0:
                    product = product[:, 1:]
                interaction_features.append(product)

            if interaction_features:
                interactions = np.concatenate(interaction_features, axis=1)
                if features.size == 0:
                    features = interactions
                else:
                    features = np.concatenate([features, interactions], axis=1)

        return features

    # Methods to be implemented by subclasses ---------------------------------

    def _setup(self) -> None:
        """Prepare subclass specific data during fitting."""

    def _scale_to_basis(self, X: np.ndarray) -> np.ndarray:
        raise NotImplementedError

    def _evaluate_basis(self, X: np.ndarray) -> np.ndarray:
        raise NotImplementedError

    def _basis_dimension(self) -> int:
        return self.degree + 1


__all__ = ["_BasePolynomialBasisTransformer"]
