"""Data processing utilities for ADRIOs."""

import dataclasses
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from functools import partial, reduce
from typing import (
    Any,
    Callable,
    Generic,
    Literal,
    Mapping,
    NamedTuple,
    Protocol,
    Self,
    Sequence,
    TypeVar,
    cast,
)

import numpy as np
import pandas as pd
from numpy.typing import NDArray
from typing_extensions import override

from epymorph.util import DateValueType, to_date_value_array

DataT = TypeVar("DataT", bound=np.generic)
"""A numpy array dtype."""


class HasRandomness(Protocol):
    """Protocol for an object containing a numpy random number generator."""

    @property
    @abstractmethod
    def rng(self) -> np.random.Generator:
        """The random number generator instance."""


#######
# Fix #
#######


class Fix(ABC, Generic[DataT]):
    """
    A method for fixing data issues as part of a `DataPipeline`. `Fix` instances act as
    functions (they have call semantics).

    `Fix` is generic in the dtype of the data it fixes (`DataT`).
    """

    @abstractmethod
    def __call__(
        self,
        rng: HasRandomness,
        replace: DataT,
        columns: tuple[str, ...],
        data_df: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        Apply this fix to some data.

        Parameters
        ----------
        rng : HasRandomness
            A source of randomness.
        replace : DataT
            The value to replace.
        columns : tuple[str, ...]
            The names of the columns to fix.
        data_df : DataFrame
            The data to fix.

        Returns
        -------
        DataFrame
            The data with the fix applied (a copy if modified).
        """

    @staticmethod
    def of_int64(fix: "FixLikeInt") -> "Fix[np.int64]":
        """
        Construct for a `Fix` for `int64` data. The type of `Fix`
        returned depends on the type of argument provided.

        Parameters
        ----------
        fix :
            A value which implies the type of fix to apply:

            - `Fix[np.int64]`: is returned unchanged (no-op)
            - `int`: returns a `ConstantFix`, to replace bad values with a constant
            - `Callable[[], int]`: return a `FunctionFix`, to replace bad values
              with values obtained from the given callable
            - `False`: return a `DontFix`, indicating not to replace bad values

        Returns
        -------
        :
            A Fix instance as determined by the type of the argument.
        """
        if fix is False:
            return DontFix()
        elif isinstance(fix, Fix):
            return fix  # for convenience, no-op if arg is a Fix
        elif isinstance(fix, int):
            return ConstantFix[np.int64](fix)  # type: ignore
        elif callable(fix):
            return FunctionFix[np.int64](fix)  # type: ignore
        raise ValueError("Not a valid fix.")

    @staticmethod
    def of_float64(fix: "FixLikeFloat") -> "Fix[np.float64]":
        """
        Construct for a `Fix` for `float64` data. The type of `Fix`
        returned depends on the type of argument provided.

        Parameters
        ----------
        fix :
            A value which implies the type of fix to apply.

            - `Fix[np.float64]`: is returned unchanged (no-op)
            - `float`: returns a `ConstantFix`, to replace bad values with a constant
            - `Callable[[], float]`: return a `FunctionFix`, to replace bad values
              with values obtained from the given callable
            - `False`: return a `DontFix`, indicating not to replace bad values

        Returns
        -------
        :
            A `Fix` instance as determined by the type of the argument.
        """
        if fix is False:
            return DontFix()
        elif isinstance(fix, Fix):
            return fix  # for convenience, no-op if arg is a Fix
        elif isinstance(fix, float):
            return ConstantFix[np.float64](fix)  # type: ignore
        elif callable(fix):
            return FunctionFix[np.float64](fix)  # type: ignore
        raise ValueError("Not a valid fix.")


@dataclass(frozen=True)
class ConstantFix(Fix[DataT]):
    """
    A `Fix` which replaces values with a constant value.

    `ConstantFix` is generic in the dtype of the data it fixes (`DataT`).

    Parameters
    ----------
    with_value :
        The value to use to replace bad values.
    """

    with_value: DataT
    """The value to use to replace bad values."""

    @override
    def __call__(self, rng, replace, columns, data_df):
        return data_df.assign(
            **{
                col: data_df[col].replace(replace, self.with_value)  # type: ignore
                for col in columns
            }
        )


@dataclass(frozen=True)
class FunctionFix(Fix[DataT]):
    """
    A `Fix` which replaces values with values generated by the given function.

    `FunctionFix` is generic in the dtype of the data it fixes (`DataT`).

    Parameters
    ----------
    with_function :
        The function that generates replacement values.
    """

    with_function: Callable[[], DataT]
    """The function that generates replacement values."""

    @override
    def __call__(self, rng, replace, columns, data_df):
        return FunctionFix.apply(data_df, replace, columns, self.with_function)

    @staticmethod
    def apply(
        data_df: pd.DataFrame,
        replace: DataT,
        columns: tuple[str, ...],
        with_function: Callable[[], DataT],
    ) -> pd.DataFrame:
        """
        Apply a `FunctionFix` to a data frame.

        This method can be useful in creating other `Fix` instances, when their
        replacement value logic can be expressed as a no-parameter function.

        Parameters
        ----------
        data_df :
            The data to fix.
        replace :
            The value to replace.
        columns :
            The data columns to fix.
        with_function :
            The function used to generate replacement values.

        Returns
        -------
        :
            A copy of the data with bad values fixed.
        """

        def replace_col(col: str) -> pd.Series:
            is_replace = data_df[col] == replace
            num_replace = is_replace.sum()
            replacements = np.array([with_function() for _ in range(num_replace)])
            updated = data_df[col].copy()
            updated[is_replace] = replacements
            return updated

        return data_df.assign(**{col: replace_col(col) for col in columns})


@dataclass(frozen=True)
class RandomFix(Fix[DataT]):
    """
    A `Fix` which replaces values with randomly-generated values.

    `RandomFix` is generic in the dtype of the data it fixes (`DataT`).

    Parameters
    ----------
    with_random :
        A function for generating replacement values using the given numpy random number
        generator.
    """

    with_random: Callable[[np.random.Generator], DataT]
    """
    A function for generating replacement values using the given numpy random number
    generator.
    """

    @override
    def __call__(self, rng, replace, columns, data_df):
        random_fn = partial(self.with_random, rng.rng)
        return FunctionFix.apply(data_df, replace, columns, random_fn)

    @staticmethod
    def from_range(low: int, high: int) -> "RandomFix[np.int64]":
        """
        Construct for a `RandomFix` which replaces values
        with values sampled uniformly from a discrete range of integers.

        Parameters
        ----------
        low :
            The lowest replacement value.
        high :
            The highest replacement value.

        Returns
        -------
        :
            The fix instance.
        """
        return RandomFix(
            lambda rng: rng.integers(low, high, endpoint=True),  # type: ignore
        )

    @staticmethod
    def from_range_float(low: float, high: float) -> "RandomFix[np.float64]":
        """
        Construct for a `RandomFix` which replaces values
        with values sampled uniformly from a continuous range.

        Parameters
        ----------
        low :
            The low end of the range of replacement values.
        high :
            The high end of the range of replacement values.
            (Not included in the possible values.)

        Returns
        -------
        :
            The fix instance.
        """
        return RandomFix(
            lambda rng: rng.uniform(low, high),  # type: ignore
        )


@dataclass(frozen=True)
class DontFix(Fix[Any]):
    """A special `Fix` which simply returns the data as-is (no-op)."""

    @override
    def __call__(self, rng, replace, columns, data_df):
        return data_df


FixLikeInt = Fix[np.int64] | int | Callable[[], int] | Literal[False]
"""A value which can be coerced into a `Fix` for integers."""

FixLikeFloat = Fix[np.float64] | float | Callable[[], float] | Literal[False]
"""A value which can be coerced into a `Fix` for floats."""


########
# Fill #
########


class Fill(ABC, Generic[DataT]):
    """
    A method for filling-in missing data as part of a `DataPipeline`. `Fill` instances
    act as functions (they have call semantics).

    `Fill` is generic in the dtype of the data it fixes (`DataT`).
    """

    @abstractmethod
    def __call__(
        self,
        rng: HasRandomness,
        data_np: NDArray[DataT],
        missing_mask: NDArray[np.bool_],
    ) -> tuple[NDArray[DataT], NDArray[np.bool_] | None]:
        """
        Apply this fix to some data.

        Parameters
        ----------
        rng :
            A source of randomness.
        data_np :
            The data to fix.
        missing_mask :
            A mask indicating values which should be considered missing.

        Returns
        -------
        :
            A tuple containing two values:

            - a copy of the data with the fill applied (a copy if modified),
            - an updated missing values mask. `Fill` methods may or may not
              replace all missing values; if all missing values have been filled,
              this will be `None`.
        """

    @staticmethod
    def of_int64(fill: "FillLikeInt") -> "Fill[np.int64]":
        """
        Construct for a `Fill` for `int64` data. The type of `Fill`
        returned depends on the type of argument provided.

        Parameters
        ----------
        fill :
            A value which implies the type of fix to apply.

            - `Fill[np.int64]`: is returned unchanged (no-op)
            - `int`: returns a `ConstantFill`, to replace missing values with a constant
            - `Callable[[], int]`: return a `FunctionFill`, to replace missing values
              with values obtained from the given callable
            - `False`: return a `DontFill`, indicating not to replace missing values

        Returns
        -------
        :
            A `Fill` instance as determined by the type of the argument.
        """
        if fill is False:
            return DontFill()
        elif isinstance(fill, Fill):
            return fill  # for convenience, no-op if arg is a Fill
        elif isinstance(fill, int):
            return ConstantFill[np.int64](fill)  # type: ignore
        elif callable(fill):
            return FunctionFill[np.int64](fill)  # type: ignore
        raise ValueError("Not a valid fill.")

    @staticmethod
    def of_float64(fill: "FillLikeFloat") -> "Fill[np.float64]":
        """
        Construct for a `Fill` for `float64` data. The type of `Fill`
        returned depends on the type of argument provided.

        Parameters
        ----------
        fill :
            A value which implies the type of fix to apply.

            - `Fill[np.float64]`: is returned unchanged (no-op)
            - `float` or `int`: returns a `ConstantFill`, to replace missing values
              with a constant
            - `Callable[[], float]`: return a `FunctionFill`, to replace missing values
              with values obtained from the given callable
            - `False`: return a `DontFill`, indicating not to replace missing values

        Returns
        -------
        :
            A `Fill` instance as determined by the type of the argument.
        """
        if fill is False:
            return DontFill()
        elif isinstance(fill, Fill):
            return fill  # for convenience, no-op if arg is a Fill
        elif isinstance(fill, float | int):
            return ConstantFill[np.float64](float(fill))  # type: ignore
        elif callable(fill):
            return FunctionFill[np.float64](fill)  # type: ignore
        raise ValueError("Not a valid fill.")


@dataclass(frozen=True)
class ConstantFill(Fill[DataT]):
    """
    A `Fill` which replaces missing values with a constant value.

    `ConstantFill` is generic in the dtype of the data it fixes (`DataT`).

    Parameters
    ----------
    with_value :
        The value to use to replace missing values.
    """

    with_value: DataT
    """The value to use to replace missing values."""

    @override
    def __call__(
        self, rng, data_np, missing_mask
    ) -> tuple[NDArray[DataT], NDArray[np.bool_] | None]:
        if missing_mask.any():
            result_np = data_np.copy()
            result_np[missing_mask] = self.with_value
        else:
            result_np = data_np
        return result_np, None


@dataclass(frozen=True)
class FunctionFill(Fill[DataT]):
    """
    A `Fill` which replaces missing values with values generated by the given function.

    `FunctionFill` is generic in the dtype of the data it fixes (`DataT`).

    Parameters
    ----------
    with_function :
        The function that generates replacement values.
    """

    with_function: Callable[[], DataT]
    """The function that generates replacement values."""

    @override
    def __call__(
        self, rng, data_np, missing_mask
    ) -> tuple[NDArray[DataT], NDArray[np.bool_] | None]:
        return FunctionFill.apply(data_np, missing_mask, self.with_function)

    @staticmethod
    def apply(
        data_np: NDArray[DataT],
        missing_mask: NDArray[np.bool_],
        with_function: Callable[[], DataT],
    ) -> tuple[NDArray[DataT], NDArray[np.bool_] | None]:
        """
        Apply a `FunctionFill` to numpy data.

        This method can be useful in creating other `Fill` instances, when their
        replacement value logic can be expressed as a no-parameter function.

        Parameters
        ----------
        data_np :
            The data to fix.
        missing_mask :
            A mask indicating values which should be considered missing.
        with_function :
            The function used to generate replacement values.

        Returns
        -------
        :
            The fill apply result (updated data and missing mask).
        """
        num_replace = missing_mask.sum()
        replacements = np.array([with_function() for _ in range(num_replace)])
        result_np = data_np.copy()
        result_np[missing_mask] = replacements
        return result_np, None


@dataclass(frozen=True)
class RandomFill(Fill[DataT]):
    """
    A `Fill` which replaces missing values with randomly-generated values.

    `RandomFill` is generic in the dtype of the data it fixes (`DataT`).

    Parameters
    ----------
    with_random :
        A function for generating replacement values using the given numpy random number
        generator.
    """

    with_random: Callable[[np.random.Generator], DataT]
    """
    A function for generating replacement values using the given numpy random number
    generator.
    """

    @override
    def __call__(self, rng, data_np, missing_mask):
        random_fn = partial(self.with_random, rng.rng)
        return FunctionFill.apply(data_np, missing_mask, random_fn)

    @staticmethod
    def from_range(low: int, high: int) -> "RandomFill[np.int64]":
        """
        Construct for a `RandomFill` which replaces values
        with values sampled uniformly from a discrete range of integers.

        Parameters
        ----------
        low :
            The lowest replacement value.
        high :
            The highest replacement value.

        Returns
        -------
        :
            The fill instance.
        """
        return RandomFill(
            lambda rng: rng.integers(low, high, endpoint=True),  # type: ignore
        )

    @staticmethod
    def from_range_float(low: float, high: float) -> "RandomFill[np.float64]":
        """
        Construct for a `RandomFill` which replaces values
        with values sampled uniformly from a continuous range.

        Parameters
        ----------
        low :
            The low end of the range of replacement values.
        high :
            The high end of the range of replacement values.
            (Not included in the possible values.)

        Returns
        -------
        :
            The fill instance.
        """
        return RandomFill(
            lambda rng: rng.uniform(low, high),  # type: ignore
        )


@dataclass(frozen=True)
class DontFill(Fill[DataT]):
    """
    A special `Fill` which does not replace missing values and simply returns the data
    as-is (no-op).

    `DontFill` is generic in the dtype of the data it fixes (`DataT`).
    """

    @override
    def __call__(
        self, rng, data_np, missing_mask
    ) -> tuple[NDArray[DataT], NDArray[np.bool_] | None]:
        return data_np, missing_mask


FillLikeInt = Fill[np.int64] | int | Callable[[], int] | Literal[False]
"""A value which can be coerced into a `Fill` for integers."""

FillLikeFloat = Fill[np.float64] | float | Callable[[], float] | Literal[False]
"""A value which can be coerced into a `Fill` for floats."""


#################
# DataPipelines #
#################


def _add_issue(
    issues: Mapping[str, NDArray[np.bool_]],
    issue_name: str,
    issue_mask: NDArray[np.bool_] | None,
) -> Mapping[str, NDArray[np.bool_]]:
    """
    Add an issue to a list of issues.

    But only if the mask is not `None` and not "no mask" or all-`False`.
    """
    if issue_mask is not None and issue_mask.any():
        return {**issues, issue_name: issue_mask}
    return issues


def _all_issues_mask(issues: Mapping[str, NDArray[np.bool_]]):
    """Compute the logical union of the masks of a set of issues."""
    return reduce(
        np.logical_or,
        [m for _, m in issues.items()],
        np.ma.nomask,
    )


@dataclass(frozen=True)
class PipelineResult(Generic[DataT]):
    """
    An object containing the result of processing data through a `DataPipeline`.

    `PipelineResult` is generic in the dtype of the resulting numpy array (`DataT`).

    Parameters
    ----------
    value :
        The resulting numpy array. In this form, the array will never masked, even if
        there are issues. If you want a masked array, see the `value_as_masked`
        property.
    issues :
        The set of outstanding issues in the underlying data, with issue-specific masks.
    """

    value: NDArray[DataT]
    """
    The resulting numpy array. In this form, the array will never masked, even if
    there are issues. If you want a masked array, see the `value_as_masked`
    property.
    """
    issues: Mapping[str, NDArray[np.bool_]]
    """
    The set of outstanding issues in the underlying data, with issue-specific masks.
    """

    def __post_init__(self):
        if np.ma.is_masked(self.value):
            err = "PipelineResult `value` should not be masked directly."
            raise ValueError(err)

    @property
    def value_as_masked(self) -> NDArray[DataT]:
        """
        The resulting numpy array which will be masked if-and-only-if there are issues.
        The mask is computed as the logical union of the individual issue masks.
        """
        if len(self.issues) == 0:
            return self.value
        mask = _all_issues_mask(self.issues)
        if not np.any(mask):
            return self.value
        return np.ma.masked_array(data=self.value, mask=mask)

    def with_issue(
        self,
        issue_name: str,
        issue_mask: NDArray[np.bool_] | None,
    ) -> Self:
        """
        Update the result by adding a data issue.

        Parameters
        ----------
        issue_name :
            The name of the issue.
        issue_mask :
            The mask indicating which values are affected by the issue. For convenience,
            the mask may be None or "no mask" to indicate the data does not have the
            named issue in fact, in which case the issue will not be added.

        Returns
        -------
        :
            The updated copy of the result.
        """
        return dataclasses.replace(
            self,
            issues=_add_issue(self.issues, issue_name, issue_mask),
        )

    def to_date_value(
        self,
        dates: NDArray[np.datetime64],
    ) -> "PipelineResult[DateValueType]":
        """
        Convert the result to a date-value-tuple array.

        Parameters
        ----------
        dates :
            The one-dimensional array of dates.

        Returns
        -------
        :
            The updated copy of the result.

        See Also
        --------
        [epymorph.util.to_date_value_array][] for more detail
        on how dates and values are combined,
        and [epymorph.util.extract_date_value][] for a convenient
        way to separate the dates and values when needed.
        """
        value = to_date_value_array(dates, self.value)
        return PipelineResult(value, self.issues)

    @staticmethod
    def sum(
        left: "PipelineResult[DataT]",
        right: "PipelineResult[DataT]",
        *,
        left_prefix: str,
        right_prefix: str,
    ) -> "PipelineResult[DataT]":
        """
        Combine two `PipelineResults` by summing unmasked data values.
        The result will include both lists of data issues by prefixing the issue names.

        Parameters
        ----------
        left :
            The first addend.
        right :
            The second addend.
        left_prefix :
            A prefix to assign to any left-side issues.
        right_prefix :
            A prefix to assign to any right-side issues.

        Returns
        -------
        :
            The combined result as a new instance.
        """
        if left.value.shape != right.value.shape:
            err = "When summing PipelineResults, their data shapes must be the same."
            raise ValueError(err)
        if np.dtype(left.value.dtype) != np.dtype(right.value.dtype):
            err = "When summing PipelineResults, their dtypes must be the same."
            raise ValueError(err)
        if not (
            np.issubdtype(left.value.dtype, np.integer)
            or np.issubdtype(left.value.dtype, np.floating)
        ):
            err = (
                "When summing PipelineResults, their dtypes must be integer or "
                "floating point numbers."
            )
            raise ValueError(err)

        new_issues = {
            **{f"{left_prefix}{iss}": m for iss, m in left.issues.items()},
            **{f"{right_prefix}{iss}": m for iss, m in right.issues.items()},
        }
        unmasked = ~_all_issues_mask(new_issues)
        new_value = np.zeros_like(left.value, dtype=left.value.dtype)
        new_value[unmasked] = left.value[unmasked] + right.value[unmasked]
        return PipelineResult(value=new_value, issues=new_issues)


class PivotAxis(NamedTuple):
    """Describes an axis on which to pivot a `DataFrame` to become a numpy array."""

    column: str
    """The name of the column in a `DataFrame`."""
    values: list | NDArray
    """
    The set of values we expect to find in the column. This will be used to expand and
    reorder the resulting pivot table. If values are in this set and not in the data,
    the table will contain missing values -- which is better than not knowing which
    values are missing!
    """


class _PipelineState(NamedTuple):
    """The state of a data pipeline."""

    data_df: pd.DataFrame
    """The data being worked on."""
    issues: Mapping[str, NDArray[np.bool_]] = {}
    """A map of outstanding issues that have been discovered in the data."""

    def next(self, updated_df: pd.DataFrame) -> "_PipelineState":
        """
        Advances the state by updating the working data. Pipeline steps should avoid
        mutating the previous state's data and instead work on a new copy.

        Parameters
        ----------
        updated_df :
            The updated working data.

        Returns
        -------
        :
            The updated copy of the state.
        """
        return _PipelineState(updated_df, self.issues)

    def next_with_issue(
        self,
        updated_df: pd.DataFrame,
        issue_name: str,
        issue_mask: NDArray[np.bool_] | None,
    ) -> "_PipelineState":
        """
        Advances the state by updating the working data and adding a data issue.
        Pipeline steps should avoid mutating the previous state's data and instead
        work on a new copy.

        Parameters
        ----------
        updated_df :
            The updated working data.
        issue_name :
            The name of the issue.
        issue_mask :
            The mask indicating which values are affected by the issue. For convenience,
            the mask may be None or "no mask" to indicate the data does not have the
            named issue in fact, in which case the issue will not be added.

        Returns
        -------
        :
            The updated copy of the state.
        """
        return _PipelineState(
            data_df=updated_df,
            issues=_add_issue(self.issues, issue_name, issue_mask),
        )


_PipelineStep = Callable[[_PipelineState], _PipelineState]
"""A step in a data pipeline is just a function from one state to another."""


@dataclass(frozen=True)
class DataPipeline(Generic[DataT]):
    """
    `DataPipeline` is a factory class for assembling data processing pipelines.

    Using builder-style syntax you define the processing steps that the data should
    flow through. Finalizing the pipeline yields a function that takes a `DataFrame`,
    executes the pipeline steps in sequence, and returns a `PipelineResult` containing
    the processed data and any unresolved data issues discovered along the way.
    The `DataPipeline` instance itself can be discarded after the processing function
    is finalized.

    `DataPipeline` was designed to produce arrays with one or two dimensions. When
    there is more than one value in the "columns" dimension, it's obvious we should
    have a 2D array. But when there's only one column, a 1D or 2D array layout are
    both valid. Because of this ambiguity, it's up to you to provide the number
    of dimensions you expect. If you specify `ndims` as 1 and the data has more than
    one column, this will result in an error.

    `DataPipeline` generic in the dtype of the data it processes (`DataT`).

    Parameters
    ----------
    axes :
        The definition of the axes which will be used to tabulate the data.
        The first axis represents rows in the result, and the second columns.
    ndims :
        The number of dimensions expected in the result: 1 or 2.
    dtype :
        The dtype of the data values in the result.
    rng :
        A source of randomness.
    pipeline_steps :
        The accumulated pipeline steps.

    Examples
    --------
    --8<-- "docs/_examples/adrio_processing_DataPipeline.md"
    """

    axes: tuple[PivotAxis, PivotAxis]
    """
    The definition of the axes which will be used to tabulate the data.
    The first axis represents rows in the result, and the second columns.

    """
    ndims: Literal[1, 2]
    """The number of dimensions expected in the result: 1 or 2."""
    dtype: type[DataT]
    """The dtype of the data values in the result."""
    rng: HasRandomness
    """A source of randomness."""

    pipeline_steps: Sequence[_PipelineStep] = field(default_factory=list)
    """The accumulated pipeline steps."""

    def __post_init__(self):
        if self.ndims == 1 and len(self.axes[1].values) > 1:
            err = (
                "If `ndims` is 1, the second axis definition in `axes` must contain "
                "exactly one value."
            )
            raise ValueError(err)
        if any(len(x.values) == 0 for x in self.axes):
            err = "All axis definitions in `axes` must contain at least one value."
            raise ValueError(err)

    def _and_then(self, f: _PipelineStep) -> Self:
        """Return a copy of this pipeline with a pipeline step appended."""
        return dataclasses.replace(self, pipeline_steps=[*self.pipeline_steps, f])

    def _process(self, data_df: pd.DataFrame) -> _PipelineState:
        """Apply all of the pipeline steps to an input."""
        state = _PipelineState(data_df)
        for f in self.pipeline_steps:
            state = f(state)
        return state

    def map_series(
        self,
        column: str,
        map_fn: Callable[[pd.Series], pd.Series] | None = None,
    ) -> Self:
        """
        Add a pipeline step that transforms a column of the `DataFrame`
        by applying a mapping function to the series.

        Parameters
        ----------
        column :
            The name of the column to transform.
        map_fn :
            The series mapping function. As a convenience you may pass `None`,
            in which case this is a no-op.

        Returns
        -------
        :
            A copy of this pipeline with the step added.
        """
        if map_fn is None:
            return self

        def map_series(state: _PipelineState) -> _PipelineState:
            series = map_fn(state.data_df[column])
            data_df = state.data_df.assign(**{column: series})
            return state.next(data_df)

        return self._and_then(map_series)

    def map_column(
        self,
        column: str,
        map_fn: Callable | None = None,
    ) -> Self:
        """
        Add a pipeline step that transforms a column of the `DataFrame`
        by applying a mapping function to all values in the series.

        Parameters
        ----------
        column :
            The name of the column to transform.
        map_fn :
            The value mapping function. As a convenience you may pass `None`,
            in which case this is a no-op.

        Returns
        -------
        :
            A copy of this pipeline with the step added.
        """
        if map_fn is None:
            return self

        return self.map_series(
            column=column,
            map_fn=lambda xs: xs.apply(map_fn),
        )

    def strip_sentinel(
        self,
        sentinel_name: str,
        sentinel_value: DataT,
        fix: "Fix[DataT]",
    ) -> Self:
        """
        Add a pipeline step for dealing with sentinel values in the `DataFrame`.
        First we apply the given `Fix`, then check for any remaining sentinel values.
        If sentinel values still remain in the data, these are recorded as a data
        issue with an associated mask.

        Parameters
        ----------
        sentinel_name :
            The name used for the data issue if any sentinel values remain.
        sentinel_value :
            The value considered a sentinel.
        fix :
            The fix to apply to attempt to replace sentinel values.

        Returns
        -------
        :
            A copy of this pipeline with the step added.
        """

        def strip_sentinel(state: _PipelineState) -> _PipelineState:
            # Apply fix.
            work_df = fix(self.rng, sentinel_value, ("value",), state.data_df)

            # Compute the mask for any remaining sentinels.
            is_sentinel = work_df["value"] == sentinel_value
            mask = (
                work_df.loc[is_sentinel]
                .pivot_table(
                    index=self.axes[0].column,
                    columns=self.axes[1].column,
                    values="value",
                    aggfunc=lambda _: True,
                    fill_value=False,
                )
                .reindex(
                    index=self.axes[0].values,
                    columns=self.axes[1].values,
                    fill_value=False,
                )
                .to_numpy(dtype=np.bool_)
            )

            # pivot_table produces 2D results; adjust if we expect a 1D result.
            if self.ndims == 1:
                if mask.shape[1] > 1:
                    err = (
                        "You specified `ndims` as 1, but the data contains more than "
                        "one column so it is impossible to convert it to this form."
                    )
                    raise Exception(err)
                mask = mask[:, 0]

            # Strip out sentinel values.
            work_df = work_df.loc[~is_sentinel]
            return state.next_with_issue(work_df, sentinel_name, mask)

        return self._and_then(strip_sentinel)

    def strip_na_as_sentinel(
        self,
        column: str,
        sentinel_name: str,
        sentinel_value: DataT,
        fix: "Fix[DataT]",
    ) -> Self:
        """
        Add a pipeline step for dealing with NaN/NA/null values in the `DataFrame`.
        First replace NA values with a user-defined sentinel value, then apply the
        given `Fix`. Finally check for any remaining such values. If sentinel values
        still remain in the data, these are recorded as a data issue with an associated
        mask.

        Parameters
        ----------
        column :
            The name of the column to transform.
        sentinel_name :
            The name used for the data issue if any NA/sentinel values remain.
        sentinel_value :
            The value to use to replace NA values. We want to replace NAs so that
            we can universally convert the data column to the desired type --
            np.int64 doesn't support NA values like np.float64 does, so this allows
            the input DataFrame to start with something like Pandas' "Int64" data type
            while the pipeline produces np.int64 results.
            The sentinel value chosen for this must not already exist in the data.
        fix :
            The fix to apply to attempt to replace NA/sentinel values.

        Returns
        -------
        :
            A copy of this pipeline with the step added.

        Raises
        ------
        Exception
            If the data naturally contains the chosen sentinel value.
        """

        def replace_na(state: _PipelineState) -> _PipelineState:
            series = state.data_df[column]
            if (series == sentinel_value).any():
                # the below S608 Ruff warning is a false positive;
                # apparently it thinks this error message looks like a SQL statement
                err = (
                    "The data contains the proposed artificial sentinel value "  # noqa: S608
                    f"({sentinel_value}), so if we replace NA values with this value "
                    "we'll get missing values mixed up with others. Please choose a "
                    "sentinel value that is guaranteed not to be in the data naturally."
                )
                raise Exception(err)

            series = series.fillna(sentinel_value).astype(self.dtype)  # type: ignore
            data_df = state.data_df.assign(**{column: series})
            return state.next(data_df)

        return self._and_then(replace_na).strip_sentinel(
            sentinel_name, sentinel_value, fix
        )

    def finalize(
        self,
        fill_missing: "Fill[DataT]",
    ) -> Callable[[pd.DataFrame], PipelineResult[DataT]]:
        """
        Completes construction of the pipeline.

        Parameters
        ----------
        fill_missing :
            A method for filling in missing data.

        Returns
        -------
        :
            The prepared pipeline: a function that processes a `DataFrame` and produces
            a result.
        """

        def pipeline(data_df: pd.DataFrame) -> PipelineResult[DataT]:
            # Run the accumulated processing pipeline.
            state = self._process(data_df)

            # Tabulate data; reindexing step can expose missing values.
            result_np = cast(
                NDArray[DataT],
                (
                    state.data_df.pivot_table(
                        index=self.axes[0].column,
                        columns=self.axes[1].column,
                        values="value",
                        aggfunc="sum",
                        fill_value=0,
                    )
                    .reindex(
                        index=self.axes[0].values,
                        columns=self.axes[1].values,
                        fill_value=0,
                    )
                    .to_numpy(dtype=self.dtype)
                ),
            )
            missing_mask = cast(
                NDArray[np.bool_],
                (
                    state.data_df.assign(value=False)
                    .pivot_table(
                        index=self.axes[0].column,
                        columns=self.axes[1].column,
                        values="value",
                        aggfunc=lambda _: False,
                        fill_value=True,
                    )
                    .reindex(
                        index=self.axes[0].values,
                        columns=self.axes[1].values,
                        fill_value=True,
                    )
                ).to_numpy(dtype=np.bool_),
            )

            # pivot_table produces 2D results; adjust if we expect a 1D result.
            if self.ndims == 1:
                if missing_mask.shape[1] > 1 or result_np.shape[1] > 1:
                    err = (
                        "You specified `ndims` as 1, but the data contains more than "
                        "one column so it is impossible to convert it to this form."
                    )
                    raise Exception(err)
                result_np = result_np[:, 0]
                missing_mask = missing_mask[:, 0]

            # If a value is present in an issue mask it's not missing,
            # but for calculation purposes we may have removed it
            # (e.g., stripping sentinels.). Correct that now.
            all_issues_mask = reduce(
                np.logical_or,
                [m for _, m in state.issues.items()],
                np.ma.nomask,
            )
            missing_mask = missing_mask & ~all_issues_mask

            # Discover and fill missing data.
            result_np, missing_mask = fill_missing(self.rng, result_np, missing_mask)
            state = state.next_with_issue(state.data_df, "missing", missing_mask)
            return PipelineResult(result_np, state.issues)

        return pipeline
