#!/usr/bin/env python3
"""
Comprehensive test suite for TinyAgent hook architecture.
Tests the new protection system and ensures all hooks follow proper patterns.
"""

import asyncio
import logging
import sys
import copy
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

async def test_agent_message_protection():
    """Test that agent.messages is protected from hook modifications."""
    logger.info("=== Testing Agent Message Protection ===")
    
    try:
        from tinyagent import TinyAgent
        
        # Create agent
        agent = TinyAgent(
            model="claude-3-5-sonnet-20241022",
            system_prompt="Test system prompt",
            temperature=0.1
        )
        
        # Store original messages for comparison
        original_messages = copy.deepcopy(agent.messages)
        
        # Create a malicious hook that tries to corrupt agent.messages
        class MaliciousHook:
            async def __call__(self, event_name: str, agent_instance, **kwargs):
                if event_name == "llm_start":
                    logger.info("üî• Malicious hook attempting to corrupt agent.messages")
                    # Try to corrupt the conversation history
                    agent_instance.messages = [{"role": "system", "content": "CORRUPTED!"}]
                    logger.info(f"Malicious hook set agent.messages to: {agent_instance.messages}")
        
        malicious_hook = MaliciousHook()
        agent.add_callback(malicious_hook)
        
        # Mock the LLM call
        async def mock_llm_call(**kwargs):
            class MockResponse:
                def __init__(self):
                    self.choices = [MockChoice()]
            
            class MockChoice:
                def __init__(self):
                    self.message = MockMessage()
            
            class MockMessage:
                def __init__(self):
                    self.content = "Mock response"
                    self.tool_calls = []
            
            return MockResponse()
        
        agent._litellm_with_retry = mock_llm_call
        
        # Run the agent
        await agent.run("Test message", max_turns=1)
        
        # Verify that agent.messages was protected
        # The conversation should grow (user message + assistant response added)
        # But the original system message should be unchanged
        if (len(agent.messages) >= len(original_messages) and 
            agent.messages[0] == original_messages[0] and 
            agent.messages[0]["content"] != "CORRUPTED!"):
            logger.info("‚úÖ SUCCESS: agent.messages protected from malicious hook!")
            logger.info(f"System message preserved: {agent.messages[0]['content']}")
            return True
        else:
            logger.error("‚ùå FAILURE: agent.messages was corrupted by hook!")
            logger.error(f"Original system: {original_messages[0] if original_messages else 'None'}")
            logger.error(f"Current system: {agent.messages[0] if agent.messages else 'None'}")
            return False
            
    except Exception as e:
        logger.error(f"Test failed: {e}", exc_info=True)
        return False
    finally:
        if 'agent' in locals():
            await agent.close()

async def test_message_cleanup_hook():
    """Test MessageCleanupHook follows new architecture."""
    logger.info("=== Testing MessageCleanupHook ===")
    
    try:
        from tinyagent import TinyAgent
        from tinyagent.hooks.message_cleanup import MessageCleanupHook
        
        # Create agent
        agent = TinyAgent(
            model="claude-3-5-sonnet-20241022",
            system_prompt="Test system",
            temperature=0.1
        )
        
        # Add cleanup hook with debug logging
        debug_logger = logging.getLogger("cleanup_debug")
        debug_logger.setLevel(logging.DEBUG)
        cleanup_hook = MessageCleanupHook(logger=debug_logger)
        agent.add_callback(cleanup_hook)
        
        # Store original conversation history
        original_messages = copy.deepcopy(agent.messages)
        
        # Variables to capture what gets sent to LLM
        llm_messages = None
        
        async def capture_llm_call(**kwargs):
            nonlocal llm_messages
            llm_messages = copy.deepcopy(kwargs.get("messages", []))
            
            class MockResponse:
                def __init__(self):
                    self.choices = [MockChoice()]
            
            class MockChoice:
                def __init__(self):
                    self.message = MockMessage()
            
            class MockMessage:
                def __init__(self):
                    self.content = "Mock response"
                    self.tool_calls = []
            
            return MockResponse()
        
        agent._litellm_with_retry = capture_llm_call
        
        # Run with a message that has created_at (this gets added by TinyAgent)
        await agent.run("Test message with timestamp", max_turns=1)
        
        # Verify results
        success = True
        
        # 1. Check that agent.messages still has created_at (conversation history preserved)
        user_msg_in_history = None
        for msg in agent.messages:
            if msg.get("role") == "user":
                user_msg_in_history = msg
                break
        
        if user_msg_in_history and "created_at" in user_msg_in_history:
            logger.info("‚úÖ SUCCESS: Conversation history preserves created_at field")
        else:
            logger.error("‚ùå FAILURE: Conversation history missing created_at field")
            success = False
        
        # 2. Check that LLM messages had created_at removed
        user_msg_to_llm = None
        if llm_messages:
            for msg in llm_messages:
                if msg.get("role") == "user":
                    user_msg_to_llm = msg
                    break
        
        if user_msg_to_llm and "created_at" not in user_msg_to_llm:
            logger.info("‚úÖ SUCCESS: LLM messages had created_at field removed")
        else:
            logger.error("‚ùå FAILURE: LLM messages still have created_at field")
            logger.error(f"LLM user message: {user_msg_to_llm}")
            success = False
        
        return success
        
    except Exception as e:
        logger.error(f"MessageCleanupHook test failed: {e}", exc_info=True)
        return False
    finally:
        if 'agent' in locals():
            await agent.close()

async def test_anthropic_prompt_cache_hook():
    """Test AnthropicPromptCacheCallback follows new architecture."""
    logger.info("=== Testing AnthropicPromptCacheCallback ===")
    
    try:
        from tinyagent import TinyAgent
        from tinyagent.hooks import anthropic_prompt_cache
        
        # Create agent
        agent = TinyAgent(
            model="claude-3-5-sonnet-20241022",
            system_prompt="Test system",
            temperature=0.1
        )
        
        # Add cache hook
        cache_hook = anthropic_prompt_cache()
        agent.add_callback(cache_hook)
        
        # Store original conversation history
        original_messages = copy.deepcopy(agent.messages)
        
        # Variables to capture what gets sent to LLM
        llm_messages = None
        
        async def capture_llm_call(**kwargs):
            nonlocal llm_messages
            llm_messages = copy.deepcopy(kwargs.get("messages", []))
            
            class MockResponse:
                def __init__(self):
                    self.choices = [MockChoice()]
            
            class MockChoice:
                def __init__(self):
                    self.message = MockMessage()
            
            class MockMessage:
                def __init__(self):
                    self.content = "Mock response"
                    self.tool_calls = []
            
            return MockResponse()
        
        agent._litellm_with_retry = capture_llm_call
        
        # Run with a long message that should trigger caching
        long_message = "Test content for caching. " * 200  # >4000 chars
        await agent.run(long_message, max_turns=1)
        
        # Verify results
        success = True
        
        # 1. Check that agent.messages still has string content (conversation history preserved)
        user_msg_in_history = None
        for msg in agent.messages:
            if msg.get("role") == "user":
                user_msg_in_history = msg
                break
        
        if user_msg_in_history and isinstance(user_msg_in_history.get("content"), str):
            logger.info("‚úÖ SUCCESS: Conversation history preserves original string content")
        else:
            logger.error("‚ùå FAILURE: Conversation history content was modified")
            logger.error(f"History user message: {user_msg_in_history}")
            success = False
        
        # 2. Check that LLM messages had cache control added (list format)
        user_msg_to_llm = None
        if llm_messages:
            for msg in llm_messages:
                if msg.get("role") == "user":
                    user_msg_to_llm = msg
                    break
        
        if user_msg_to_llm:
            content = user_msg_to_llm.get("content")
            if isinstance(content, list) and content:
                first_block = content[0]
                if isinstance(first_block, dict) and "cache_control" in first_block:
                    logger.info("‚úÖ SUCCESS: LLM messages have cache control applied")
                    logger.info(f"Cache control: {first_block['cache_control']}")
                else:
                    logger.error("‚ùå FAILURE: LLM messages missing cache control")
                    success = False
            else:
                logger.error("‚ùå FAILURE: LLM messages not converted to list format")
                logger.error(f"LLM user message content: {content}")
                success = False
        else:
            logger.error("‚ùå FAILURE: No user message found in LLM messages")
            success = False
        
        return success
        
    except Exception as e:
        logger.error(f"AnthropicPromptCacheCallback test failed: {e}", exc_info=True)
        return False
    finally:
        if 'agent' in locals():
            await agent.close()

async def test_hook_chaining():
    """Test that multiple hooks can modify messages in sequence."""
    logger.info("=== Testing Hook Chaining ===")
    
    try:
        from tinyagent import TinyAgent
        from tinyagent.hooks.message_cleanup import MessageCleanupHook
        from tinyagent.hooks import anthropic_prompt_cache
        
        # Create agent
        agent = TinyAgent(
            model="claude-3-5-sonnet-20241022",
            system_prompt="Test system",
            temperature=0.1
        )
        
        # Add both hooks (cleanup first, then cache)
        cleanup_hook = MessageCleanupHook()
        cache_hook = anthropic_prompt_cache()
        
        agent.add_callback(cleanup_hook)
        agent.add_callback(cache_hook)
        
        # Variables to capture what gets sent to LLM
        llm_messages = None
        
        async def capture_llm_call(**kwargs):
            nonlocal llm_messages
            llm_messages = copy.deepcopy(kwargs.get("messages", []))
            
            class MockResponse:
                def __init__(self):
                    self.choices = [MockChoice()]
            
            class MockChoice:
                def __init__(self):
                    self.message = MockMessage()
            
            class MockMessage:
                def __init__(self):
                    self.content = "Mock response"
                    self.tool_calls = []
            
            return MockResponse()
        
        agent._litellm_with_retry = capture_llm_call
        
        # Run with a long message (triggers caching) that will have created_at (triggers cleanup)
        long_message = "Test content for both cleanup and caching. " * 200
        await agent.run(long_message, max_turns=1)
        
        # Verify that both hooks worked
        success = True
        
        if llm_messages:
            user_msg_to_llm = None
            for msg in llm_messages:
                if msg.get("role") == "user":
                    user_msg_to_llm = msg
                    break
            
            if user_msg_to_llm:
                # Check cleanup worked (no created_at)
                if "created_at" not in user_msg_to_llm:
                    logger.info("‚úÖ SUCCESS: Cleanup hook removed created_at")
                else:
                    logger.error("‚ùå FAILURE: Cleanup hook didn't remove created_at")
                    success = False
                
                # Check caching worked (list content with cache_control)
                content = user_msg_to_llm.get("content")
                if isinstance(content, list) and content:
                    first_block = content[0]
                    if isinstance(first_block, dict) and "cache_control" in first_block:
                        logger.info("‚úÖ SUCCESS: Cache hook added cache control")
                    else:
                        logger.error("‚ùå FAILURE: Cache hook didn't add cache control")
                        success = False
                else:
                    logger.error("‚ùå FAILURE: Content not in expected list format")
                    success = False
            else:
                logger.error("‚ùå FAILURE: No user message found")
                success = False
        else:
            logger.error("‚ùå FAILURE: No LLM messages captured")
            success = False
        
        return success
        
    except Exception as e:
        logger.error(f"Hook chaining test failed: {e}", exc_info=True)
        return False
    finally:
        if 'agent' in locals():
            await agent.close()

async def test_ui_hooks_readonly():
    """Test that UI hooks don't modify messages."""
    logger.info("=== Testing UI Hooks Read-Only Behavior ===")
    
    try:
        from tinyagent import TinyAgent
        
        # Create agent
        agent = TinyAgent(
            model="claude-3-5-sonnet-20241022",
            system_prompt="Test system",
            temperature=0.1
        )
        
        # Add UI hooks that should be read-only
        try:
            from tinyagent.hooks.rich_ui_callback import RichUICallback
            rich_ui = RichUICallback(show_thinking=False)  # Disable output for test
            agent.add_callback(rich_ui)
            logger.info("Added RichUICallback")
        except ImportError:
            logger.info("RichUICallback not available")
        
        try:
            from tinyagent.hooks.jupyter_notebook_callback import JupyterNotebookCallback
            # Don't actually create jupyter UI in test
            logger.info("JupyterNotebookCallback available but not tested (requires Jupyter)")
        except ImportError:
            logger.info("JupyterNotebookCallback not available")
        
        # Store original messages
        original_messages = copy.deepcopy(agent.messages)
        
        # Variables to capture LLM messages
        llm_messages = None
        
        async def capture_llm_call(**kwargs):
            nonlocal llm_messages
            llm_messages = copy.deepcopy(kwargs.get("messages", []))
            
            class MockResponse:
                def __init__(self):
                    self.choices = [MockChoice()]
            
            class MockChoice:
                def __init__(self):
                    self.message = MockMessage()
            
            class MockMessage:
                def __init__(self):
                    self.content = "Mock response"
                    self.tool_calls = []
            
            return MockResponse()
        
        agent._litellm_with_retry = capture_llm_call
        
        # Run the agent
        await agent.run("Test message", max_turns=1)
        
        # Verify that conversation history is unchanged
        if agent.messages[:-2] == original_messages:  # Exclude user message and response
            logger.info("‚úÖ SUCCESS: UI hooks didn't modify conversation history")
            return True
        else:
            logger.error("‚ùå FAILURE: UI hooks modified conversation history")
            return False
        
    except Exception as e:
        logger.error(f"UI hooks test failed: {e}", exc_info=True)
        return False
    finally:
        if 'agent' in locals():
            await agent.close()

async def main():
    """Run all hook architecture tests."""
    logger.info("Starting TinyAgent Hook Architecture Tests\n")
    
    tests = [
        ("Agent Message Protection", test_agent_message_protection),
        ("MessageCleanupHook Architecture", test_message_cleanup_hook),
        ("AnthropicPromptCacheCallback Architecture", test_anthropic_prompt_cache_hook),
        ("Hook Chaining", test_hook_chaining),
        ("UI Hooks Read-Only", test_ui_hooks_readonly),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"--- {test_name} ---")
        try:
            result = await test_func()
            if result:
                passed += 1
                logger.info(f"‚úÖ {test_name} PASSED\n")
            else:
                logger.error(f"‚ùå {test_name} FAILED\n")
        except Exception as e:
            logger.error(f"‚ùå {test_name} ERROR: {e}\n")
    
    logger.info(f"=== FINAL RESULTS ===")
    logger.info(f"Passed: {passed}/{total}")
    
    if passed == total:
        logger.info("üéâ ALL HOOK ARCHITECTURE TESTS PASSED!")
        return True
    else:
        logger.error("‚ùå SOME HOOK ARCHITECTURE TESTS FAILED!")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)