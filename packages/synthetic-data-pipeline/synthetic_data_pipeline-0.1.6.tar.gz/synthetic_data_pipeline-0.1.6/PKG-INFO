Metadata-Version: 2.4
Name: synthetic-data-pipeline
Version: 0.1.6
Summary: AI-powered synthetic data generation pipeline with web search and topic extraction
Author-email: Omar Youssef <omarjooo595@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/Omar-YYoussef/Data_Gen_Agent
Project-URL: Repository, https://github.com/Omar-YYoussef/Data_Gen_Agent
Keywords: synthetic data,ai,data generation,nlp
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: pydantic<3.0.0,>=2.7.4
Requires-Dist: google-generativeai>=0.3.0
Requires-Dist: langgraph>=0.2.20
Requires-Dist: langchain>=0.3.0
Requires-Dist: langchain-core>=0.3.0
Requires-Dist: langchain-community>=0.3.0
Requires-Dist: tavily-python>=0.7.0
Requires-Dist: crawl4ai>=0.7.0
Requires-Dist: requests>=2.31.0
Requires-Dist: beautifulsoup4>=4.12.3
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: httpx>=0.25.0
Requires-Dist: Scrapy>=2.11.0
Requires-Dist: scraperapi-sdk>=1.5.3
Requires-Dist: langchain-scraperapi>=0.1.0
Requires-Dist: pandas>=2.2.3
Requires-Dist: numpy<3.0.0,>=1.26.0
Requires-Dist: jsonschema>=4.23.0
Requires-Dist: asyncio-throttle>=1.0.2
Requires-Dist: nest-asyncio>=1.5.0
Requires-Dist: langdetect>=1.0.9
Requires-Dist: structlog>=23.2.0
Requires-Dist: colorlog>=6.8.0
Requires-Dist: cryptography>=41.0.8
Requires-Dist: python-jose>=3.3.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.10.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Provides-Extra: api
Requires-Dist: fastapi>=0.104.0; extra == "api"
Requires-Dist: uvicorn>=0.24.0; extra == "api"
Requires-Dist: python-multipart>=0.0.6; extra == "api"
Provides-Extra: ui
Requires-Dist: gradio>=4.0.0; extra == "ui"
Requires-Dist: streamlit>=1.28.0; extra == "ui"
Requires-Dist: plotly>=5.17.0; extra == "ui"
Provides-Extra: cli
Requires-Dist: click>=8.1.7; extra == "cli"
Requires-Dist: rich>=13.7.0; extra == "cli"
Requires-Dist: typer>=0.9.0; extra == "cli"
Provides-Extra: monitoring
Requires-Dist: prometheus-client>=0.19.0; extra == "monitoring"
Dynamic: license-file

ï»¿# Synthetic Data Pipeline

AI-powered synthetic data generation pipeline with web search and topic extraction.

## Installation

Install the package using pip:

    pip install synthetic-data-pipeline

## Requirements

- Python >= 3.8
- API Keys for: Gemini, Tavily, ScraperAPI

## Quick Start

**Step 1: Create Configuration File**

Create a `.env` file in your project directory:

    GEMINI_API_KEY=your_gemini_api_key_here
    TAVILY_API_KEY=your_tavily_api_key_here
    SCRAPERAPI_API_KEY=your_scraper_api_key_here
    OUTPUT_DIR=/path/to/your/output

Note: The `.env` file must be in the directory where you run your scripts.

**Step 2: Use in Python Code**

Basic usage example:

    from synthetic_data_pipeline import generate_synthetic_data
    
    result = generate_synthetic_data("Generate 100 QA pairs about Python")

Advanced usage with custom parameters:

    generate_synthetic_data(
        user_query="prompt",
        refined_queries_count=20,
        search_results_per_query=5,
        rows_per_subtopic=5
    )

**Step 3: Use CLI**

Command line usage:

    synthetic-data "Generate data about machine learning"

## Configuration

**Environment Variables**

| Variable | Required | Description |
|----------|----------|-------------|
| GEMINI_API_KEY | Yes | Google Gemini API key |
| TAVILY_API_KEY | Yes | Tavily search API key |
| SCRAPERAPI_API_KEY | Yes | ScraperAPI key |
| OUTPUT_DIR | Yes | Output directory path |

**Custom Output Location**

Set via environment variable in `.env`:

    OUTPUT_DIR=/custom/path

Or via code:

    generate_synthetic_data("prompt")

Or via CLI:

    synthetic-data "prompt"

## API Reference

**generate_synthetic_data(user_query, refined_queries_count, search_results_per_query, rows_per_subtopic)**

Generate synthetic data based on a natural language prompt.

Parameters:
- user_query (str): Natural language description of data to generate
- refined_queries_count (int, optional): Number of search queries to generate
- search_results_per_query (int, optional): Number of search links per query
- rows_per_subtopic (int, optional): Number of rows to generate per subtopic

Example:

    generate_synthetic_data(
        user_query="Generate customer reviews",
        refined_queries_count=20,
        search_results_per_query=10,
        rows_per_subtopic=10
    )

## Development

**Local Installation**

    git clone https://github.com/yourusername/synthetic-data-pipeline
    cd synthetic-data-pipeline
    pip install -e .

## License

MIT License - see LICENSE file for details.

## Support

- Issues: GitHub Issues
- Email: omarjooo595@gmail.com
