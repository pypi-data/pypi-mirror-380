"""A big ol' module that contains high-efficiency tools for calculating shap scores.

In user code, it's actually not that bad to use this module.
Here's what you do.

1. Create a Generator.
2. Create a Saver.
3. Pass those to an InterpRunner.
4. Call .run() on the Runner.
"""
from typing import Any
from collections.abc import Iterator, Iterable, Callable
import multiprocessing
import ctypes
import pysam
import numpy as np
import tqdm
import h5py
import pybedtools
from bpreveal import logUtils
from bpreveal import utils
from bpreveal import ushuffle
from bpreveal.internal.constants import IMPORTANCE_AR_T, NUM_BASES, ONEHOT_T, ONEHOT_AR_T, \
    IMPORTANCE_T, H5_CHUNK_SIZE, MODEL_ONEHOT_T, PRED_AR_T, PRED_T
import bpreveal.internal.files as bprfiles
from bpreveal.internal.crashQueue import CrashQueue


class Query:
    """A Query is what is passed to the batcher.

    It has three things.

    :param sequence: The ``(input-length, NUM_BASES)`` one-hot encoded sequence of the current base.
    :param passData: As with Result objects, is either a tuple of ``(chromName, position)``
        (for when you have a bed file) or a string with a fasta description line
        (for when you're starting with a fasta). If you're using the
        :py:class:`~ListGenerator`, then it can be anything.
    :param index: Indicates which output slot this data should be put in.
        Since there's no guarantee that the results will arrive in order, we have to
        track which query was which.

    Lifetime:

    1. Created by a Generator's iterator
    2. _generatorThread iterates, and puts the Query in each inQueue.
    3. inQueue is read by _batcherThread.
    4. _batcherThread calls .add on the Batcher.
    5. Batcher processes query.
    """

    sequence: ONEHOT_AR_T
    passData: Any
    index: int

    def __init__(self, oneHotSequence: ONEHOT_AR_T, passData: Any, index: int):
        """Just store the given data."""
        self.sequence = oneHotSequence
        self.passData = passData
        self.index = index

    def __str__(self):
        """Make a string with information about this Query."""
        fmtStr = "seqSize {0:s} passData {1:s} index {2:d}"
        return fmtStr.format(str(self.sequence.shape), str(self.passData), self.index)


class Result:
    """A result from doing shap.

    Lifetime:

    1. Generated by a Batcher.
    2. Batcher places in outQueue.
    3. outQueue is read by saverThread
    4. saverThread calls .add() on the Saver.

    :param inputPrediction: A scalar floating point value, the value of the metric on the
        given input window.

    :param shufflePredictions: A ``(numShuffles,)`` numpy array of the values of the metric
        on each of the shuffled sequences.

    :param sequence: is a ``(input-length, NUM_BASES)`` numpy array of the
        one-hot encoded input sequence.

    :param shap: is a ``(input-length, NUM_BASES)`` numpy array of shap scores.

    :param passData: is data that is not touched by the batcher, but added by
        the generator and necessary for creating the output file.
        If the generator is reading from a bed file, then it is a tuple of (chromName, position)
        and that data should be used to populate the coords_chrom and coords_base fields.
        If the generator was using a fasta file, it is the title line from the original fasta,
        with its leading ``>`` removed.

    :param index: Indicates which address the data should be stored at in the output hdf5.
        Since there's no order guarantee when you're receiving data, we have to keep track
        of the order in the original input. It is up to the Saver to re-organize the Results
        that it gets if the Saver wants to make an order guarantee.
    """

    def __init__(self, inputPrediction: PRED_AR_T,
                 shufflePredictions: PRED_AR_T,
                 sequence: ONEHOT_AR_T,
                 shap: IMPORTANCE_AR_T,
                 passData: Any,
                 index: int):
        """Just store all the provided data."""
        self.inputPrediction = inputPrediction
        self.shufflePredictions = shufflePredictions
        self.sequence = sequence
        self.shap = shap
        self.passData = passData
        self.index = index


class Generator:
    """The base class for generating PISA samples.

    Lifetime:

    1. Initially created in user code. The generator at this point only
       contains serializable data.
    2. Passed to a Runner.
    3. The Runner passes it to a generatorThread, which runs in a separate Process.
    4. generatorThread calls .construct(), and the Generator opens any files it needs.
    5. generatorThread iterates over the Generator until it is exhausted.
    6. generatorThread calls .done().
    """

    def __init__(self):
        raise NotImplementedError()

    def __iter__(self) -> Iterator[Query]:
        """Construct an iterator that generates all of the queries."""
        raise NotImplementedError()

    def __next__(self):
        """Get the next step for the iterator.

        If your generator's __iter__ returns self, you must implement __next__.
        """
        raise NotImplementedError()

    def construct(self) -> None:
        """Set up the generator (in the child thread).

        When the generator is about to start, this method is called once before
        requesting the iterator.
        """
        raise NotImplementedError()

    def done(self) -> None:
        """When the batcher is done, this is called to free any allocated resources."""
        raise NotImplementedError()


class Saver:
    """Class to receive Results and deal with them.

    Descendants of this class shall receive Results from the batcher and save them to an
    appropriate structure, usually on disk.
    The user creates a Saver object in the main thread, and then the saver gets
    construct()ed inside a separate thread created by the runners. Therefore, you
    should only create members that are serializable in __init__.

    Lifetime:

    1. Created in user code with only serializable data.
    2. Passed to a Runner, which passes it to a saverThread in a new process.
    3. saverThread calls .construct().
    4. saverThread reads from a queue and repeatedly calls .add().
    5. saverThread calls .done()
    6. The Runner, once saverThread is joined, calls parentFinish in the parent thread.
    """

    def __init__(self):
        raise NotImplementedError()

    def construct(self) -> None:
        """Do any setup necessary in the child thread.

        This function should actually open the output files, since it will
        be called from inside the actual saver thread.
        """
        raise NotImplementedError()

    def add(self, result: Result) -> None:
        """Add the given Result to wherever you're saving them out.

        :param result: The thing to save out.
        :raise NotImplementedError: because this is abstract.

        Note that this will not be called with None at the end of the run,
        that is special-cased in the code that runs your saver. (That code will
        call done() when all of the results have been added.)
        """
        del result
        raise NotImplementedError()

    def parentFinish(self) -> None:
        """Called in the parent thread when the saver is done.

        Usually, there's nothing to do, but the parent thread might need
        to close shared memory, so this function is guaranteed to be called
        by the Runners.
        """
        logUtils.debug("Called parentFinish on the base Saver class.")

    def done(self) -> None:
        """Called when the batcher is complete (indicated by putting a None in its output queue).

        Now is the time to close all of your files.
        This function is called in the child thread, not the parent thread.
        """
        logUtils.debug("Called done on the base Saver class.")


class InterpRunner:
    """Runs shap scores.

    I try to avoid class-based wrappers around simple things, but this is not simple.
    This class creates threads to read in data, and then creates two threads that run interpretation
    samples in batches. Finally, it takes the results from the interpretation thread and saves
    those out to an hdf5-format file.

    :param modelFname: is the name of the model on disk
    :param metrics: a list of functions that accept a model and return a scalar output. These
        are the values that will be explained.
    :param batchSize: is the *shap* batch size, which should be your usual batch size divided
        by the number of shuffles. (since the original sequence and shuffles are run together)
    :param generator: is a Generator object that will be passed to _generatorThread
    :param savers: is a list of Saver objects that will be used to save out the data.
        There should be one saver per metric.
    :param numShuffles: is the number of reference sequences that should be generated for each
        shap evaluation. (I recommend 20 or so) (Only applicable when ``backend`` is ``"shap"``).
    :param kmerSize: Should the shuffle preserve k-mer distribution? If kmerSize == 1, then no.
        If kmerSize > 1, preserve the distribution of kmers of the given size. If using the
        ``ism`` backend, then this parameter controls the width of the shuffled region
        (in which case it must be an odd number).
    :param backend: If "shap", use the DeepShap backend for interpretation scores. If "ism",
        use scanning mutagenesis instead.
    :param useHypotheticalContribs: (Only valid with ``shap`` backend.) If True, calculate
        hypothetical contribution scores as was done in the original paper. If False, then
        use the normal shap algorithm. Normally, you set this to True for scores you intend
        to feed to MoDISco and false for PISA.
    :param shuffler: (Only valid with the ``ism`` backend.) The function to use to generate shuffles
        of the sequences. The width of the shuffled sequence will be taken from ``kmerSize``, which
        must be an odd number when using the ``ism`` backend.
        This function will take two arguments. First, a one-hot encoded sequence to generate
        shuffles from, and second, an integer giving the position in the overall input where
        this shuffle is being performed. It should return a list of one-hot encoded sequences.
    """

    def __init__(self, modelFname: str, metrics: list[Callable],
                 batchSize: int, generator: Generator, savers: list[Saver],
                 numShuffles: int, kmerSize: int, numThreads: int,
                 backend: str,
                 useHypotheticalContribs: bool | None = None,
                 shuffler: Callable | None = None):
        logUtils.info("Initializing interpretation runner.")
        assert len(metrics) == len(savers), "You must supply as many savers as metrics "\
            "you are calculating."
        assert numThreads >= len(metrics), "You must allow for at least as many threads as metrics."
        assert numThreads % len(metrics) == 0, "numThreads must be a multiple of the number of "\
            "metrics."
        match numThreads:
            case 1:
                memFrac = 0.9
            case 2:
                memFrac = 0.4
            case 3:
                memFrac = 0.25
            case _:
                logUtils.error("Very high number of batchers requested. "
                               "BPReveal has not been tested with 4 batchers!")
                memFrac = 0.7 / numThreads

        self._savers = savers
        self._inQueues = []
        self._outQueues = []
        for _ in range(len(metrics)):
            self._inQueues.append(CrashQueue(maxsize=100))
            self._outQueues.append(CrashQueue(maxsize=100))
        self._genThread = multiprocessing.Process(
            target=_generatorThread,
            args=(self._inQueues, generator, numThreads // len(metrics)),
            daemon=True)
        self._batchers = []
        self._saverThreads = []
        for i in range(numThreads):
            metricIdx = i % len(metrics)
            batcherArgs = (modelFname, batchSize, self._inQueues[metricIdx],
                           self._outQueues[metricIdx], metrics[metricIdx],
                           numShuffles, kmerSize, memFrac, backend, useHypotheticalContribs,
                           shuffler)
            self._batchers.append(multiprocessing.Process(target=_generalBatcherThread,
                                                          args=batcherArgs, daemon=True))
        for i in range(len(metrics)):
            saverArgs = (self._outQueues[i], savers[i], numThreads // len(metrics))
            self._saverThreads.append(multiprocessing.Process(target=_saverThread,
                                                              args=saverArgs,
                                                              daemon=True))

    def run(self) -> None:
        """Start up the threads and waits for them to finish."""
        logUtils.info("Beginning flat run.")
        self._genThread.start()
        logUtils.debug("Started generator.")
        for b in self._batchers:
            b.start()
        logUtils.debug("Started batchers. Starting savers.")
        for s in self._saverThreads:
            s.start()
        logUtils.info("All processes started. Beginning main loop.")
        self._genThread.join()
        logUtils.debug("Generator joined.")
        for b in self._batchers:
            b.join()
        logUtils.debug("Batchers joined.")
        for s in self._saverThreads:
            s.join()
        logUtils.debug("Savers joined.")
        for s in self._savers:
            s.parentFinish()
        logUtils.info("Savers complete. Done.")


class FlatListSaver(Saver):
    """A simple Saver that holds the results in memory so you can use them immediately.

    Since the Saver is created in its own thread, just storing the results
    in this object doesn't work - they get removed when the writer
    process completes.
    So we need to create some shared memory. This Saver takes care of that for
    sequences, shap scores, and input predictions, but discards passData, since
    we don't know a priori how large passData objects will be. In a typical use
    case, you'd use this in situations where you already know which sequence is
    which, so saving passData doesn't really make sense anyway.

    :param numSamples: The total number of samples that this saver will get.
        It has to know this during construction so it can allocate
        enough memory.
    :param inputLength: The input length of the model.
    """

    inputLength: int
    numSamples: int
    shap: IMPORTANCE_AR_T
    seq: ONEHOT_AR_T
    inputPredictions: PRED_AR_T
    _results: list[Result]

    def __init__(self, numSamples: int, inputLength: int):
        self.inputLength = inputLength
        self.numSamples = numSamples
        # Note that these arrays are shared with the child.
        # Also note that the internal shared arrays are float32 or int8,
        # not float16 like normal importance scores (this is because
        # there's a float in ctypes, but not a float16.)
        self._outShapArray = multiprocessing.Array(ctypes.c_float,
                                                   numSamples * inputLength * NUM_BASES)
        self._outPredArray = multiprocessing.Array(ctypes.c_float,
                                                   numSamples)
        self._outSeqArray = multiprocessing.Array(ctypes.c_int8,
                                                  numSamples * inputLength * NUM_BASES)
        logUtils.debug("Created shared arrays for the list saver.")

    def construct(self) -> None:
        """Set up the data sets.

        This is run in the child process.
        """
        # Do this in the child so the parent doesn't accidentally mess with
        # an empty array.
        self._results = []
        logUtils.debug("Constructed child thread flat list saver.")

    def parentFinish(self) -> None:
        """Extract the data from the child process.

        This must be called to load the shap data from the child,
        since it's currently packed away inside a linear array.
        I could just expose _outShapArray, but this reorganizes it in a much
        more intuitive way.
        """
        logUtils.debug("Finishing in parent.")
        # Note that this will be IMPORTANCE_T, even though the internal
        # self.outShapArray is float32.
        self.shap = np.zeros((self.numSamples, self.inputLength, NUM_BASES), dtype=IMPORTANCE_T)
        self.seq = np.zeros((self.numSamples, self.inputLength, NUM_BASES), dtype=ONEHOT_T)
        self.inputPredictions = np.zeros((self.numSamples,), dtype=PRED_T)
        for idx in range(self.numSamples):
            self.inputPredictions[idx] = self._outPredArray[idx]
            for outOffset in range(self.inputLength):
                for k in range(NUM_BASES):
                    readHead = idx * self.inputLength * NUM_BASES + outOffset * NUM_BASES + k
                    self.shap[idx, outOffset, k] = self._outShapArray[readHead]
                    self.seq[idx, outOffset, k] = self._outSeqArray[readHead]
        logUtils.debug("Finished list saver in parent thread. Your data are ready!")

    def done(self) -> None:
        """Copy over the data from the child thread to the parent.

        This method is called from the child process.
        """
        for r in self._results:
            idx = r.index
            svals = r.shap
            seqvals = r.sequence
            for outOffset in range(self.inputLength):
                for k in range(NUM_BASES):
                    writeHead = idx * self.inputLength * NUM_BASES + outOffset * NUM_BASES + k
                    self._outShapArray[writeHead] = svals[outOffset, k]
                    oneHotBase = ctypes.c_int8(int(seqvals[outOffset, k]))
                    self._outSeqArray[writeHead] = oneHotBase  # type: ignore
            self._outPredArray[idx] = float(r.inputPrediction)
        logUtils.debug("Finished list saver in child thread.")

    def add(self, result: Result) -> None:  # type: ignore
        """Add the result to the internal list.

        :param result: The result from shapping a region.

        This is called from the child process.
        """
        self._results.append(result)


class FlatH5Saver(Saver):
    """Saves the shap scores to the output file.

    :param outputFname: is the name of the hdf5-format file that the shap scores will
        be deposited in.
    :param numSamples: is the number of regions (i.e., bases) that PISA will be run on.
        This is needed because we store reference predictions.
    :param inputLength: The input length of the model.
    :param genome: (Optional) Gives the name of a fasta-format file that contains
        the genome of the organism. If provided, then chromosome name and size information
        will be included in the output, and, additionally, two other datasets will be
        created: coords_chrom, and coords_base.
    :param useTqdm: Should a progress bar be displayed?
    """

    chunkShape: tuple[int, int, int]
    _outputFname: str
    numSamples: int
    genomeFname: str | None
    inputLength: int
    _useTqdm: bool = False
    _outFile: h5py.File
    _chunksToWrite: dict[int, dict]
    pbar: tqdm.tqdm | None = None

    def __init__(self, outputFname: str, numSamples: int, inputLength: int,
                 genome: str | None = None, useTqdm: bool = False,
                 config: str | None = None):
        self.chunkShape = (min(H5_CHUNK_SIZE, numSamples), inputLength, NUM_BASES)
        self._outputFname = outputFname
        self.numSamples = numSamples
        self.genomeFname = genome
        self.inputLength = inputLength
        self._useTqdm = useTqdm
        self.config = str(config)

    def construct(self) -> None:
        """Set up the data sets for writing.

        This is called inside the child thread.
        """
        logUtils.info("Initializing saver.")
        self._outFile = h5py.File(self._outputFname, "w")
        bprfiles.addH5Metadata(self._outFile, config=self.config)
        self._chunksToWrite = {}

        self._outFile.create_dataset("hyp_scores",
                                     (self.numSamples, self.inputLength, NUM_BASES),
                                     dtype=IMPORTANCE_T, chunks=self.chunkShape,
                                     compression="gzip")
        self._outFile.create_dataset("input_predictions",
                                     (self.numSamples,),
                                     dtype=PRED_T, chunks=(self.chunkShape[0],),
                                     compression="gzip")
        self._outFile.create_dataset("input_seqs",
                                     (self.numSamples, self.inputLength, NUM_BASES),
                                     dtype=ONEHOT_T, chunks=self.chunkShape,
                                     compression="gzip")
        if self.genomeFname is not None:
            self._loadGenome()
        else:
            self._outFile.create_dataset("descriptions", (self.numSamples,),
                                         dtype=h5py.string_dtype("utf-8"))
        logUtils.debug("Saver initialized, hdf5 file created.")

    def _loadGenome(self) -> None:
        """Does a few things.

        1. It creates chrom_names and chrom_sizes datasets in the output hdf5.
          These two datasets just contain the (string) names and (unsigned) sizes for each one.
        2. It populates these datasets.
        3. It creates two datasets: coords_chrom and coords_base, which store, for every
           shap value calculated, what chromosome it was on, and where on that chromosome
           it was. These fields are populated during data receipt, because we don't have
           an ordering guarantee when we get data from the batcher.
        """
        logUtils.info("Loading genome information.")
        with pysam.FastaFile(self.genomeFname) as genome:  # type: ignore
            self._outFile.create_dataset("chrom_names",
                                         (genome.nreferences,),
                                         dtype=h5py.string_dtype(encoding="utf-8"))
            self.chromNameToIdx = {}
            posDtype = "u4"
            for chromName in genome.references:
                # Are any chromosomes longer than 2^31 bp long? If so (even though it's unlikely),
                # I must increase the storage size for positions.
                if genome.get_reference_length(chromName) > 2**31:
                    posDtype = "u8"

            self._outFile.create_dataset("chrom_sizes", (genome.nreferences, ), dtype=posDtype)

            for i, chromName in enumerate(genome.references):
                self._outFile["chrom_names"][i] = chromName
                self.chromNameToIdx[chromName] = i
                self._outFile["chrom_sizes"][i] = genome.get_reference_length(chromName)

            if genome.nreferences <= 255:  # type: ignore
                # If there are less than 255 chromosomes,
                # I only need 8 bits to store the chromosome ID.
                chromDtype = "u1"
            elif genome.nreferences <= 65535:  # type: ignore
                # If there are less than 2^16 chromosomes, I can use 16 bits.
                chromDtype = "u2"
            else:
                # If you have over four billion chromosomes, you deserve to have code
                # that will break. So I just assume that 32 bits will be enough.
                chromDtype = "u4"
            self._outFile.create_dataset("coords_chrom", (self.numSamples, ), dtype=chromDtype)

            self._outFile.create_dataset("coords_start", (self.numSamples, ), dtype=posDtype)
            self._outFile.create_dataset("coords_end", (self.numSamples, ), dtype=posDtype)
        logUtils.debug("Genome data loaded.")

    def done(self) -> None:
        """Close up shop.

        Called in the child process.
        """
        logUtils.debug("Saver closing.")
        if self.pbar is not None:
            self.pbar.close()
        self._outFile.close()

    def add(self, result: Result) -> None:  # type: ignore
        """Add the given result to the output file.

        :param result: The output from the batcher.
        """
        if self.pbar is None and self._useTqdm:
            # Initialize the progress bar.
            # I would do it earlier, but starting it before the batchers
            # have warmed up skews the stats.
            self.pbar = tqdm.tqdm(total=self.numSamples, smoothing=0.03)
        if self.pbar is not None:
            self.pbar.update()
        index = result.index
        # Which chunk does this result go in?
        chunkIdx = index // self.chunkShape[0]
        # And where in the chunk does it go?
        chunkOffset = index % self.chunkShape[0]
        if chunkIdx not in self._chunksToWrite:
            # Allocate a new chunk.
            # Note that we can't just statically allocate one chunk buffer because we don't
            # guarantee the order that the results will arrive in.
            numToAdd = self.chunkShape[0]
            if chunkIdx == self.numSamples // self.chunkShape[0]:
                numToAdd = self.numSamples % self.chunkShape[0]
            curChunkShape = (numToAdd, self.chunkShape[1], self.chunkShape[2])
            self._chunksToWrite[chunkIdx] = {
                "hyp_scores": np.empty(curChunkShape, dtype=IMPORTANCE_T),
                "input_seqs": np.empty(curChunkShape, dtype=ONEHOT_T),
                "input_predictions": np.empty((curChunkShape[0],), dtype=PRED_T),
                "numToAdd": numToAdd,
                "writeStart": chunkIdx * self.chunkShape[0],
                "writeEnd": chunkIdx * self.chunkShape[0] + numToAdd}
        curChunk = self._chunksToWrite[chunkIdx]
        curChunk["numToAdd"] -= 1
        curChunk["hyp_scores"][chunkOffset] = result.shap
        curChunk["input_seqs"][chunkOffset] = result.sequence
        curChunk["input_predictions"][chunkOffset] = result.inputPrediction
        if curChunk["numToAdd"] == 0:
            # We added the last missing entry to this chunk. Write it to the file.
            start = curChunk["writeStart"]
            end = curChunk["writeEnd"]
            self._outFile["input_seqs"][start:end, :, :] = curChunk["input_seqs"]
            self._outFile["hyp_scores"][start:end, :, :] = curChunk["hyp_scores"]
            self._outFile["input_predictions"][start:end] = curChunk["input_predictions"]
            # Free the memory held by this chunk.
            del self._chunksToWrite[chunkIdx]

        # Okay, now we either add the description line, or add a genomic coordinate.
        # These are not chunked, since the data aren't compressed.
        if self.genomeFname is not None:
            self._outFile["coords_chrom"][index] = self.chromNameToIdx[result.passData[0]]
            self._outFile["coords_start"][index] = result.passData[1]
            self._outFile["coords_end"][index] = result.passData[2]
        else:
            self._outFile["descriptions"][index] = result.passData


class PisaH5Saver(Saver):
    """Saves the shap scores to the output file.

    :param outputFname: is the name of the hdf5-format file that the shap scores
        will be deposited in.
    :param numSamples: is the number of regions (i.e., bases) that PISA will be run on.
    :param numShuffles: is the number of shuffles that are used to generate the reference.
        This is needed because we store reference predictions.
    :param receptiveField: How wide is the model's receptive field?
    :param genome: an optional parameter, gives the name of a fasta-format file that contains
        the genome of the organism. If provided, then chromosome name and size information
        will be included in the output, and, additionally, two other datasets will be
        created: coords_chrom, and coords_base.
    :param useTqdm: Should a progress bar be displayed?
    """

    def __init__(self, outputFname: str, numSamples: int, numShuffles: int, receptiveField: int,
                 genome: str | None = None, useTqdm: bool = False, config: str | None = None):
        logUtils.info("Initializing saver.")
        self._outputFname = outputFname
        self.numSamples = numSamples
        self.numShuffles = numShuffles
        self.genomeFname = genome
        self._useTqdm = useTqdm
        self.receptiveField = receptiveField
        self.chunkShape = (min(H5_CHUNK_SIZE, numSamples), receptiveField, NUM_BASES)
        self.config = str(config)

    def construct(self) -> None:
        """Run in the child thread."""
        self._outFile = h5py.File(self._outputFname, "w")
        bprfiles.addH5Metadata(self._outFile, config=self.config)
        self._outFile.create_dataset("input_predictions",
                                     (self.numSamples,), dtype=PRED_T)
        self._outFile.create_dataset("shuffle_predictions",
                                     (self.numSamples, self.numShuffles),
                                     dtype=PRED_T)
        self._chunksToWrite = {}

        self._outFile.create_dataset("shap",
                                     (self.numSamples, self.receptiveField, NUM_BASES),
                                     dtype=IMPORTANCE_T, chunks=self.chunkShape,
                                     compression="gzip")
        self._outFile.create_dataset("sequence",
                                     (self.numSamples, self.receptiveField, NUM_BASES),
                                     dtype=ONEHOT_T, chunks=self.chunkShape,
                                     compression="gzip")
        self.pbar = None
        if self._useTqdm:
            self.pbar = tqdm.tqdm(total=self.numSamples, smoothing=0.03)
        if self.genomeFname is not None:
            self._loadGenome()
        else:
            self._outFile.create_dataset("descriptions", (self.numSamples,),
                                         dtype=h5py.string_dtype("utf-8"))
        logUtils.debug("Saver initialized, hdf5 file created.")

    def _loadGenome(self) -> None:
        """Load up genome information into the output hdf5.

        1. It creates chrom_names and chrom_sizes datasets in the output hdf5.
           These two datasets just contain the (string) names and (unsigned) sizes for each one.
        2. It populates these datasets.
        3. It creates two datasets: coords_chrom and coords_base, which store, for every
           shap value calculated, what chromosome it was on, and where on that chromosome
           it was. These fields are populated during data receipt, because we don't have
           an ordering guarantee when we get data from the batcher.
        """
        logUtils.info("Loading genome information.")
        with pysam.FastaFile(self.genomeFname) as genome:  # type: ignore
            self._outFile.create_dataset("chrom_names",
                                         (genome.nreferences,),
                                         dtype=h5py.string_dtype(encoding="utf-8"))
            self.chromNameToIdx = {}
            posDtype = "u4"
            for chromName in genome.references:
                # Are any chromosomes longer than 2^31 bp long? If so (even though it's unlikely),
                # I must increase the storage size for positions.
                if genome.get_reference_length(chromName) > 2**31:
                    posDtype = "u8"

            self._outFile.create_dataset("chrom_sizes", (genome.nreferences, ), dtype=posDtype)

            for i, chromName in enumerate(genome.references):
                self._outFile["chrom_names"][i] = chromName
                self.chromNameToIdx[chromName] = i
                self._outFile["chrom_sizes"][i] = genome.get_reference_length(chromName)

            if genome.nreferences <= 255:  # type: ignore
                # If there are less than 255 chromosomes,
                # I only need 8 bits to store the chromosome ID.
                chromDtype = "u1"
            elif genome.nreferences <= 65535:  # type: ignore
                # If there are less than 2^16 chromosomes, I can use 16 bits.
                chromDtype = "u2"
            else:
                # If you have over four billion chromosomes, you deserve to have code
                # that will break. So I just assume that 32 bits will be enough.
                chromDtype = "u4"
            self._outFile.create_dataset("coords_chrom", (self.numSamples, ), dtype=chromDtype)
            self._outFile.create_dataset("coords_base", (self.numSamples, ), dtype="u4")
        logUtils.debug("Genome data loaded.")

    def done(self) -> None:
        """Close out any open files before the child process exits.

        Called from the child process.
        """
        logUtils.debug("Saver closing.")
        if self.pbar is not None:
            self.pbar.close()
        self._outFile.close()

    def add(self, result: Result) -> None:  # type: ignore
        """Add the given result to the output file.

        :param result: The output from the batcher.
        """
        if self.pbar is not None:
            self.pbar.update()
        index = result.index
        # Which chunk does this result go in?
        chunkIdx = index // self.chunkShape[0]
        # And where in the chunk does it go?
        chunkOffset = index % self.chunkShape[0]
        if chunkIdx not in self._chunksToWrite:
            # Allocate a new chunk.
            # Note that we can't just statically allocate one chunk buffer because we don't
            # guarantee the order that the results will arrive in.
            numToAdd = self.chunkShape[0]
            if chunkIdx == self.numSamples // self.chunkShape[0]:
                numToAdd = self.numSamples % self.chunkShape[0]
            curChunkShape = (numToAdd, self.chunkShape[1], self.chunkShape[2])
            self._chunksToWrite[chunkIdx] = {
                "shap": np.empty(curChunkShape, dtype=IMPORTANCE_T),
                "sequence": np.empty(curChunkShape, dtype=ONEHOT_T),
                "numToAdd": numToAdd,
                "writeStart": chunkIdx * self.chunkShape[0],
                "writeEnd": chunkIdx * self.chunkShape[0] + numToAdd}

        curChunk = self._chunksToWrite[chunkIdx]
        curChunk["numToAdd"] -= 1
        curChunk["shap"][chunkOffset] = result.shap[:self.receptiveField]
        curChunk["sequence"][chunkOffset] = result.sequence[:self.receptiveField]
        if curChunk["numToAdd"] == 0:
            # We added the last missing entry to this chunk. Write it to the file.
            start = curChunk["writeStart"]
            end = curChunk["writeEnd"]
            self._outFile["sequence"][start:end, :, :] = curChunk["sequence"]
            self._outFile["shap"][start:end, :, :] = curChunk["shap"]
            # Free the memory held by this chunk.
            del self._chunksToWrite[chunkIdx]
        self._outFile["input_predictions"][index] = result.inputPrediction
        self._outFile["shuffle_predictions"][index, :] = result.shufflePredictions
        # Okay, now we either add the description line, or add a genomic coordinate.
        if self.genomeFname is not None:
            self._outFile["coords_chrom"][index] = self.chromNameToIdx[result.passData[0]]
            self._outFile["coords_base"][index] = result.passData[1]
        else:
            self._outFile["descriptions"][index] = result.passData


class ListGenerator(Generator):
    """A very simple Generator that is initialized with an iterable of strings.

    (A list of strings is an iterable, but this works with generator functions
    and other things too!)

    :param sequences: Any iterable that yields strings, like a list of strings.
        Note that this function immediately converts whatever you pass in
        into a list, so very large iterables will consume a lot of memory.
    :param passDataList: (optional) Will be passed through the batcher to
        the saver.
    """

    def __init__(self, sequences: Iterable[str],
                 passDataList: list | None = None):
        self._readHead = 0
        self._sequences = list(sequences)
        self.numSamples = len(self._sequences)
        self.inputLength = len(self._sequences[0])
        self._passDataList = passDataList

    def construct(self) -> None:
        """Set up stuff in the child thread.

        Note that this doesn't load data - because the child thread is
        forked from the parent, it already contains the lists of data.
        """
        self._indexes = list(range(len(self._sequences)))
        if self._passDataList is None:
            self._passData = [""] * len(self._sequences)
        else:
            self._passData = self._passDataList

    def done(self) -> None:
        """Called in the child thread, does nothing."""

    def __iter__(self):
        """Returns self, because Generators are iterable."""
        return self

    def __next__(self) -> Query:
        """Get the next query, or raise StopIteration."""
        if len(self._sequences) == self._readHead:
            raise StopIteration()
        oneHotSequence = utils.oneHotEncode(self._sequences[self._readHead])
        idx = self._indexes[self._readHead]
        passData = self._passData[self._readHead]
        self._readHead += 1
        q = Query(oneHotSequence, passData, idx)
        return q


class FastaGenerator(Generator):
    """Reads a fasta file from disk and generates Queries from it.

    :param fastaFname: The name of the fasta-format file containing
        query sequences.
    """

    def __init__(self, fastaFname: str):
        logUtils.info("Creating fasta generator.")
        self.fastaFname = fastaFname
        self.nowStop = False
        numRegions = 0

        with open(fastaFname, "r") as fp:
            for line in fp:
                if len(line) > 0 and line[0] == ">":
                    numRegions += 1
        # Iterate through the input file real quick and find out how many regions I'll have.
        self.numRegions = numRegions
        self.index = 0
        logUtils.info(f"Fasta generator initialized with {self.numRegions} regions.")

    def construct(self) -> None:
        """Open the file and start reading."""
        logUtils.info("Constructing fasta generator in its thread.")
        self.fastaFile = open(self.fastaFname, "r")  # pylint: disable=consider-using-with
        self.nextSequenceID = self.fastaFile.readline()[1:].strip()
        # [1:] to get rid of the '>'.
        logUtils.debug(f"Initial sequence to read: {self.nextSequenceID}")

    def done(self) -> None:
        """Close the Fasta file."""
        logUtils.info("Closing fasta generator.")
        self.fastaFile.close()

    def __iter__(self):
        """Return self, because generators are Iterable."""
        logUtils.debug("Creating fasta iterator.")
        return self

    def __next__(self) -> Query:
        """Get the next Query."""
        if self.nowStop:
            raise StopIteration()
        sequence = ""
        prevSequenceID = self.nextSequenceID
        while True:
            nextLine = self.fastaFile.readline()
            if len(nextLine) == 0:
                self.nowStop = True
                break
            if nextLine[0] == ">":
                self.nextSequenceID = nextLine[1:].strip()
                break
            sequence += nextLine.strip()
        oneHotSequence = utils.oneHotEncode(sequence)
        q = Query(oneHotSequence, prevSequenceID, self.index)
        self.index += 1
        return q


class FlatBedGenerator(Generator):
    """Reads in lines from a bed file and fetches the genomic sequence around them.

    Note that the regions should have length outputLength, and they will be automatically
    padded to the appropriate input length.

    :param bedFname: The bed file to read.
    :param genomeFname: The genome fasta that sequences will be drawn from.
    :param inputLength: The input length of your model.
    :param outputLength: The output length of your model.
    """

    def __init__(self, bedFname: str, genomeFname: str, inputLength: int, outputLength: int):
        logUtils.info("Creating bed generator.")
        self.bedFname = bedFname
        self.genomeFname = genomeFname
        self.inputLength = inputLength
        self.outputLength = outputLength
        numRegions = 0
        # Note that this opens a file during construction (a no-no for my threading model)
        # but it just reads it and closes it right back up. When the initializer finishes,
        # there are no pointers to file handles in this object.
        fp = pybedtools.BedTool(self.bedFname)
        for _ in fp:
            numRegions += 1
        self.numRegions = numRegions
        logUtils.info(f"Bed generator initialized with {self.numRegions} regions")

    def construct(self) -> None:
        """Open the bed file and fasta genome."""
        # We create a list of all the regions now, but we'll look up the sequences
        # and stuff on the fly.
        logUtils.info("Constructing bed generator it its thread.")
        self.shapTargets = []
        self.genome = pysam.FastaFile(self.genomeFname)
        self.readHead = 0
        fp = pybedtools.BedTool(self.bedFname)
        for line in fp:
            self.shapTargets.append(line)

    def __iter__(self):
        """An iterator that makes a query for every region in the bed file."""
        logUtils.debug("Creating iterator for bed generator.")
        return self

    def __next__(self) -> Query:
        """Get the next sequence and make a Query with it."""
        if self.readHead >= len(self.shapTargets):
            raise StopIteration()
        r = self.shapTargets[self.readHead]
        padding = (self.inputLength - self.outputLength) // 2
        startPos = r.start - padding
        endPos = startPos + self.inputLength
        seq = self.genome.fetch(r.chrom, startPos, endPos)
        oneHot = utils.oneHotEncode(seq)
        ret = Query(oneHot, (r.chrom, startPos, endPos), self.readHead)
        self.readHead += 1
        return ret

    def done(self) -> None:
        """Close the fasta file."""
        logUtils.debug(f"Closing bed generator, read {self.readHead} entries")
        self.genome.close()


class PisaBedGenerator(Generator):
    """Reads in lines from a bed file and fetches the genomic sequence at every base.

    This is very different than the :py:class:`~FlatBedGenerator`, which generates
    one sequence query per bed file entry. This class generates a query for every
    base that the bed file contains.

    :param bedFname: The bed file to read.
    :param genomeFname: The genome fasta that sequences will be drawn from.
    :param inputLength: The input length of your model.
    :param outputLength: The output length of your model.
    """

    def __init__(self, bedFname: str, genomeFname: str, inputLength: int, outputLength: int):
        logUtils.info("Creating bed generator.")
        self.bedFname = bedFname
        self.genomeFname = genomeFname
        self.inputLength = inputLength
        self.outputLength = outputLength
        numRegions = 0
        # Note that this opens a file during construction (a no-no for my threading model)
        # but it just reads it and closes it right back up. When the initializer finishes,
        # there are no pointers to file handles in this object.
        fp = pybedtools.BedTool(self.bedFname)
        for line in fp:
            numRegions += line.end - line.start
        self.numRegions = numRegions
        logUtils.info(f"Bed generator initialized with {self.numRegions} regions")

    def construct(self) -> None:
        """Run in the child thread, opens up the files and reads the bed."""
        # We create a list of all the regions now, but we'll look up the sequences
        # and stuff on the fly.
        logUtils.debug("Constructing bed generator it its thread.")
        self.shapTargets = []
        self.genome = pysam.FastaFile(self.genomeFname)
        self.readHead = 0
        fp = pybedtools.BedTool(self.bedFname)
        for line in fp:
            for pos in range(line.start, line.end):
                self.shapTargets.append((line.chrom, pos))

    def __iter__(self):
        """An iterable that gives a Query for every *base* that the bed file contains."""
        logUtils.debug("Creating iterator for bed generator.")
        return self

    def __next__(self) -> Query:
        """Get the next sequence and make a Query with it."""
        if self.readHead >= len(self.shapTargets):
            raise StopIteration()
        curChrom, curStart = self.shapTargets[self.readHead]
        padding = (self.inputLength - self.outputLength) // 2
        startPos = curStart - padding
        stopPos = startPos + self.inputLength
        seq = self.genome.fetch(curChrom, startPos, stopPos)
        assert len(seq) == self.inputLength, \
            f"Sequence at {curChrom}, {curStart} has wrong length: {len(seq)}"
        oneHot = utils.oneHotEncode(seq)
        ret = Query(oneHot, (curChrom, curStart), self.readHead)
        self.readHead += 1
        return ret

    def done(self) -> None:
        """Close the fasta."""
        logUtils.debug(f"Closing bed generator, read {self.readHead} entries")
        self.genome.close()


def _generalBatcherThread(modelName: str, batchSize: int, inQueue: CrashQueue,
                          outQueue: CrashQueue, metric: Callable,
                          numShuffles: int, kmerSize: int, memFrac: float,
                          backend: str,
                          useHypotheticalContribs: bool | None = None,
                          shuffler: Callable | None = None) -> None:
    """Spins up a batcher for the given metric."""
    logUtils.debug("Starting flat batcher thread.")
    if backend == "shap":
        assert isinstance(useHypotheticalContribs, bool)
        b = _ShapBatcher(modelName, batchSize, outQueue, metric,
                         numShuffles, kmerSize, memFrac,
                         useHypotheticalContribs)
    elif backend == "ism":
        assert isinstance(shuffler, Callable)
        b = _IsmBatcher(modelName, batchSize, outQueue, metric,
                        memFrac, kmerSize, shuffler)
    else:
        raise RuntimeError(f"{backend} is not a supported interpretation backend.")
    logUtils.debug("Batcher created.")
    while True:
        query = inQueue.get()
        if query is None:
            break
        b.addSample(query)
    logUtils.debug("Last query received. Finishing batcher thread.")
    b.finishBatch()
    outQueue.put(None)
    outQueue.close()
    logUtils.debug("Batcher thread finished.")


def _generatorThread(inQueues: list[CrashQueue], generator: Generator,
                     numBatchers: int) -> None:
    """The thread that spins up the generator and emits queries.

    Each query that is generated will be placed in *all* of the input queues.
    This is so that two batchers that are working on different metrics will
    both get all of the input queries. (Each of these batchers would have its
    own saver) If you have two batchers working on the same metric (and
    therefore feeding to the same saver), then you would only provide one input
    queue and both of the batchers would pull from that queue.
    """
    logUtils.debug("Starting generator thread.")
    generator.construct()
    for elem in generator:
        for inQueue in inQueues:
            inQueue.put(elem)
    for inQueue in inQueues:
        for _ in range(numBatchers):
            inQueue.put(None)
        inQueue.close()
    logUtils.debug("Done with generator, None added to queue.")
    generator.done()
    logUtils.debug("Generator thread finished.")


def _saverThread(outQueue: CrashQueue, saver: Saver,
                 numBatchers: int) -> None:
    """The thread that spins up the saver."""
    logUtils.debug("Saver thread started.")
    saver.construct()
    batchersLeft = numBatchers
    while True:
        rv = outQueue.get()
        if rv is None:
            batchersLeft -= 1
            if batchersLeft == 0:
                break
            continue
        saver.add(rv)
    saver.done()
    logUtils.debug("Saver thread finished.")


class _IsmBatcher:
    """The workhorse of this stack.

    Accepts queries until its internal storage is full,
    then predicts them all at once, and runs ism.

    :param modelFname: The name of the keras model to interpret.
    :param batchSize: How many sequences should be interpreted at once?
    :param outQueue: The batcher will put its :py:class:`~Result` objects here.
    :param metric: A function that accepts a model and returns the scalar tensor
        for which attribution scores should be calculated.
    :param memFrac: What fraction of the total GPU memory is this batcher allowed to use?
    :param shuffleLength: The length of subsequences that will be shuffled.
    :param shuffler: A function to shuffle the bases for ISM calculations.
    """

    def __init__(self, modelFname: str, batchSize: int, outQueue: CrashQueue,
                 metric: Callable, memFrac: float,
                 shuffleLength: int, shuffler: Callable):
        logUtils.info("Initializing batcher.")
        utils.limitMemoryUsage(memFrac, 1024)
        self.model = utils.loadModel(modelFname)
        import bpreveal.internal.disableTensorflowLogging  # pylint: disable=unused-import # noqa
        from bpreveal.internal import shap
        self.batchSize = 1  # We use a single-sequence batch for this batcher,
        # because the ISMDeepExplainer does its own internal batching.
        self.outQueue = outQueue
        self.curBatch = []
        logUtils.debug("Batcher prepared, creating explainer.")
        self._outTarget = metric(self.model)
        self.profileExplainer = shap.ISMDeepExplainer(
            (self.model.input, self._outTarget),
            shuffler, shuffleLength, self.model.useOldKeras,
            batchSize)
        logUtils.info("Batcher initialized, Explainer initialized. Ready for Queries to explain.")

    def addSample(self, query: Query) -> None:
        """Add a query to the batch.

        :param query: The query to enqueue.

        Runs the batch if it has enough work accumulated.
        """
        self.curBatch.append(query)
        if len(self.curBatch) >= self.batchSize:
            self.finishBatch()

    def finishBatch(self) -> None:
        """If there's any work waiting to be done, do it."""
        if len(self.curBatch) > 0:
            self.runPrediction()
            self.curBatch = []

    def runPrediction(self) -> None:
        """Actually run the batch."""
        # Run the ism shap tool.
        numQueries = len(self.curBatch)
        inputLength = self.curBatch[0].sequence.shape[0]
        oneHotBuf = np.empty((numQueries, inputLength, NUM_BASES),
                             dtype=MODEL_ONEHOT_T)
        for i, q in enumerate(self.curBatch):
            oneHotBuf[i, :, :] = q.sequence
        shapScores, inputPreds = self.profileExplainer.shap_values([oneHotBuf])  # type: ignore
        # And now we need to run over that batch again to write the output.
        for i, q in enumerate(self.curBatch):
            querySequence = oneHotBuf[i, :, :]
            queryPred = inputPreds[i]
            queryShufPreds = [0]  # This is meaningless for the ism backend.
            queryShapScores = shapScores[i, :, :]  # type: ignore
            ret = Result(queryPred, queryShufPreds, querySequence,  # type: ignore
                         queryShapScores, q.passData, q.index)
            self.outQueue.put(ret)


class _ShapBatcher:
    """The workhorse of this stack.

    Accepts queries until its internal storage is full, then predicts
    them all at once, and runs shap.

    :param modelFname: The name of the keras model to interpret.
    :param batchSize: How many sequences should be interpreted at once?
    :param outQueue: The batcher will put its :py:class:`~Result` objects here.
    :param metric: A function that accepts a model and returns the scalar tensor
        for which attribution scores should be calculated.
    :param numShuffles: How many shuffled samples should be run? This is ignored for
        the ``"ism"`` backend.
    :param receptiveField: What is the receptive field of the model?
    :param kmerSize: What length of kmer should have its distribution preserved in the shuffles?
    :param memFrac: What fraction of the total GPU memory is this batcher allowed to use?
    :param useHypotheticalContribs: Should the contribution scores use the hypothetical
        combine_mult_and_diffref function, or the normal shap one?
    """

    def __init__(self, modelFname: str, batchSize: int, outQueue: CrashQueue,
                 metric: Callable, numShuffles: int, kmerSize: int, memFrac: float,
                 useHypotheticalContribs: bool):
        logUtils.info("Initializing batcher.")
        utils.limitMemoryUsage(memFrac, 1024)
        self.model = utils.loadModel(modelFname)
        import bpreveal.internal.disableTensorflowLogging  # pylint: disable=unused-import # noqa
        from bpreveal.internal import shap
        if not isShappable(self.model):
            logUtils.error(f"The model you have provided ({modelFname}) is not shappable "
                           "because it contains LinearRegression layers. Re-train the "
                           "model with BPReveal > 4.3.0 and then re-run interpretation.")
            raise TypeError("Invalid model architecture.")
        self.batchSize = batchSize
        self.outQueue = outQueue
        self.curBatch = []
        self.numShuffles = numShuffles
        self.kmerSize = kmerSize
        logUtils.debug("Batcher prepared, creating explainer.")
        self._outTarget = metric(self.model)
        if useHypotheticalContribs:
            combiner = combineMultAndDiffref
        else:
            combiner = shap.standard_combine_mult_and_diffref
        self.profileExplainer = shap.TFDeepExplainer(
            (self.model.input, self._outTarget),
            self.generateShuffles,
            useOldKeras=self.model.useOldKeras,
            combine_mult_and_diffref=combiner)
        logUtils.info("Batcher initialized, Explainer initialized. Ready for Queries to explain.")

    def generateShuffles(self, modelInputs: list) -> list:
        """Take the input tensor and generate shuffled reference sequences.

        :param modelInputs: The inputs to the model. modelInputs[0] will be the OHE input sequence.
        :return: A list of one item, that item is all of the shuffled sequences.
        """
        if self.kmerSize == 1:
            rng = np.random.default_rng(seed=355687)
            shuffles = [rng.permutation(modelInputs[0], axis=0) for _ in range(self.numShuffles)]
            shuffles = np.array(shuffles)
            return [shuffles]
        shuffles = ushuffle.shuffleOHE(modelInputs[0], self.kmerSize, self.numShuffles,
                                       seed=355687)
        return [shuffles]

    def addSample(self, query: Query) -> None:
        """Add a query to the batch.

        :param query: The query to add to the batcher.

        Runs the batch if it has enough work accumulated.
        """
        self.curBatch.append(query)
        if len(self.curBatch) >= self.batchSize:
            self.finishBatch()

    def finishBatch(self) -> None:
        """If there's any work waiting to be done, do it."""
        if len(self.curBatch) > 0:
            self.runPrediction()
            self.curBatch = []

    def runPrediction(self) -> None:
        """Actually run the batch."""
        # Now for the meat of all this boilerplate. Take the query sequences and
        # run them through the model, then run the shuffles, then run shap.
        # Finally, put all the results in the output queue.
        # This needs more optimization, but for this initial pass, I'm not actually batching -
        # instead, I'm running all the samples individually. But I can change this later,
        # and that's what counts.
        # First, build up an array of sequences to test.
        numQueries = len(self.curBatch)
        inputLength = self.curBatch[0].sequence.shape[0]
        oneHotBuf = np.empty((numQueries * (self.numShuffles + 1), inputLength, NUM_BASES),
                             dtype=MODEL_ONEHOT_T)
        # To predict on as large a batch as possible, I put the actual sequences and all the
        # references for the current batch into this array. The first <nsamples> rows are the real
        # sequence, the next <numShuffles> rows are the shuffles of the first sequence, then
        # the next is the shuffles of the second sequence, like this:
        # REAL_SEQUENCE_1_REAL_SEQUENCE_1_REAL_SEQUENCE_1
        # REAL_SEQUENCE_2_REAL_SEQUENCE_2_REAL_SEQUENCE_2
        # SEQUENCE_1_FIRST_SHUFFLE_SEQUENCE_1_FIRST_SHUFF
        # SEQUENCE_1_SECOND_SHUFFLE_SEQUENCE_1_SECOND_SHU
        # SEQUENCE_1_THIRD_SHUFFLE_SEQUENCE_1_THIRD_SHUFF
        # SEQUENCE_2_FIRST_SHUFFLE_SEQUENCE_2_FIRST_SHUFF
        # SEQUENCE_2_SECOND_SHUFFLE_SEQUENCE_2_SECOND_SHU
        # SEQUENCE_2_THIRD_SHUFFLE_SEQUENCE_2_THIRD_SHUFF

        # Points to the index into oneHotBuf where the next data should be added.
        shuffleInsertHead = numQueries
        # These are the (real) sequences that will be passed to the explainer.
        # Note that it's a list of arrays, and each array has shape (1,inputLength,NUM_BASES)

        for i, q in enumerate(self.curBatch):
            oneHotBuf[i, :, :] = q.sequence
            shuffles = self.generateShuffles([q.sequence])[0]
            oneHotBuf[shuffleInsertHead:shuffleInsertHead + self.numShuffles, :, :] = shuffles
            shuffleInsertHead += self.numShuffles
        # Okay, now the data structures are set up.
        inputPred = self.profileExplainer.model.predict(oneHotBuf, verbose=0)
        # (We'll deconvolve that in a minute...)
        shapScores = self.profileExplainer.shap_values([oneHotBuf[:numQueries, :, :]])
        # And now we need to run over that batch again to write the output.
        shuffleReadHead = numQueries
        for i, q in enumerate(self.curBatch):
            querySequence = oneHotBuf[i, :, :]
            queryPred = inputPred[i]
            queryShufPreds = inputPred[shuffleReadHead:shuffleReadHead + self.numShuffles]
            shuffleReadHead += self.numShuffles
            queryShapScores = shapScores[i, :, :]  # type: ignore
            ret = Result(queryPred, queryShufPreds, querySequence,  # type: ignore
                         queryShapScores, q.passData, q.index)
            self.outQueue.put(ret)


def combineMultAndDiffref(mult: IMPORTANCE_AR_T, originalInput: ONEHOT_AR_T,
                          backgroundData: ONEHOT_AR_T) -> list:
    """Combine the shap multipliers and difference from reference to generate hypothetical scores.

    :param mult: The shap multipliers.
    :param originalInput: The one-hot encoded sequence being shapped.
    :param backgroundData: The shuffled references.
    :return: A list of hypothetical contributions.

    This is injected deep into shap and generates the hypothetical importance scores.
    """
    # This is copied from Zahoor's code.
    projectedHypotheticalContribs = \
        np.zeros_like(backgroundData[0]).astype("float")
    assert len(originalInput[0].shape) == 2
    for i in range(NUM_BASES):  # We're going to go over all the base possibilities.
        hypotheticalInput = np.zeros_like(originalInput[0]).astype("float")
        hypotheticalInput[:, i] = 1.0
        hypotheticalDiffref = hypotheticalInput[None, :, :] - backgroundData[0]
        hypotheticalContribs = hypotheticalDiffref * mult[0]
        projectedHypotheticalContribs[:, :, i] = np.sum(hypotheticalContribs, axis=-1)
    # There are no bias importances, so the np.zeros_like(orig_inp[1]) is not needed.
    return [np.mean(projectedHypotheticalContribs, axis=0)]


def isShappable(model) -> bool:  # noqa: ANN001
    """Check to see if the model can be shapped.

    Early versions of BPReveal created combined and transformation models that
    were incompatible with DeepShap.

    :param model: The (loaded) Keras model that you want to check.
    :type model: keras.Model
    :return: True if it's safe to shap the model.
    """

    def isShappableLayer(layer) -> bool:  # noqa: ANN001
        """Determine if a given layer is shappable.

        :param layer: The layer to check.
        :type layer: keras.Layer.
        :return: True if this layer can be used with DeepShap.
        """
        if hasattr(layer, "layers"):
            for sublayer in layer.layers:
                if not isShappableLayer(sublayer):
                    return False
        if str(layer).find("saved_model.load.LinearRegression") >= 0:
            # We have an old-style model. Shapping won't work.
            return False
        return True
    for layer in model.layers:
        if not isShappableLayer(layer):
            return False
    return True

# Copyright 2022-2025 Charles McAnany. This file is part of BPReveal. BPReveal is free software: You can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 2 of the License, or (at your option) any later version. BPReveal is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with BPReveal. If not, see <https://www.gnu.org/licenses/>.  # noqa
