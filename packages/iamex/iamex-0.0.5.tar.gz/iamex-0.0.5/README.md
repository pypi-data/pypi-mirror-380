# iamex

Librer√≠a Python simple y poderosa para acceder a m√∫ltiples modelos de inteligencia artificial de forma unificada.

[![PyPI version](https://badge.fury.io/py/iamex.svg)](https://badge.fury.io/py/iamex)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üöÄ Instalaci√≥n

```bash
pip install iamex
```

## üîë Clave de API (requerida)

Necesitas una API key para usar el SDK.

- D√≥nde obtenerla: `dev.iamex.io`
- C√≥mo configurarla (elige 1):
  - Variable de entorno
    - Windows PowerShell: `$env:IAMEX_API_KEY="TU_API_KEY"`
    - Linux/macOS: `export IAMEX_API_KEY="TU_API_KEY"`
  - Archivo `.env`
    ```
    IAMEX_API_KEY=TU_API_KEY
    ```
  - Pasarla directo en el c√≥digo
    ```python
    from iamex import IAMEX
    client = IAMEX(api_key="TU_API_KEY")
    ```

### Hello World (stream)

```python
from iamex import IAMEX

client = IAMEX()  # usa IAMEX_API_KEY del entorno
for chunk in client.chat.completions.create(
    model="IAM-advanced",
    messages=[{"role": "user", "content": "Di hola en 3 palabras."}],
    stream=True,
):
    piece = (chunk.get("choices") or [{}])[0].get("delta", {}).get("content") or ""
    if piece:
        print(piece, end="", flush=True)
```

### Async (nuevo)

```python
import asyncio
from iamex import AsyncIAMEX

async def main():
    async with AsyncIAMEX() as client:  # usa IAMEX_API_KEY del entorno
        buffer = []
        async for chunk in client.chat.completions.create(
            model="IAM-advanced",
            messages=[{"role": "user", "content": "Di hola en 3 palabras."}],
            stream=True,
        ):
            try:
                choice = (chunk.get("choices") or [])[0]
                delta = choice.get("delta") or {}
                piece = delta.get("content")
                if piece:
                    buffer.append(piece)
                    print(piece, end="", flush=True)
            except Exception:
                pass
        print("\n\nFinal:\n" + "".join(buffer))

asyncio.run(main())
```

Dependencias para async

Para usar el cliente as√≠ncrono instala `httpx`:

```bash
pip install httpx
```

## ‚úÖ Compatibilidad estilo OpenAI (Nuevo en v0.0.5)

Ahora puedes usar `iamex` en proyectos pensados para el SDK de OpenAI sin reescribir tu app. Solo instala el paquete y cambia el import. As√≠ de f√°cil.

### Migraci√≥n en 1 l√≠nea

```python
# Antes (OpenAI):
# from openai import OpenAI

# Ahora (iamex con interfaz OpenAI):
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")
```

### Responses API (estilo v1)

```python
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")

response = client.responses.create(
    model="IAM-advanced",
    input="Dime un haiku sobre Python"
)

print(response.output_text)
```

### Chat Completions

```python
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")

chat = client.chat.completions.create(
    model="IAM-advanced",
    messages=[
        {"role": "system", "content": "Eres un asistente √∫til"},
        {"role": "user", "content": "Explica la recursi√≥n en una frase"}
    ]
)

print(chat["choices"][0]["message"]["content"])
```

### Text Completions

```python
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")

comp = client.completions.create(
    model="IAM-advanced",
    prompt="Crea una lista de 3 ideas de apps"
)

print(comp["choices"][0]["text"])  # si el proveedor devuelve 'text'
```

### Listado de modelos

```python
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")
models = client.models.list()
print(models)
```

## ü§ñ Modelos Disponibles

- **iam-adv-mex** - Modelo avanzado en espa√±ol
- **IAM-lite** - Modelo ligero en espa√±ol
- **IAM-advanced** - Modelo avanzado en espa√±ol (recomendado)
- **iam-lite-mex** - Modelo ligero en espa√±ol

## üìù Uso R√°pido

### Estilo compatible (recomendado)

```python
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")

# Responses API
resp = client.responses.create(model="IAM-advanced", input="¬øQu√© es Python?")
print(resp.output_text)

# Chat Completions
chat = client.chat.completions.create(
    model="IAM-advanced",
    messages=[{"role": "user", "content": "Explica una funci√≥n lambda"}]
)
print(chat["choices"][0]["message"]["content"])
```

### Listar modelos

```python
models = client.models.list()
print(models)
```

## ‚ö° Streaming (Nuevo en v0.0.5)

Soportamos respuestas en streaming con `stream=True` en `chat.completions.create`, `completions.create` y `responses.create`.

- Formato de stream: l√≠neas `data: { ... }` y cierre `data: [DONE]`.
- Los chunks intermedios traen el texto parcial; `finish_reason` llega al final.
- M√©tricas `usage` normalmente no vienen dentro del stream.

### Ejemplo: Chat Completions (stream)

```python
from iamex import IAMEX

client = IAMEX(api_key="tu_api_key")

buffer = []
for chunk in client.chat.completions.create(
    model="IAM-advanced",
    messages=[{"role": "user", "content": "Escribe un haiku sobre Python"}],
    stream=True,
):
    try:
        choice = (chunk.get("choices") or [])[0]
        delta = choice.get("delta") or {}
        piece = delta.get("content")
        if piece:
            buffer.append(piece)
            print(piece, end="", flush=True)
    except Exception:
        pass

print("\n\nFinal:\n" + "".join(buffer))
```

Chunk t√≠pico (simplificado):

```text
data: {"id":"chatcmpl_x","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"role":"assistant","content":"Hola"},"finish_reason":null}]}
data: {"id":"chatcmpl_x","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":", mundo"},"finish_reason":null}]}
data: {"id":"chatcmpl_x","object":"chat.completion.chunk","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}
data: [DONE]
```

### Ejemplo: Text Completions (stream)

```python
buffer = []
for chunk in client.completions.create(
    model="IAM-advanced",
    prompt="Di hola en 3 palabras.",
    stream=True,
):
    try:
        choice = (chunk.get("choices") or [])[0]
        piece = choice.get("text") or (choice.get("delta") or {}).get("content")
        if piece:
            buffer.append(piece)
            print(piece, end="", flush=True)
    except Exception:
        pass

print("\n\nFinal:\n" + "".join(buffer))
```

Chunk t√≠pico (simplificado):

```text
data: {"id":"cmpl_x","object":"text_completion.chunk","choices":[{"index":0,"text":"Hola","finish_reason":null}]}
data: {"id":"cmpl_x","object":"text_completion.chunk","choices":[{"index":0,"text":", mundo","finish_reason":null}]}
data: {"id":"cmpl_x","object":"text_completion.chunk","choices":[{"index":0,"text":"","finish_reason":"stop"}]}
data: [DONE]
```

### Ejemplo: Responses (stream)

```python
for chunk in client.responses.create(
    model="IAM-advanced",
    input="Escribe un haiku de Python.",
    stream=True,
):
    # Si el chunk trae texto directo
    if isinstance(chunk, dict) and chunk.get("output_text"):
        print(chunk["output_text"], end="", flush=True)
        continue
    # Fallback: mostrar chunk crudo para debugging
    print(chunk)
```

Notas r√°pidas:
- Imprime texto a medida que llegan los chunks; al final, llega `finish_reason` con "stop" o similar.
- Si tu app necesita el texto completo, acum√∫lalo en un buffer y √∫nelo al final.

## üîß Funcionalidades Principales

### 1. Solo Contenido vs Respuesta Completa (v0.0.4)

Por defecto, `iamex` devuelve solo el texto de la respuesta. Si necesitas metadatos adicionales, usa `full_response=True`:

```python
# Solo contenido (default)
content = send_prompt("Hola", api_key, "IAM-advanced")
print(content)  # "¬°Hola! ¬øEn qu√© puedo ayudarte?"

# Respuesta completa con metadatos
full_response = send_prompt("Hola", api_key, "IAM-advanced", full_response=True)
print(full_response)  # {"choices": [...], "usage": {"tokens": 25}, ...}
```

### 2. Par√°metros de Control

```python
# Con l√≠mite de tokens
response = send_prompt(
    prompt="Explica la IA en detalle",
    api_key="tu_api_key",
    model="IAM-advanced",
    max_tokens=100  # Limitar respuesta
)

# Con mensaje del sistema
response = send_prompt(
    prompt="¬øC√≥mo funciona un bucle for?",
    api_key="tu_api_key",
    model="IAM-advanced",
    system_prompt="Eres un tutor de programaci√≥n para principiantes"
)
```

### 3. Conversaciones Avanzadas (v0.0.4)

```python
# Conversaci√≥n multi-turno
messages = [
    {"role": "system", "content": "Eres un asistente de cocina"},
    {"role": "user", "content": "¬øC√≥mo hago pasta?"},
    {"role": "assistant", "content": "Para hacer pasta necesitas..."},
    {"role": "user", "content": "¬øY qu√© salsa me recomiendas?"}
]

response = send_messages(messages, api_key, "IAM-advanced")
```

## üõ†Ô∏è Uso Avanzado

### Cliente Personalizado

```python
from iamex import PromptClient

# Inicializar el cliente con tu API key
client = PromptClient(api_key="tu_api_key_aqui")

# Enviar un prompt (usa modelo por defecto 'IAM-advanced')
response = client.send_prompt(
    prompt="Explica qu√© es la inteligencia artificial"
)
print(response)

# Con respuesta completa
full_response = client.send_prompt("¬øQu√© es la IA?", full_response=True)
print(full_response['usage']['total_tokens'])  # Ver tokens usados
```

### Par√°metros Avanzados

El m√©todo `send_prompt` y `PromptClient` aceptan par√°metros adicionales para control fino:

```python
response = client.send_prompt(
    prompt="Tu prompt aqu√≠",
    model="IAM-advanced",
    temperature=0.5,        # Controla la creatividad (0.0 - 1.0)
    max_tokens=1000,        # M√°ximo n√∫mero de tokens en la respuesta
    top_p=0.9,             # Controla la diversidad de la respuesta
    top_k=50,              # Limita las opciones de tokens
    repetition_penalty=1.1, # Evita repeticiones
    presence_penalty=0.1,   # Penaliza tokens ya presentes
    frequency_penalty=0.1,  # Penaliza tokens frecuentes
    stream=False            # Respuesta en streaming
)
```

### Par√°metros por Defecto

Si no especificas par√°metros, se usan estos valores optimizados:

- `model`: `"IAM-advanced"`
- `temperature`: `0.3`
- `max_tokens`: `12000`
- `top_p`: `0.9`
- `top_k`: `50`
- `repetition_penalty`: `1.1`
- `presence_penalty`: `0.1`
- `frequency_penalty`: `0.1`
- `stream`: `False`

## üìä Estructura de Datos

### Estructura del Payload

La funci√≥n `send_prompt` env√≠a autom√°ticamente este payload a la API:

```json
{
  "apikey": "tu_api_key_aqui",
  "model": "tu_modelo",
  "prompt": "tu_prompt"
}
```

Para `send_messages`:

```json
{
  "apikey": "tu_api_key_aqui",
  "model": "tu_modelo",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."}
  ]
}
```

**Par√°metros obligatorios:**
- `apikey`: Tu clave de API para autenticaci√≥n
- `model`: El modelo de IA a usar
- `prompt` o `messages`: Tu consulta o conversaci√≥n

**Par√°metros opcionales:**
- `system_prompt`: Instrucciones del sistema (solo con `send_prompt`)
- `max_tokens`: L√≠mite de tokens en la respuesta
- `temperature`, `top_p`, `top_k`: Par√°metros de generaci√≥n
- `full_response`: Control del tipo de respuesta (v0.0.4)

## üìö Funciones Disponibles

### `send_prompt()`
```python
send_prompt(
    prompt: str,           # Tu pregunta o instrucci√≥n
    api_key: str,          # Clave de API (obt√©n en dev.iamex.io)
    model: str,            # Modelo a usar
    full_response: bool = False,  # Respuesta completa o solo contenido
    max_tokens: int = None,       # L√≠mite de tokens (opcional)
    system_prompt: str = None,    # Mensaje del sistema (opcional)
    **kwargs                      # Par√°metros adicionales
)
```

### `send_messages()` (Nuevo en v0.0.4)
```python
send_messages(
    messages: list,        # Lista de mensajes de conversaci√≥n
    api_key: str,          # Clave de API
    model: str,            # Modelo a usar
    full_response: bool = False,  # Respuesta completa o solo contenido
    max_tokens: int = None,       # L√≠mite de tokens (opcional)
    **kwargs                      # Par√°metros adicionales
)
```

### `PromptClient`
```python
client = PromptClient(api_key="tu_api_key")
client.send_prompt(prompt, model="IAM-advanced", **kwargs)
client.send_messages(messages, model="IAM-advanced", **kwargs)
client.get_models()  # Obtener modelos disponibles
```

## üîê Obtener API Key

1. Visita [dev.iamex.io](https://dev.iamex.io)
2. Reg√≠strate o inicia sesi√≥n
3. Obt√©n tu API key
4. Adquiere tokens seg√∫n tus necesidades

## ‚ö†Ô∏è Manejo de Errores

```python
try:
    response = send_prompt("Test", "api_key_invalida", "IAM-advanced")
    print(response)
except Exception as e:
    print(f"Error: {e}")
    # Salida: Error 401: API Key inv√°lida o no encontrada
```

### Errores Comunes
- **401**: API Key inv√°lida o no encontrada
- **400**: Par√°metros incorrectos
- **500**: Error interno del servidor
- **502**: Servidor temporalmente no disponible

## üìö Ejemplos Completos

### Chat B√°sico
```python
from iamex import send_messages

def chat_simple():
    api_key = "tu_api_key_aqui"
    model = "IAM-advanced"
    
    messages = [
        {"role": "system", "content": "Eres un asistente √∫til y amigable"}
    ]
    
    while True:
        user_input = input("T√∫: ")
        if user_input.lower() == 'salir':
            break
            
        messages.append({"role": "user", "content": user_input})
        
        try:
            response = send_messages(messages, api_key, model)
            print(f"IA: {response}")
            messages.append({"role": "assistant", "content": response})
        except Exception as e:
            print(f"Error: {e}")

chat_simple()
```

### An√°lisis de Texto
```python
from iamex import send_prompt

def analizar_sentimiento(texto, api_key):
    prompt = f"Analiza el sentimiento del siguiente texto: '{texto}'"
    
    response = send_prompt(
        prompt=prompt,
        api_key=api_key,
        model="IAM-advanced",
        system_prompt="Eres un experto en an√°lisis de sentimientos. Responde solo: Positivo, Negativo o Neutral."
    )
    
    return response

# Uso
sentimiento = analizar_sentimiento("¬°Me encanta este producto!", "tu_api_key")
print(sentimiento)  # "Positivo"
```

### Uso con M√©tricas (v0.0.4)
```python
from iamex import send_prompt

# Para obtener informaci√≥n de uso
response = send_prompt(
    "Explica brevemente qu√© es Python",
    api_key="tu_api_key",
    model="IAM-advanced",
    full_response=True
)

# Extraer m√©tricas
if isinstance(response, dict):
    usage = response.get('data', {}).get('response', {}).get('usage', {})
    print(f"Tokens usados: {usage.get('total_tokens', 'N/A')}")
    print(f"Costo estimado: ${usage.get('total_tokens', 0) * 0.001:.4f}")
```

## üèóÔ∏è Desarrollo

### Instalaci√≥n para Desarrollo
```bash
git clone https://github.com/IA-Mexico/iamex.git
cd iamex
pip install -e ".[dev]"
```

### Ejecutar Tests
```bash
pytest tests/
```

### Formatear C√≥digo
```bash
black src/ tests/
```

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver [LICENSE](LICENSE) para m√°s detalles.

## üÜò Soporte

- **Documentaci√≥n**: [GitHub](https://github.com/IA-Mexico/iamex)
- **Issues**: [GitHub Issues](https://github.com/IA-Mexico/iamex/issues)
- **Email**: hostmaster@iamex.io
- **Portal de desarrolladores**: [dev.iamex.io](https://dev.iamex.io)

## üìã Changelog

### v0.0.5 (Actual)
- ‚úÖ **NUEVO**: Compatibilidad estilo OpenAI SDK v1 (Responses y Completions)
- ‚úÖ **NUEVO**: Streaming en `chat.completions`, `completions` y `responses` con `stream=True`
- ‚úÖ **NUEVO**: Migraci√≥n en 1 l√≠nea: usa la clase `IAMEX` (interfaz estilo OpenAI)
- ‚úÖ **MEJORADO**: Documentaci√≥n para usar `responses.create`, `chat.completions.create` y `completions.create`
- ‚úÖ **NOTA**: No es necesario cambiar endpoints en tu c√≥digo

### v0.0.4
- ‚úÖ **NUEVO**: Par√°metro `full_response` para controlar el tipo de respuesta
- ‚úÖ **NUEVO**: Funci√≥n `send_messages` para conversaciones con formato de mensajes
- ‚úÖ **NUEVO**: Soporte para conversaciones multi-turno
- ‚úÖ **MEJORADO**: Extracci√≥n inteligente de contenido de respuestas
- ‚úÖ **MEJORADO**: Compatibilidad hacia atr√°s mantenida
- ‚úÖ **DOCUMENTACI√ìN**: Gu√≠as completas y ejemplos pr√°cticos

### v0.0.3
- ‚úÖ **NUEVO**: Par√°metro opcional `max_tokens` en funci√≥n `send_prompt`
- ‚úÖ **MEJORADO**: Control de longitud de respuestas del modelo
- ‚úÖ **MEJORADO**: Endpoint real de iam-hub implementado
- ‚úÖ **OPTIMIZADO**: Estructura de payload exacta para la API
- ‚úÖ **DOCUMENTACI√ìN**: Gu√≠as de uso para max_tokens

### v0.0.2
- ‚úÖ **NUEVO**: Funci√≥n simple `send_prompt(prompt, api_key, model)`
- ‚úÖ **NUEVO**: Soporte completo para autenticaci√≥n con API key
- ‚úÖ **NUEVO**: Conexi√≥n directa al endpoint real de iam-hub
- ‚úÖ **MEJORADO**: Estructura de payload exacta que espera la API

### v0.0.1
- Versi√≥n inicial con cliente b√°sico `PromptClient`
- Soporte para m√∫ltiples modelos de inferencia
- Modelo por defecto: `IAM-advanced`
- Par√°metros optimizados seg√∫n la API

---

**¬°Desarrollado con ‚ù§Ô∏è por Inteligencia Artificial M√©xico!**