@dlt.view()
{% if fail_expectations %}
# These expectations will fail the pipeline if violated
@dlt.expect_all_or_fail({{ fail_expectations | tojson }})
{% endif %}
{% if drop_expectations %}
# These expectations will drop rows that violate them
@dlt.expect_all_or_drop({{ drop_expectations | tojson }})
{% endif %}
{% if warn_expectations %}
# These expectations will log warnings but not drop rows
@dlt.expect_all({{ warn_expectations | tojson }})
{% endif %}
def {{ target_view }}():
    """{{ description }}"""
    {% if readMode == "stream" %}
    df = spark.readStream.table("{{ source_view }}")
    {% else %}
    df = spark.read.table("{{ source_view }}")
    {% endif %}
    {% if add_operational_metadata %}
    
    # Add operational metadata columns
    {% for col_name, expression in metadata_columns.items()|sort %}
    df = df.withColumn('{{ col_name }}', {{ expression }})
    {% endfor %}
    {% endif %}
    
    return df 