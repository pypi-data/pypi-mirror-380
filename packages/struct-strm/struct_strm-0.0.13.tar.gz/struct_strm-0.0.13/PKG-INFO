Metadata-Version: 2.4
Name: struct_strm
Version: 0.0.13
Summary: Stream partial json generated by LLMs into valid json responses
Project-URL: Homepage, https://prestonblackburn.github.io/structured_streamer/
Project-URL: Documentation, https://prestonblackburn.github.io/structured_streamer/
Project-URL: Issues, https://github.com/PrestonBlackburn/structured_streamer/issues
Project-URL: Repository, https://github.com/PrestonBlackburn/structured_streamer
Author-email: Preston Blackburn <prestonblckbrn@gmail.com>
License-Expression: MIT
License-File: LICENSE
Keywords: htmx,json,llm,streaming,web components
Classifier: Development Status :: 3 - Alpha
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: Implementation :: CPython
Requires-Python: >=3.10
Requires-Dist: tree-sitter-json
Requires-Dist: tree-sitter>=0.25
Provides-Extra: benchmark
Requires-Dist: matplotlib; extra == 'benchmark'
Requires-Dist: pandas; extra == 'benchmark'
Provides-Extra: dev
Requires-Dist: black; extra == 'dev'
Requires-Dist: fastapi; extra == 'dev'
Requires-Dist: jinja2; extra == 'dev'
Requires-Dist: openai; extra == 'dev'
Requires-Dist: pydantic; extra == 'dev'
Requires-Dist: pyright; extra == 'dev'
Requires-Dist: pytest; extra == 'dev'
Requires-Dist: pytest-asyncio; extra == 'dev'
Requires-Dist: pytest-cov[all]; extra == 'dev'
Requires-Dist: pytest-playwright; extra == 'dev'
Requires-Dist: python-multipart; extra == 'dev'
Requires-Dist: python-on-whales; extra == 'dev'
Requires-Dist: requests; extra == 'dev'
Requires-Dist: sse-starlette; extra == 'dev'
Requires-Dist: uvicorn; extra == 'dev'
Provides-Extra: docs
Requires-Dist: mkdocs; extra == 'docs'
Requires-Dist: mkdocs-material; extra == 'docs'
Requires-Dist: mkdocs-techdocs-core; extra == 'docs'
Provides-Extra: hf
Requires-Dist: numpy; extra == 'hf'
Requires-Dist: torch[cpu]; extra == 'hf'
Requires-Dist: transformers; extra == 'hf'
Requires-Dist: xgrammar; extra == 'hf'
Provides-Extra: openai
Requires-Dist: openai; extra == 'openai'
Provides-Extra: pydantic
Requires-Dist: pydantic; extra == 'pydantic'
Description-Content-Type: text/markdown


<div align="center">

<img src="https://raw.githubusercontent.com/PrestonBlackburn/structured_streamer/refs/heads/gh-pages/img/logo_bg_wide.png" alt="Struct Strm Logo" width="750" role="img">

| | |
| --- | --- |
| CI/CD | [![CI - Test](https://github.com/PrestonBlackburn/structured_streamer/actions/workflows/test_versions.yaml/badge.svg)](https://github.com/PrestonBlackburn/structured_streamer/actions/workflows/test_versions.yaml) [![CI - Build](https://github.com/PrestonBlackburn/structured_streamer/actions/workflows/release.yaml/badge.svg)](https://github.com/PrestonBlackburn/structured_streamer/actions/workflows/release.yaml)| 
| Package | [![PyPI - Version](https://img.shields.io/pypi/v/struct-strm.svg?logo=pypi&label=PyPI&logoColor=gold)](https://pypi.org/project/struct-strm/) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/struct-strm.svg?logo=python&label=Python&logoColor=gold)](https://pypi.org/project/struct-strm/) |
| Meta | [![Codestyle - Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/PrestonBlackburn/structured_streamer/blob/main/LICENSE) ![Coverage](https://PrestonBlackburn.github.io/structured_streamer/assets/coverage.svg)


</div>

<br/>


# Structured Streamer

**struct_strm** (structured streamer) is a Python package that makes it easy to stream partial json generated by LLMs into valid json responses. This enables partial rendering of UI components without needing to wait for a full response, drastically reducing the time to the first word on the user's screen.

## Why Use Structured Streamer?

JSON format is the standard when dealing with structured responses from LLMs. In the early days of LLM structured generation we had to validate the JSON response only after the whole JSON response had been returned. Modern approaches use constrained decoding to ensure that only valid json is returned, eliminating the need for post generation validation, and allowing us to use the response imediately. However, the streamed json response is incomplete, so it can't be parsed using traditional methods. This library aims to make it easier to handle this partially generated json to provide a better end user experience.   
See the [benchmarks](./benchmarks.md) section in the docs for more details about how this library can speed up your structured response processing. 

<br/>

You can learn more about constrained decoding and context free grammar here: [XGrammar - Achieving Efficient, Flexible, and Portable Structured Generation with XGrammar](https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar)   


### Installation

```bash
pip install struct-strm
```

<br/>

## Main Features

The primary feature is to wrap LLM outputs to produce valid incremental JSON from partial invalid JSON based on user provided structures. Effectively this acts as a wrapper for your LLM calls. Due to the nature of this library (it is primarily inteded for use in web servers), it is expected that it will be used in async workflows, and is async first.   

The library also provides simple HTML templates that serve as examples of how you can integrate the streams in your own components.  

Due to the nature of partial json streaming, there can be "wrong" ways to stream responses that are not effective for partial rendering of responeses in the UI. The library also provides examples of tested ways to apply the library to get good results.   

**High Level Flow**  
![High level flow](https://raw.githubusercontent.com/PrestonBlackburn/structured_streamer/refs/heads/main/docs/img/high_level_flow.png)



## Example Component
This is an example of a form component being incrementally rendered. By using a structured query response from an LLM, in this case a form with form field names and field placeholders, we can stream the form results directly to a HTML component. This drastically reduces the time to first token, and the precieved time that a user needs to wait. More advanced components are under development. 


```python
from stuct_strm import parse_openai
from pydantic import BaseModel
from openai import AsyncOpenAI

...

class DefaultFormItem(BaseModel):
    field_name: str = ""
    field_placeholder: str = ""

class DefaultFormStruct(BaseModel):
    form_fields: List[DefaultFormItem] = []


stream_response = client.beta.chat.completions.stream(
    model="gpt-4.1",
    messages=messages,
    response_format=DefaultFormStruct,
    temperature=0.0,
) 

form_struct_response = parse_openai(DefaultFormStruct, stream_response)
async for instance in form_struct_response:
    async for formstruct in instance:
        print(formstruct)
```


Fully formed python classes are returned:
```bash
>>>  DefaultFormStruct(form_fields=[DefaultFormItem(field_name="fruits", field_placeholder="")])
>>>  DefaultFormStruct(form_fields=[DefaultFormItem(field_name="fruits", field_placeholder="apple ")])
>>>  DefaultFormStruct(form_fields=[DefaultFormItem(field_name="fruits", field_placeholder="apple orange strawberry")])
>>>  etc....
```

And the corresponding incomplete json string streams would have looked like:
```txt
>>> "{"form_fields": [{"field_name": "fruits"
>>> "{"form_fields": [{"field_name": "fruits", "field_placeholder": "apple "
>>> "{"form_fields": [{"field_name": "fruits", "field_placeholder": "apple orange strawberry"}
>>> etc...
```

### Component Streaming
The structured responses can then be easily used to generate incrementally rendered web components.  
For example this form:   

![Example Form Streaming](https://raw.githubusercontent.com/PrestonBlackburn/structured_streamer/refs/heads/main/docs/img/form_struct_strm.gif)

<br/>

Or we can return data in a grid in more interesting ways.  
For example this rubric:   

![Example Rubric Streaming](https://raw.githubusercontent.com/PrestonBlackburn/structured_streamer/refs/heads/main/docs/img/rubric_example.gif)

## Other

I started **struct_strm** to support another project I'm working on to provide an easy entrypoint for Teachers to use LLM tools in their workflows. Check it out if you're interested - [Teachers PET](https://www.teacherspet.tech/)