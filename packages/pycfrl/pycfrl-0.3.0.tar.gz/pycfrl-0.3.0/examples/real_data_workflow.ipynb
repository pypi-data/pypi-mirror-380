{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e63007",
   "metadata": {},
   "source": [
    "# Assessing Policies Using Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ada81",
   "metadata": {},
   "source": [
    "In this workflow, PyCFRL takes in an offline trajectory and then preprocesses the offline trajectory using `SyntheticPreprocessor`. After that, the preprocessed trajectory is passed into `FQI` to train a counterfactually fair policy, which is then assessed using :code:`evaluate_reward_through_fqe()` and `evaluate_fairness_through_model()` based on a `SimulatedEnvironment` that mimics the transition rules of the true environment underlying the training trajectory. The final output of the workflow is the policy trained on the preprocessed data as well as its estimated value and counterfactual fairness metric. This workflow is appropriate when the user is interested in knowing the value and counterfactual fairness achieved by the trained policy when interacting with the true underlying environment.\n",
    "\n",
    "We begin by importing the libraries needed for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6ee3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"E:/learning/university/MiSIL/CFRL Python Package/CFRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78109a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pycfrl in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (0.1.1)\n",
      "Requirement already satisfied: gymnasium>=0.26.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pycfrl) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pycfrl) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pycfrl) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn~=1.7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pycfrl) (1.7.2)\n",
      "Requirement already satisfied: torch~=2.4.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pycfrl) (2.4.1)\n",
      "Requirement already satisfied: tqdm~=4.66.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pycfrl) (4.66.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from gymnasium>=0.26.0->pycfrl) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from gymnasium>=0.26.0->pycfrl) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from gymnasium>=0.26.0->pycfrl) (0.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.5.0->pycfrl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.5.0->pycfrl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.5.0->pycfrl) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn~=1.7.0->pycfrl) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn~=1.7.0->pycfrl) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn~=1.7.0->pycfrl) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from torch~=2.4.1->pycfrl) (3.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from torch~=2.4.1->pycfrl) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from torch~=2.4.1->pycfrl) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from torch~=2.4.1->pycfrl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from torch~=2.4.1->pycfrl) (2024.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tqdm~=4.66.2->pycfrl) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->pycfrl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch~=2.4.1->pycfrl) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch~=2.4.1->pycfrl) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pycfrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22b08a76610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycfrl.reader.reader import read_trajectory_from_dataframe\n",
    "from pycfrl.preprocessor.preprocessor import SequentialPreprocessor\n",
    "from pycfrl.agents.agents import FQI\n",
    "from pycfrl.environment.environment import SimulatedEnvironment\n",
    "from pycfrl.evaluation.evaluation import evaluate_reward_through_fqe, evaluate_fairness_through_model\n",
    "np.random.seed(10) # ensure reproducibility\n",
    "torch.manual_seed(10) # ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c8df4",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136b2d7",
   "metadata": {},
   "source": [
    "In this demonstration, we use an offline trajectory generated from a `SyntheticEnvironment` using some pre-specified transition rules. Although it is actually synthesized, we treat it as if it is from some unknown environment for pedagogical convenience in this demonstration.\n",
    "\n",
    "The trajectory contains 500 individuals (i.e. $N=500$) and 10 transitions (i.e. $T=10$). The actions are binary ($0$ or $1$) and were sampled using a random policy that selects $0$ or $1$ randomly with equal probability. It is stored in a tabular format in a `.csv` file. The sensitive attribute variable is univariate, stored in the column `z1`. The legit values of the sensitive attribute are $0$ and $1$. The state variable is also univariate, stored in the column `state1`. The actions are stored in the column `action` and rewards in the column `reward`. The tabular data also includes an extra irrelevant column `timestamp`. \n",
    "\n",
    "We can load and view the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8affb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>z1</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>state1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.324345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.524345</td>\n",
       "      <td>-0.813722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.613722</td>\n",
       "      <td>-0.526683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.326683</td>\n",
       "      <td>-0.464447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.264447</td>\n",
       "      <td>-2.075518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>5495</td>\n",
       "      <td>500.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.468460</td>\n",
       "      <td>-0.941954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>5496</td>\n",
       "      <td>500.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.430345</td>\n",
       "      <td>-2.536595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>5497</td>\n",
       "      <td>500.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.068298</td>\n",
       "      <td>-0.946557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>5498</td>\n",
       "      <td>500.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273278</td>\n",
       "      <td>-0.709017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>5499</td>\n",
       "      <td>500.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.154508</td>\n",
       "      <td>-0.761890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     ID  timestamp   z1  action    reward    state1\n",
       "0              0    1.0        1.0  0.0     NaN       NaN  1.324345\n",
       "1              1    1.0        2.0  0.0     1.0  1.524345 -0.813722\n",
       "2              2    1.0        3.0  0.0     1.0 -0.613722 -0.526683\n",
       "3              3    1.0        4.0  0.0     1.0 -0.326683 -0.464447\n",
       "4              4    1.0        5.0  0.0     1.0 -0.264447 -2.075518\n",
       "...          ...    ...        ...  ...     ...       ...       ...\n",
       "5495        5495  500.0        7.0  1.0     1.0 -2.468460 -0.941954\n",
       "5496        5496  500.0        8.0  1.0     1.0 -1.430345 -2.536595\n",
       "5497        5497  500.0        9.0  1.0     0.0 -1.068298 -0.946557\n",
       "5498        5498  500.0       10.0  1.0     0.0 -0.273278 -0.709017\n",
       "5499        5499  500.0       11.0  1.0     0.0 -0.154508 -0.761890\n",
       "\n",
       "[5500 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = pd.read_csv('../data/sample_data_large_uni.csv')\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fd846",
   "metadata": {},
   "source": [
    "We now read the trajectory from the tabular format into Trajectory Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae8c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs, states, actions, rewards, ids = read_trajectory_from_dataframe(\n",
    "                                                data=trajectory, \n",
    "                                                z_labels=['z1'], \n",
    "                                                state_labels=['state1'], \n",
    "                                                action_label='action', \n",
    "                                                reward_label='reward', \n",
    "                                                id_label='ID', \n",
    "                                                T=10\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429bf34",
   "metadata": {},
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4550d0",
   "metadata": {},
   "source": [
    "We split the trajectory data into a training set (80%) and a testing set (20%). The training set is used to train the counterfactually fair policy, while the testing set is used to evaluate the value and counterfactual fairness metric achieved by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1849086",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    zs_train, zs_test, \n",
    "    states_train, states_test, \n",
    "    actions_train, actions_test, \n",
    "    rewards_train, rewards_test\n",
    ") = train_test_split(zs, states, actions, rewards, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569dad1",
   "metadata": {},
   "source": [
    "## Preprocessor Training & Trajectory Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477df69",
   "metadata": {},
   "source": [
    "We now train the preprocessor and preprocess the trajectory. As demonstrated in the other workflows, we might want to first train the preprocessor using only a subset of the data, then preprocess the remaining subset of the data, and finally use the preprocessed subset for policy learning. However, when the amount of data is limited, the preprocessed trajectory resulting from the procedure above might be too small to be useful for policy learning. We essentially want to preprocess as many individuals as possible. Fortunately, we can directly preprocess all individuals using the `train_preprocessor()` function when we set `cross_folds` to a relatively large number.\n",
    "\n",
    "When `cross_folds=K` where `K` is greater than 1, `train_preprocessor()` will internally divide the training data into `K` folds. For each $i=1,\\dots,K$, it trains a transition dynamics model based on all the folds other than the $i$-th one, and this model is then used to preprocess data in the $i$-th fold. This results in `K` folds of preprocessed data, each of which is processed using a model that is trained on the other folds. These `K` folds of preprocessed data are then combined and returned by `train_preprocessor()`. This method allows us to preprocess all individuals in the trajectory while reducing overfitting.\n",
    "\n",
    "To use this functionality, we first initialize a `SequentialPreprocessor` with `cross_folds` greater than 1. We use `cross_folds=5` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab09c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SequentialPreprocessor(z_space=[[0], [1]], \n",
    "                            num_actions=2, \n",
    "                            cross_folds=5, \n",
    "                            mode='single', \n",
    "                            reg_model='nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e82e21",
   "metadata": {},
   "source": [
    "We now simultaneously train the preprocessor and preprocess all individuals in the trajectory using the precedure described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f097e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:40<00:00, 24.47it/s]\n",
      "\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 1000/1000 [00:40<00:00, 24.68it/s]\n",
      "100%|██████████| 1000/1000 [00:47<00:00, 21.04it/s]\n",
      "100%|██████████| 1000/1000 [00:48<00:00, 20.50it/s]\n",
      "100%|██████████| 1000/1000 [00:49<00:00, 20.27it/s]\n"
     ]
    }
   ],
   "source": [
    "states_tilde, rewards_tilde = sp.train_preprocessor(zs=zs_train, \n",
    "                                                    xs=states_train, \n",
    "                                                    actions=actions_train, \n",
    "                                                    rewards=rewards_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1682729",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b046e",
   "metadata": {},
   "source": [
    "Now we train a policy using `FQI` and the preprocessed data with `sp` as its internal preprocessor. Note that the training data `state_tilde` and `rewards_tilde` are already preprocessed. Thus, we set `preprocess=False` during training so that the input trajectory will not be preprocessed again by the internal preprocessor (i.e. `sp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b7cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<01:11,  1.37it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQI training\n"
     ]
    }
   ],
   "source": [
    "agent = FQI(num_actions=2, model_type='nn', preprocessor=sp)\n",
    "agent.train(zs=zs_train, \n",
    "            xs=states_tilde, \n",
    "            actions=actions_train, \n",
    "            rewards=rewards_tilde, \n",
    "            max_iter=100, \n",
    "            preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b243926",
   "metadata": {},
   "source": [
    "## `SimulatedEnvironment` Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdddb8",
   "metadata": {},
   "source": [
    "Before moving on to the evaluation stage, there is one more thing to do: We need to train a `SimulatedEnvironment` that mimics the transition rules of the true environment that generated the training trajectory, which will be used by the evaluation functions to simulate the true data-generating environment. To do so, we initialize a `SimulatedEnvironment` and train it on the whole trajectory data (i.e. training set and testing set combined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599323d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.30it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.79it/s]\n",
      "100%|██████████| 1000/1000 [00:34<00:00, 29.40it/s]\n",
      "\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 1000/1000 [00:38<00:00, 25.97it/s]\n"
     ]
    }
   ],
   "source": [
    "env = SimulatedEnvironment(num_actions=2, \n",
    "                           state_model_type='nn', \n",
    "                           reward_model_type='nn')\n",
    "env.fit(zs=zs, states=states, actions=actions, rewards=rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823b9cc",
   "metadata": {},
   "source": [
    "## Value Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309b993",
   "metadata": {},
   "source": [
    "We now estimate the value achieved by the trained policy when interacting with the target environment using fitted Q evaluation (FQE), which is provided by `evaluate_value_through_fqe()`. We use a discount factor of $0.9$ by setting `gamma=0.9`. We use the testing set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0d38237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "  0%|          | 1/200 [00:00<01:21,  2.45it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "  2%|▏         | 4/200 [00:02<01:52,  1.74it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "  3%|▎         | 6/200 [00:03<01:48,  1.79it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      " 62%|██████▏   | 124/200 [01:11<00:43,  1.74it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      " 66%|██████▌   | 131/200 [01:15<00:38,  1.80it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      " 72%|███████▎  | 145/200 [01:23<00:32,  1.68it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      " 79%|███████▉  | 158/200 [01:34<00:43,  1.03s/it]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 200/200 [02:06<00:00,  1.58it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQE training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.446245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = evaluate_reward_through_fqe(zs=zs_test, \n",
    "                                    states=states_test, \n",
    "                                    actions=actions_test, \n",
    "                                    rewards=rewards_test, \n",
    "                                    policy=agent, \n",
    "                                    model_type='nn', \n",
    "                                    gamma=0.9)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2a9ff",
   "metadata": {},
   "source": [
    "## Counterfactual Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57928dd",
   "metadata": {},
   "source": [
    "We now estimate the counterfactual fairness acheived by the policy when interacting with the target environment. To do so, we use `evaluate_fairness_through_model()`. This function first estimates the counterfactual trajectories of each individual in the data under a set of legit sensitive attribute values. Then it takes actions based on the counterfactual states using the policy that is to be evaluated. In the end, it calculates and returns a counterfactual fairness metric (CF metric) following the formula \n",
    "\n",
    "$\\max_{z', z \\in eval(Z)} \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\mathbb{I} \\left( A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right) \\neq A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right) \\right),$\n",
    "\n",
    "where $eval(Z)$ is the set of sensitive attribute values passed in by `z_eval_levels`, $A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken in the counterfactual trajectory under $Z=z'$, and $A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken under the counterfactual trajectory under $Z=z$. The CF metric is bounded between 0 and 1, with 0 representing perfect fairness and 1 indicating complete unfairness. We again use the testing set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c68971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029999999999999995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric = evaluate_fairness_through_model(env=env, \n",
    "                                            zs=zs_test, \n",
    "                                            states=states_test, \n",
    "                                            actions=actions_test, \n",
    "                                            policy=agent)\n",
    "cf_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac24d2b",
   "metadata": {},
   "source": [
    "We can see that our policy achieves a low CF metric value, which indicates it is close to being perfectly counterfactually fair. Indeed, the CF metric should be exactly 0 if we know the true underlying environment; the reason why it is not exactly 0 here is because we need to estimate the true underlying environment during preprocessing, which can introduce errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d221c98",
   "metadata": {},
   "source": [
    "## Comparisons: Assessing the Performance of Baseline Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb45ba0",
   "metadata": {},
   "source": [
    "We can follow a similar approach to evaluate the performance of a few baselines: random, fairness-through-unawareness, and full. In this section, we briefly discuss the implementation and performance of these baselines. We will use custom preprocessors and agents to implement these baselines, so we first import the `Preprocessor` and `Agent` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1533a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfrl.preprocessor import Preprocessor\n",
    "from cfrl.agents import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6859e",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e1756",
   "metadata": {},
   "source": [
    "As its name suggests, a random baseline is a policy that selects actions randomly. For this baseline, we implement a custom agent called `RandomAgent` that selects actions at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29c3d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, num_action_levels: int):\n",
    "        self.num_action_levels = num_action_levels\n",
    "        self.__name__ = 'RandomAgent'\n",
    "\n",
    "    def act(self, \n",
    "            z: list | np.ndarray, \n",
    "            xt: list | np.ndarray, \n",
    "            xtm1: list | np.ndarray | None = None, \n",
    "            atm1: list | np.ndarray | None = None, \n",
    "            uat: list | np.ndarray | None = None, \n",
    "            **kwargs) -> np.ndarray:\n",
    "        if uat is None:\n",
    "            N = z.shape[0]\n",
    "            out = np.zeros(N)\n",
    "            for i in range(N):\n",
    "                out[i] = np.random.randint(self.num_action_levels)\n",
    "            return out\n",
    "        else:\n",
    "            action = (uat.flatten() <= 0.5).astype(int)\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c9c70",
   "metadata": {},
   "source": [
    "We now initialize an instance of `RandomAgent` and estimate the value of the random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a8bf8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 200/200 [01:26<00:00,  2.32it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQE training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.1994956"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_random = RandomAgent(num_action_levels=2)\n",
    "value_random = evaluate_reward_through_fqe(zs=zs_test, \n",
    "                                           states=states_test, \n",
    "                                           actions=actions_test, \n",
    "                                           rewards=rewards_test, \n",
    "                                           policy=agent_random, \n",
    "                                           model_type='nn', \n",
    "                                           gamma=0.9)\n",
    "value_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7241c",
   "metadata": {},
   "source": [
    "Finally, we estimate the CF metric of the random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3481d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric_random = evaluate_fairness_through_model(env=env, \n",
    "                                                   zs=zs_test, \n",
    "                                                   states=states_test, \n",
    "                                                   actions=actions_test, \n",
    "                                                   policy=agent_random)\n",
    "cf_metric_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1a5f0",
   "metadata": {},
   "source": [
    "The random policy achieved perfect fairness. This is expected because all the counterfactual trajectories for the same individual should share the same randomness, which means the random policy should select the same action in these counterfactual trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141005af",
   "metadata": {},
   "source": [
    "### Fairness-through-unawareness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c20148",
   "metadata": {},
   "source": [
    "Fairness-through-unawareness proposes to ensure fairness by excluding the sensitive attribute from the state variable (and thus from the agent's decision-making). Nevertheless, it has been argued that this method can still be unfair because the agent might learn the bias indirectly from the states and rewards, which are often biased. In this section, we train a policy following fairness-through-unawareness using the same training trajectory data and estimate its value and CF metric.\n",
    "\n",
    "We begin by training a fairness-through-unawareness policy. As shown in the code below, we directly use the raw training trajectory for policy learning without performing preprocessing. This enforces fairness-through-unawareness because `agent_unaware` only uses `states_train`, `actions_train`, and `rewards_train` during training (i.e. the sensitive attribute is not used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "883047f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:55<00:00,  1.80it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQI training\n"
     ]
    }
   ],
   "source": [
    "agent_unaware = FQI(num_actions=2, model_type='nn', preprocessor=None)\n",
    "agent_unaware.train(zs=zs_train, \n",
    "                    xs=states_train, \n",
    "                    actions=actions_train, \n",
    "                    rewards=rewards_train, \n",
    "                    max_iter=100, \n",
    "                    preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614bff4a",
   "metadata": {},
   "source": [
    "We now estimate the value of the fairness-through-unawareness policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ef3b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [00:06<02:15,  1.40it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 200/200 [01:30<00:00,  2.22it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQE training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.948082"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_unaware = evaluate_reward_through_fqe(zs=zs_test, \n",
    "                                            states=states_test, \n",
    "                                            actions=actions_test, \n",
    "                                            rewards=rewards_test, \n",
    "                                            policy=agent_unaware, \n",
    "                                            model_type='nn', \n",
    "                                            gamma=0.9)\n",
    "value_unaware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5461ff98",
   "metadata": {},
   "source": [
    "Finally, we estimate the CF metric of the fairness-through-unawareness policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62b43fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric_unaware = evaluate_fairness_through_model(env=env, \n",
    "                                                    zs=zs_test, \n",
    "                                                    states=states_test, \n",
    "                                                    actions=actions_test, \n",
    "                                                    policy=agent_unaware)\n",
    "cf_metric_unaware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98dd02",
   "metadata": {},
   "source": [
    "We can see that the fairness-through-unawareness policy is much less fair than the policy learned using the preprocessed trajectory. This suggests that the preprocessing method likely reduced the bias in the training trajectory effectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc1ad2",
   "metadata": {},
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ccc768",
   "metadata": {},
   "source": [
    "The full baseline directly uses the sensitive attribute as part of the state variable for policy learning. It should achieve higher value than the other baselines, but the fairness can be compromised in return. Note that the state variable in our original trajectory does not contain the sensitive attribute. Therefore, to enforce the full baseline, we should concatenate the sensitive attribute to the state variable before policy learning or decision-making. This can be done using the following custom preprocessor, which we call `ConcatenatePreprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1148b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatePreprocessor(Preprocessor):\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "\n",
    "        def preprocess(\n",
    "                self, \n",
    "                z: list | np.ndarray, \n",
    "                xt: list | np.ndarray\n",
    "            ) -> tuple[np.ndarray]:\n",
    "            if xt.ndim == 1:\n",
    "                xt = xt[np.newaxis, :]\n",
    "                z = z[np.newaxis, :]\n",
    "                xt_new = np.concatenate([xt, z], axis=1)\n",
    "                return xt_new.flatten()\n",
    "            elif xt.ndim == 2:\n",
    "                xt_new = np.concatenate([xt, z], axis=1)\n",
    "                return xt_new\n",
    "            \n",
    "        def preprocess_single_step(\n",
    "                self, \n",
    "                z: list | np.ndarray, \n",
    "                xt: list | np.ndarray, \n",
    "                xtm1: list | np.ndarray | None = None, \n",
    "                atm1: list | np.ndarray | None = None, \n",
    "                rtm1: list | np.ndarray | None = None, \n",
    "                verbose: bool = False\n",
    "            ) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "            z = np.array(z)\n",
    "            xt = np.array(xt)\n",
    "            if verbose:\n",
    "                print(\"Preprocessing a single step...\")\n",
    "\n",
    "            xt_new = self.preprocess(z, xt)\n",
    "            if rtm1 is None:\n",
    "                return xt_new\n",
    "            else:\n",
    "                return xt_new, rtm1\n",
    "            \n",
    "\n",
    "        def preprocess_multiple_steps(\n",
    "                self, \n",
    "                zs: list | np.ndarray, \n",
    "                xs: list | np.ndarray, \n",
    "                actions: list | np.ndarray, \n",
    "                rewards: list | np.ndarray | None = None, \n",
    "                verbose: bool = False\n",
    "            ) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "            zs = np.array(zs)\n",
    "            xs = np.array(xs)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            if verbose:\n",
    "                print(\"Preprocessing multiple steps...\")\n",
    "        \n",
    "            # some convenience variables\n",
    "            N, T, xdim = xs.shape\n",
    "            \n",
    "            # define the returned arrays; the arrays will be filled later\n",
    "            xs_tilde = np.zeros([N, T, xdim + zs.shape[-1]])\n",
    "            rs_tilde = np.zeros([N, T - 1])\n",
    "\n",
    "            # preprocess the initial step\n",
    "            np.random.seed(0)\n",
    "            xs_tilde[:, 0, :] = self.preprocess_single_step(zs, xs[:, 0, :])\n",
    "\n",
    "            # preprocess subsequent steps\n",
    "            if rewards is not None:\n",
    "                for t in range (1, T):\n",
    "                    np.random.seed(t)\n",
    "                    xs_tilde[:, t, :], rs_tilde[:, t-1] = self.preprocess_single_step(zs, \n",
    "                                                                                    xs[:, t, :], \n",
    "                                                                                    xs[:, t-1, :], \n",
    "                                                                                    actions[:, t-1], \n",
    "                                                                                    rewards[:, t-1]\n",
    "                                                                                    )\n",
    "                return xs_tilde, rs_tilde                \n",
    "            else:\n",
    "                for t in range (1, T):\n",
    "                    np.random.seed(t)\n",
    "                    xs_tilde[:, t, :] = self.preprocess_single_step(zs, \n",
    "                                                                    xs[:, t, :], \n",
    "                                                                    xs[:, t-1, :], \n",
    "                                                                    actions[:, t-1]\n",
    "                                                                    )\n",
    "                return xs_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942f05b",
   "metadata": {},
   "source": [
    "We perform policy learning using an FQI agent with `ConcatenatePreprocessor` as its internal preprocessor. In this case, we can directly pass the raw trajectories to the FQI agent, and the internal `ConcatenatePreprocessor` will concatenate the sensitive attribute to the state variable before policy learning or decision-making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cad05a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQI training\n"
     ]
    }
   ],
   "source": [
    "cp = ConcatenatePreprocessor()\n",
    "agent_full = FQI(num_actions=2, model_type='nn', preprocessor=cp)\n",
    "agent_full.train(zs=zs_train, \n",
    "                 xs=states_train, \n",
    "                 actions=actions_train, \n",
    "                 rewards=rewards_train, \n",
    "                 max_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f463b99",
   "metadata": {},
   "source": [
    "We now estimate the value of the full policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c16ab1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:02<02:55,  1.12it/s]\n",
      "The fluctuation in the loss is not small enough in at least one of the final 10 epochs during neural network training\n",
      "100%|██████████| 200/200 [02:57<00:00,  1.13it/s]\n",
      "\n",
      "The fluctuation in the Q values is not small enough in at least one of the final 5 iterations during FQE training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.889597"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_full = evaluate_reward_through_fqe(zs=zs_test, \n",
    "                                         states=states_test, \n",
    "                                         actions=actions_test, \n",
    "                                         rewards=rewards_test, \n",
    "                                         policy=agent_full, \n",
    "                                         model_type='nn', \n",
    "                                         gamma=0.9)\n",
    "value_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1708dc2",
   "metadata": {},
   "source": [
    "Finally, we estimate the CF metric of the full policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67ce7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46181818181818185"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric_full = evaluate_fairness_through_model(env=env, \n",
    "                                                 zs=zs_test, \n",
    "                                                 states=states_test, \n",
    "                                                 actions=actions_test, \n",
    "                                                 policy=agent_full)\n",
    "cf_metric_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65059678",
   "metadata": {},
   "source": [
    "The full policy is also much less fair than the policy learned using the preprocessed trajectory. This again suggests that the preprocessing method likely reduced the bias in the training trajectory effectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
