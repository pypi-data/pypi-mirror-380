{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d541a30b",
   "metadata": {},
   "source": [
    "# Assessing Preprocessors Using Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e429",
   "metadata": {},
   "source": [
    "In this workflow, CFRL first uses `sample_trajectory()` to sample a trajectory from a \n",
    "`SyntheticEnvironment` whose transition rules are pre-specified. It then preprocesses the \n",
    "sampled trajectory using some custom preprocessor defined by the user. \n",
    "After that, the preprocessed trajectory is passed into `FQI` to train a policy, which is then \n",
    "assessed using synthetic data via `evaluate_reward_through_simulation()` and \n",
    "`evaluate_fairness_through_simulation()`. The final output of the workflow is the policy trained \n",
    "on the preprocessed data as well as its estimated value and counterfactual fairness metric. This \n",
    "workflow is appropriate when the user wants to examine the impact of some trajectory preprocessing \n",
    "method on the value and counterfactual fairness of the trained policy.\n",
    "\n",
    "We begin by importing the libraries needed for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce542a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this temporarily to import CFRL before it is officially published to PyPI\n",
    "import sys\n",
    "sys.path.append(\"E:/learning/university/MiSIL/CFRL Python Package/CFRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65336cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1da84ffe650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from cfrl.preprocessor import Preprocessor\n",
    "from cfrl.agents import FQI\n",
    "from cfrl.environment import SyntheticEnvironment, sample_trajectory\n",
    "from cfrl.evaluation import evaluate_reward_through_simulation\n",
    "from cfrl.evaluation import evaluate_fairness_through_simulation\n",
    "from examples.baseline_agents import RandomAgent\n",
    "np.random.seed(10) # ensure reproducibility\n",
    "torch.manual_seed(10) # ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63df0",
   "metadata": {},
   "source": [
    "## Demonstration Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7135f",
   "metadata": {},
   "source": [
    "Suppose we have a preprocessor `ConcatenatePreprocessor`, which is defined in the code block below. It essentially adds the senstive attribute to the state variable, which means the policy will directly take the senstive attribute into account for decision-making. We want to assess how this preprocessing method performs in terms of the value and counterfactual fairness of the resulting policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatePreprocessor(Preprocessor):\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "\n",
    "        def preprocess(\n",
    "                self, \n",
    "                z: list | np.ndarray, \n",
    "                xt: list | np.ndarray\n",
    "            ) -> tuple[np.ndarray]:\n",
    "            if xt.ndim == 1:\n",
    "                xt = xt[np.newaxis, :]\n",
    "                z = z[np.newaxis, :]\n",
    "                xt_new = np.concatenate([xt, z], axis=1)\n",
    "                return xt_new.flatten()\n",
    "            elif xt.ndim == 2:\n",
    "                xt_new = np.concatenate([xt, z], axis=1)\n",
    "                return xt_new\n",
    "            \n",
    "        def preprocess_single_step(\n",
    "                self, \n",
    "                z: list | np.ndarray, \n",
    "                xt: list | np.ndarray, \n",
    "                xtm1: list | np.ndarray | None = None, \n",
    "                atm1: list | np.ndarray | None = None, \n",
    "                rtm1: list | np.ndarray | None = None, \n",
    "                verbose: bool = False\n",
    "            ) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "            z = np.array(z)\n",
    "            xt = np.array(xt)\n",
    "            if verbose:\n",
    "                print(\"Preprocessing a single step...\")\n",
    "\n",
    "            xt_new = self.preprocess(z, xt)\n",
    "            if rtm1 is None:\n",
    "                return xt_new\n",
    "            else:\n",
    "                return xt_new, rtm1\n",
    "            \n",
    "\n",
    "        def preprocess_multiple_steps(\n",
    "                self, \n",
    "                zs: list | np.ndarray, \n",
    "                xs: list | np.ndarray, \n",
    "                actions: list | np.ndarray, \n",
    "                rewards: list | np.ndarray | None = None, \n",
    "                verbose: bool = False\n",
    "            ) -> tuple[np.ndarray, np.ndarray] | np.ndarray:\n",
    "            zs = np.array(zs)\n",
    "            xs = np.array(xs)\n",
    "            actions = np.array(actions)\n",
    "            rewards = np.array(rewards)\n",
    "            if verbose:\n",
    "                print(\"Preprocessing multiple steps...\")\n",
    "        \n",
    "            # some convenience variables\n",
    "            N, T, xdim = xs.shape\n",
    "            \n",
    "            # define the returned arrays; the arrays will be filled later\n",
    "            xs_tilde = np.zeros([N, T, xdim + zs.shape[-1]])\n",
    "            rs_tilde = np.zeros([N, T - 1])\n",
    "\n",
    "            # preprocess the initial step\n",
    "            np.random.seed(0)\n",
    "            xs_tilde[:, 0, :] = self.preprocess_single_step(zs, xs[:, 0, :])\n",
    "\n",
    "            # preprocess subsequent steps\n",
    "            if rewards is not None:\n",
    "                for t in range (1, T):\n",
    "                    np.random.seed(t)\n",
    "                    xs_tilde[:, t, :], rs_tilde[:, t-1] = self.preprocess_single_step(zs, \n",
    "                                                                                    xs[:, t, :], \n",
    "                                                                                    xs[:, t-1, :], \n",
    "                                                                                    actions[:, t-1], \n",
    "                                                                                    rewards[:, t-1]\n",
    "                                                                                    )\n",
    "                return xs_tilde, rs_tilde                \n",
    "            else:\n",
    "                for t in range (1, T):\n",
    "                    np.random.seed(t)\n",
    "                    xs_tilde[:, t, :] = self.preprocess_single_step(zs, \n",
    "                                                                    xs[:, t, :], \n",
    "                                                                    xs[:, t-1, :], \n",
    "                                                                    actions[:, t-1]\n",
    "                                                                    )\n",
    "                return xs_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98ac0c",
   "metadata": {},
   "source": [
    "Meanwhile, we also define the environment in which we want to assess `ConcatenatePreprocessor`. We define the transition rules of the environment as follows, which has univariate sensitive attributes and univariate states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50561f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x0(\n",
    "        zs: list | np.ndarray, \n",
    "        ux0: list | np.ndarray, \n",
    "        z_coef: int | float = 1\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    ux0 = np.array(ux0)\n",
    "    gamma0 = np.array([-0.3, 1 * z_coef, 1])\n",
    "    n = zs.shape[0]\n",
    "    M = np.concatenate(\n",
    "        [\n",
    "            np.ones([n, 1]),\n",
    "            zs,\n",
    "            ux0,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    x0 = M @ gamma0\n",
    "    x0 = x0.reshape(-1, 1)\n",
    "    return x0\n",
    "\n",
    "def f_xt(\n",
    "        zs: list | np.ndarray, \n",
    "        xtm1: list | np.ndarray, \n",
    "        atm1: list | np.ndarray, \n",
    "        uxt: list | np.ndarray, \n",
    "        z_coef: int | float = 1\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    xtm1 = np.array(xtm1)\n",
    "    atm1 = np.array(atm1)\n",
    "    uxt = np.array(uxt)\n",
    "    gamma = np.array([-0.3, 1 * z_coef, 0.5, 0.4, 0.3, 0.3 * z_coef, 0.4 * z_coef, 1]) #-0.3\n",
    "    n = xtm1.shape[0]\n",
    "    M = np.concatenate(\n",
    "        [\n",
    "            np.ones([n, 1]),\n",
    "            (zs - 0.5),\n",
    "            xtm1,\n",
    "            atm1.reshape(-1, 1) - 0.5,\n",
    "            xtm1 * (atm1.reshape(-1, 1) - 0.5),\n",
    "            xtm1 * (zs - 0.5),\n",
    "            (zs - 0.5) * (atm1.reshape(-1, 1) - 0.5),\n",
    "            uxt,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    xt = M @ gamma\n",
    "    xt = xt.reshape(-1, 1)\n",
    "    return xt\n",
    "\n",
    "def f_rt(\n",
    "        zs: list | np.ndarray, \n",
    "        xt: list | np.ndarray, \n",
    "        at: list | np.ndarray, \n",
    "        urtm1: list | np.ndarray, \n",
    "        z_coef: int | float = 1\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "    zs = np.array(zs)\n",
    "    xt = np.array(xt)\n",
    "    at = np.array(at)\n",
    "    urtm1 = np.array(urtm1)\n",
    "    lmbda = np.array([-0.3, 0.3, 0.5 * z_coef, 0.5, 0.2 * z_coef, 0.7, -1.0 * z_coef])\n",
    "    n = xt.shape[0]\n",
    "    at = at.reshape(-1, 1)\n",
    "    M = np.concatenate(\n",
    "        [np.ones([n, 1]), xt, zs, at, xt * zs, xt * at, zs * at], axis=1\n",
    "    )\n",
    "    rt = M @ lmbda\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6865a",
   "metadata": {},
   "source": [
    "## Training Trajectory Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6153124",
   "metadata": {},
   "source": [
    "We now generate a trajectory with 300 inviduals (i.e. $N=300$) and 10 transitions used for training the `FQI` agent. Note that we do not train the preprocessor here because `ConcatenatePreprocessor` does not require training.\n",
    "\n",
    "We first initialize a `SyntheticEnvironment` using the transition rules defined in the previous section. This `SyntheticEnvironment` will be used to generate trajectories throughout this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0b2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SyntheticEnvironment(state_dim=1, \n",
    "                           z_coef=1, \n",
    "                           f_x0=f_x0, \n",
    "                           f_xt=f_xt, \n",
    "                           f_rt=f_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1ebe8",
   "metadata": {},
   "source": [
    "Before generating the trajectory, we need to generate the sensitive attributes of the 300 individuals in the trajectory. We allow the senstive attributes to take on two different values: $0$ and $1$. Each individual's sensitive attribute value is sampled randomly from a uniform distribution over all the legit sensitive attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_in = np.random.binomial(n=1, p=0.5, size=300).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1a2c5",
   "metadata": {},
   "source": [
    "We now generate the trajectory using the `sample_trajectory()` function. The actions in the trajectory are taken using a `RandomAgent` that selects $0$ and $1$ randomly with equal probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764a065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_agent = RandomAgent(2)\n",
    "zs, states, actions, rewards = sample_trajectory(env=env, \n",
    "                                                 zs=zs_in, \n",
    "                                                 state_dim=1, \n",
    "                                                 T=10, \n",
    "                                                 policy=behavior_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a385e2",
   "metadata": {},
   "source": [
    "We check the shapes of the Trajectory Arrays generated by `sample_trajectory()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee7ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 1), (300, 11, 1), (300, 10), (300, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs.shape, states.shape, actions.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdf2d5",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfaa60",
   "metadata": {},
   "source": [
    "We now learn a policy using `FQI` and `ConcatenatePreprocessor`. We first initialize an `FQI` agent that uses `ConcatenatePreprocessor` as its internal preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ed4f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ConcatenatePreprocessor()\n",
    "agent = FQI(num_actions=2, model_type='nn', preprocessor=cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3af5f",
   "metadata": {},
   "source": [
    "We now perform training. Since we set `preprocess=True` in `train()`, `agent` will use its internal preprocessor (i.e. `cp`) to automatically preprocess the input training trajectory before using the trajectory for policy learning. Therefore, we can directly pass in the unpreprocessed states and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef15eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:52<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "agent.train(zs=zs, \n",
    "            xs=states, \n",
    "            actions=actions, \n",
    "            rewards=rewards, \n",
    "            max_iter=100, \n",
    "            preprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2722f2a",
   "metadata": {},
   "source": [
    "## Value Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951ec0f",
   "metadata": {},
   "source": [
    "We now estimate the value achieved by the trained policy when interacting with the target environment (i.e. `env`). Since the underlying transition rules are known, we can directly use `evaluate_rewards_through_simulation()`. This function generates a new trajectory using `agent` under `env` and computes the discounted cumulative rewards collected in the trajectory.\n",
    "\n",
    "We evaluate the discounted cumulative rewards using a simulation with 100 individuals (i.e. $N=100$) and 500 transitions ($T=500$). We use a discount factor of $0.9$ by setting `gamma=0.9`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fec81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.272778066358667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = evaluate_reward_through_simulation(env=env, \n",
    "                                           z_eval_levels=[[0], [1]], \n",
    "                                           state_dim=1, \n",
    "                                           N=100, \n",
    "                                           T=500, \n",
    "                                           policy=agent, \n",
    "                                           gamma=0.9)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1920078",
   "metadata": {},
   "source": [
    "## Counterfactual Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fc023",
   "metadata": {},
   "source": [
    "We now estimate the counterfactual fairness acheived by the policy when interacting with the target environment (i.e. `env`). To do so, we use `evaluate_fairness_through_simulation()`. This function first generates the counterfactual trajectories of each individual in the data under a set legit sensitive attribute values using the policy that is to be evaluated. It then  calculates and returns a counterfactual fairness metric (CF metric) following the formula \n",
    "\n",
    "$\\max_{z', z \\in eval(Z)} \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\mathbb{I} \\left( A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right) \\neq A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right) \\right),$\n",
    "\n",
    "where $eval(Z)$ is the set of sensitive attribute values passed in by `z_eval_levels`, $A_t^{Z \\leftarrow z'}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken in the counterfactual trajectory under $Z=z'$, and $A_t^{Z \\leftarrow z}\\left(\\bar{U}_t(h_{it})\\right)$ is the action taken under the counterfactual trajectory under $Z=z$. The CF metric is bounded between 0 and 1, with 0 representing perfect fairness and 1 indicating complete unfairness.\n",
    "\n",
    "We evaluate the counterfactual fairness using a simulation with 100 individuals (i.e. $N=100$) and 10 transitions ($T=10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3282ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_metric = evaluate_fairness_through_simulation(env=env, \n",
    "                                                 z_eval_levels=[[0], [1]], \n",
    "                                                 state_dim=1, \n",
    "                                                 N=100, \n",
    "                                                 T=10, \n",
    "                                                 policy=agent)\n",
    "cf_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
