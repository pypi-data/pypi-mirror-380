    Linear solve converged due to CONVERGED_RTOL iterations 5
SNES Object: 4 MPI processes
  type: ksponly
  maximum iterations=1, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=5
  total number of function evaluations=1
  norm schedule ALWAYS
  Jacobian is never rebuilt
  KSP Object: 4 MPI processes
    type: cg
    maximum iterations=100, initial guess is zero
    tolerances: relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using UNPRECONDITIONED norm type for convergence test
  PC Object: 4 MPI processes
    type: gamg
      type is MULTIPLICATIVE, levels=2 cycles=v
        Cycles per PCApply=1
        Using externally compute Galerkin coarse grid matrices
        GAMG specific options
          Threshold for dropping small values in graph on each level =     0.001     0.001    
          Threshold scaling factor for each level not specified = 1.
          Using aggregates made with 3 applications of heavy edge matching (HEM) to define subdomains for PCASM
            MatCoarsen Object: 4 MPI processes
              type: hem
              3 matching steps with threshold = 0.
            AGG specific options
              Number of levels of aggressive coarsening 1
              Square graph aggressive coarsening
              MatCoarsen Object: (pc_gamg_) 4 MPI processes
                type: mis
              Number smoothing steps to construct prolongation 1
            Complexity:    grid = 1.11111    operator = 1.02041
            Per-level complexity: op = operator, int = interpolation
                #equations  | #active PEs | avg nnz/row op | avg nnz/row int
                      1            1              1                0
                      9            4              6                1
      Coarse grid solver -- level 0 -------------------------------
        KSP Object: (mg_coarse_) 4 MPI processes
          type: preonly
          maximum iterations=10000, initial guess is zero
          tolerances: relative=1e-05, absolute=1e-50, divergence=10000.
          left preconditioning
          using NONE norm type for convergence test
        PC Object: (mg_coarse_) 4 MPI processes
          type: bjacobi
            number of blocks = 4
            Local solver information for first block is in the following KSP and PC objects on rank 0:
            Use -mg_coarse_ksp_view ::ascii_info_detail to display information for all blocks
            KSP Object: (mg_coarse_sub_) 1 MPI process
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances: relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (mg_coarse_sub_) 1 MPI process
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 1., needed 1.
                  Factored matrix follows:
                    Mat Object: (mg_coarse_sub_) 1 MPI process
                      type: seqaij
                      rows=1, cols=1
                      package used to perform factorization: petsc
                      total: nonzeros=1, allocated nonzeros=1
                        not using I-node routines
              linear system matrix = precond matrix:
              Mat Object: (mg_coarse_sub_) 1 MPI process
                type: seqaij
                rows=1, cols=1
                total: nonzeros=1, allocated nonzeros=1
                total number of mallocs used during MatSetValues calls=0
                  not using I-node routines
          linear system matrix = precond matrix:
          Mat Object: 4 MPI processes
            type: mpiaij
            rows=1, cols=1
            total: nonzeros=1, allocated nonzeros=1
            total number of mallocs used during MatSetValues calls=0
              using nonscalable MatPtAP() implementation
              not using I-node (on process 0) routines
      Down solver (pre-smoother) on level 1 -------------------------------
        KSP Object: (mg_levels_1_) 4 MPI processes
          type: chebyshev
            Chebyshev polynomial of first kind
            eigenvalue targets used: min 0.260428, max 1.43236
            eigenvalues provided (min 0.413297, max 1.30214) with transform: [0. 0.2; 0. 1.1]
          maximum iterations=2, nonzero initial guess
          tolerances: relative=1e-05, absolute=1e-50, divergence=10000.
          left preconditioning
          using NONE norm type for convergence test
        PC Object: (mg_levels_1_) 4 MPI processes
          type: jacobi
            type DIAGONAL
          linear system matrix = precond matrix:
          Mat Object: 4 MPI processes
            type: mpiaij
            rows=9, cols=9
            total: nonzeros=49, allocated nonzeros=49
            total number of mallocs used during MatSetValues calls=0
              not using I-node (on process 0) routines
      Up solver (post-smoother) same as down solver (pre-smoother)
      linear system matrix = precond matrix:
      Mat Object: 4 MPI processes
        type: mpiaij
        rows=9, cols=9
        total: nonzeros=49, allocated nonzeros=49
        total number of mallocs used during MatSetValues calls=0
          not using I-node (on process 0) routines
  DM Object: box 4 MPI processes
    type: plex
  box in 3 dimensions:
    Number of 0-cells per rank:   8   8   8   8  
    Number of 1-cells per rank:   12   12   12   12  
    Number of 2-cells per rank:   6   6   6   6  
    Number of 3-cells per rank:   1   1   1   1  
  Labels:
    depth: 4 strata with value/size (0 (8), 1 (12), 2 (6), 3 (1))
    marker: 1 strata with value/size (1 (23))
    Face Sets: 4 strata with value/size (1 (1), 2 (1), 3 (1), 6 (1))
    celltype: 4 strata with value/size (0 (8), 1 (12), 4 (6), 7 (1))
    boundary: 1 strata with value/size (1 (23))
  Field deformation:
    adjacency FEM
  DM Object: Mesh 4 MPI processes
    type: plex
  Mesh in 3 dimensions:
    Number of 0-cells per rank:   27   27   27   27  
    Number of 1-cells per rank:   54   54   54   54  
    Number of 2-cells per rank:   36   36   36   36  
    Number of 3-cells per rank:   8   8   8   8  
  Labels:
    celltype: 4 strata with value/size (0 (27), 1 (54), 4 (36), 7 (8))
    depth: 4 strata with value/size (0 (27), 1 (54), 2 (36), 3 (8))
    marker: 1 strata with value/size (1 (77))
    Face Sets: 4 strata with value/size (1 (9), 2 (9), 3 (9), 6 (9))
    boundary: 1 strata with value/size (1 (77))
  Field deformation:
    adjacency FEM
      Linear solve converged due to CONVERGED_RTOL iterations 8
  SNES Object: 4 MPI processes
    type: ksponly
    maximum iterations=1, maximum function evaluations=10000
    tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
    total number of linear solver iterations=8
    total number of function evaluations=1
    norm schedule ALWAYS
    Jacobian is never rebuilt
    KSP Object: 4 MPI processes
      type: cg
      maximum iterations=100, initial guess is zero
      tolerances: relative=1e-10, absolute=1e-50, divergence=10000.
      left preconditioning
      using UNPRECONDITIONED norm type for convergence test
    PC Object: 4 MPI processes
      type: gamg
        type is MULTIPLICATIVE, levels=2 cycles=v
          Cycles per PCApply=1
          Using externally compute Galerkin coarse grid matrices
          GAMG specific options
            Threshold for dropping small values in graph on each level =       0.001       0.001      
            Threshold scaling factor for each level not specified = 1.
            Using aggregates made with 3 applications of heavy edge matching (HEM) to define subdomains for PCASM
              MatCoarsen Object: 4 MPI processes
                type: hem
                3 matching steps with threshold = 0.
              AGG specific options
                Number of levels of aggressive coarsening 1
                Square graph aggressive coarsening
                MatCoarsen Object: (pc_gamg_) 4 MPI processes
                  type: mis
                Number smoothing steps to construct prolongation 1
              Complexity:    grid = 1.02721    operator = 1.00432
              Per-level complexity: op = operator, int = interpolation
                  #equations  | #active PEs | avg nnz/row op | avg nnz/row int
                        4            1              4                0
                      147            4             26                3
        Coarse grid solver -- level 0 -------------------------------
          KSP Object: (mg_coarse_) 4 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances: relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (mg_coarse_) 4 MPI processes
            type: bjacobi
              number of blocks = 4
              Local solver information for first block is in the following KSP and PC objects on rank 0:
              Use -mg_coarse_ksp_view ::ascii_info_detail to display information for all blocks
              KSP Object: (mg_coarse_sub_) 1 MPI process
                type: preonly
                maximum iterations=1, initial guess is zero
                tolerances: relative=1e-05, absolute=1e-50, divergence=10000.
                left preconditioning
                using NONE norm type for convergence test
              PC Object: (mg_coarse_sub_) 1 MPI process
                type: lu
                  out-of-place factorization
                  tolerance for zero pivot 2.22045e-14
                  using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                  matrix ordering: nd
                  factor fill ratio given 5., needed 1.
                    Factored matrix follows:
                      Mat Object: (mg_coarse_sub_) 1 MPI process
                        type: seqaij
                        rows=4, cols=4
                        package used to perform factorization: petsc
                        total: nonzeros=16, allocated nonzeros=16
                          using I-node routines: found 1 nodes, limit used is 5
                linear system matrix = precond matrix:
                Mat Object: (mg_coarse_sub_) 1 MPI process
                  type: seqaij
                  rows=4, cols=4
                  total: nonzeros=16, allocated nonzeros=16
                  total number of mallocs used during MatSetValues calls=0
                    using I-node routines: found 1 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 4 MPI processes
              type: mpiaij
              rows=4, cols=4
              total: nonzeros=16, allocated nonzeros=16
              total number of mallocs used during MatSetValues calls=0
                using nonscalable MatPtAP() implementation
                using I-node (on process 0) routines: found 1 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (mg_levels_1_) 4 MPI processes
            type: chebyshev
              Chebyshev polynomial of first kind
              eigenvalue targets used: min 0.327489, max 1.80119
              eigenvalues provided (min 0.133814, max 1.63744) with transform: [0. 0.2; 0. 1.1]
            maximum iterations=2, nonzero initial guess
            tolerances: relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (mg_levels_1_) 4 MPI processes
            type: jacobi
              type DIAGONAL
            linear system matrix = precond matrix:
            Mat Object: 4 MPI processes
              type: mpiaij
              rows=147, cols=147
              total: nonzeros=3703, allocated nonzeros=3703
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 4 MPI processes
          type: mpiaij
          rows=147, cols=147
          total: nonzeros=3703, allocated nonzeros=3703
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
[0] 0) N=           9, max displ=2.5713786e+01, error=9.564e+00
[0] 1) N=         147, max displ=3.1758769e+01, disp diff= 6.04e+00, error=3.519e+00, rate=1.4
