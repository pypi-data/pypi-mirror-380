from typing import Any

from amberflow.primitives import dirpath_t
from amberflow.artifacts import BaseArtifact, ArtifactContainer
# from amberflow.worknodes import BaseSingleWorkNode, worknodehelper

__all__ = ("TemplateWorkNode",)


class TemplateWorkNode(object):
    """
    A template for creating custom worknodes in the Amberflow pipeline.

    This class serves as a blueprint to ensure that user-defined worknodes
    adhere to the required structure and interface of the Amberflow system.
    Developers should inherit from this class and override its methods to
    implement custom logic for their workflows.

    The primary purpose of a worknode is to perform a specific, isolated task
    such as data processing, simulation, or analysis. It receives artifacts
    from upstream nodes, operates on them, and produces new artifacts that
    can be used by downstream nodes.

    To create a custom worknode:
    1.  Inherit from `BaseSingleWorkNode` or `BaseBatchWorkNode`
    2.  Update the `@worknodehelper` decorator to specify the expected input and output artifact
        types for your node. This decorator is optional, but strongly recommended
    3.  Customize the `__init__` method to accept any parameters your worknode requires.
        Always call `super().__init__` with kwargs forwarded.
    4.  Implement the core logic of your node within the `_run()` method, or `run()` if you don't use the
        @worknodehelper decorator. This is where you will process input artifacts and return output artifacts.
    5.  Optionally, iff your node's execution can be skipped under certain conditions (e.g., if output files
        already exist), implement the logic for this in the `_try_and_skip` and `fill_output_artifacts` methods.
    """

    def __init__(
        self,
        wnid: str,
        *args,
        # add your custom parameters here
        **kwargs,
    ) -> None:
        """
        Initializes the TemplateWorkNode.

        It is crucial to call `super().__init__` to ensure the base worknode is properly initialized
        with its unique ID and other required setup.

        Don't keep attributes inside the class that are associated to a file in the filesystem, if you want your
        worknode to be serializable, that is, to run remotely inside a `Pipeline` that's inside a `Campaign`.
        If you need to set these attributes, then passed them to `@worknodehelper` parameter `empty_attrs`.
        The helper will inject a `__getstate__` method that will delete these attributes before the serialization.
        A common example of objects that should not be kept as attributes are `MDanalysis` `Universe`s.

        Args:
            wnid (str): A unique identifier for the worknode instance.
            *args: Positional arguments passed to the parent class.
            **kwargs: Keyword arguments passed to the parent class. You can
                      add your own custom parameters here.
        """
        super().__init__(
            wnid=wnid,
            *args,
            **kwargs,
        )

    def _run(
        self,
        *,
        cwd: dirpath_t,
        sysname: str,
        **kwargs,
    ) -> Any:
        """
        The core execution method for the worknode. This method is not called directly by the user, nor the scheduler.
        @worknodehelper will wrap it inside a `run()` method.

        Before this method is called, the `@worknodehelper` decorator has already:
            - Filtered input artifacts based on `input_artifact_types`.
            - Populated `self.input_artifacts` with the filtered artifacts.
            - Created and assigned the node's working directory to `self.work_dir`.
            - Set up a dedicated logger, `self.node_logger`, that logs to the work_dir.
            - Set the node's status to RUNNING.
            - Initialized the `self.command` object for command execution, if necessary.
            - Wrapped the call in a try/except block

        After this method completes, the decorator will:
            - Optionally validate the output artifacts against `output_artifact_types`.
            - Set the node's status to COMPLETED or FAILED.
            - Record the execution time.

        Args:
            cwd (dirpath_t): The working directory for this specific execution.
            sysname (str): The name of the system being processed.
            binpath (Optional[filepath_t]): Path to an external binary, if needed.
            **kwargs: Additional keyword arguments passed from the scheduler.

        Returns:
            The output artifacts generated by this node. The decorator expects these to be in `self.output_artifacts`.
        """
        if self.skippable:
            if self._try_and_skip(sysname):
                return self.output_artifacts
        # --- Implement your custom logic here ---
        # 1. Access input artifacts using `self.input_artifacts`.
        # 2. Perform computations, run external tools, or process data.
        # 3. Generate new artifacts. For file-based artifacts, write them out inside `self.work_dir`.
        # 4. Add generated artifacts to `self.output_artifacts`, preferably with the `self.fill_output_artifacts` method

        return self.output_artifacts

    def _try_and_skip(self, sysname: str) -> bool:
        """
        Determines if the worknode execution can be skipped.

        This method is called if the `skippable` attribute is True. It should check for the existence of expected
        output files or other conditions that would make re-running the node unnecessary.
        It's not strictly required to implement this method.

        Args:
            sysname (str): The name of the system being processed.

        Returns:
            True if the node can be skipped, False otherwise.
        """
        if self.skippable:
            try:
                self.output_artifacts = self.fill_output_artifacts(sysname)
                self.node_logger.info(f"Skipped {self.id} WorkNode.")
                return True
            except (FileNotFoundError, ValueError) as e:
                self.node_logger.debug(f"Can't skip {self.id} Got: {e}")
            except NotImplementedError:
                self.node_logger.debug(
                    f"Can't skip {self.id}. {self.__class__.__name__} did not implement `fill_output_artifacts()`"
                )
        return False

    def fill_output_artifacts(
        self,
        sysname: str,
    ) -> ArtifactContainer:
        """
        Populates output artifacts when a node is skipped.

        If `_try_and_skip` determines that a node can be skipped, this method
        is responsible for creating the artifact objects that correspond to
        the pre-existing output files.

        Args:
            sysname (str): The name of the system being processed.

        Returns:
            An `ArtifactContainer` or `BatchArtifacts` object containing the
            output artifacts for the skipped node.
        """
        # return ArtifactContainer(sysname, [self.artifact_builder[BaseTopologyFile](outfile)])
        raise NotImplementedError
