# pylint: disable=line-too-long,useless-suppression,too-many-lines
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from collections.abc import MutableMapping
from io import IOBase
import json
from typing import Any, Callable, IO, Literal, Optional, TypeVar, Union, overload
import urllib.parse

from azure.core import AsyncPipelineClient
from azure.core.async_paging import AsyncItemPaged, AsyncList
from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    StreamClosedError,
    StreamConsumedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.rest import AsyncHttpResponse, HttpRequest
from azure.core.tracing.decorator import distributed_trace
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import models as _models2
from ....openai import models as _openai_models6
from ..._utils.model_base import SdkJSONEncoder, _deserialize, _failsafe_deserialize
from ..._utils.serialization import Deserializer, Serializer
from ..._utils.utils import ClientMixinABC
from ...operations._operations import (
    build_agents_create_agent_version_request,
    build_agents_create_or_update_agent_event_handler_request,
    build_agents_delete_agent_container_request,
    build_agents_delete_agent_event_handler_request,
    build_agents_delete_agent_request,
    build_agents_delete_agent_version_request,
    build_agents_get_agent_container_operation_request,
    build_agents_get_agent_container_request,
    build_agents_get_agent_event_handler_request,
    build_agents_get_agent_request,
    build_agents_get_agent_version_request,
    build_agents_list_agent_container_operations_request,
    build_agents_list_agent_event_handlers_request,
    build_agents_list_agent_version_container_operations_request,
    build_agents_list_agent_versions_request,
    build_agents_list_agents_request,
    build_agents_start_agent_container_request,
    build_agents_stop_agent_container_request,
    build_agents_update_agent_container_request,
    build_conversations_create_conversation_items_request,
    build_conversations_create_conversation_request,
    build_conversations_delete_conversation_item_request,
    build_conversations_delete_conversation_request,
    build_conversations_get_conversation_item_request,
    build_conversations_get_conversation_request,
    build_conversations_list_conversation_items_request,
    build_conversations_list_conversations_request,
    build_conversations_update_conversation_request,
    build_responses_cancel_response_request,
    build_responses_create_response_request,
    build_responses_create_response_stream_request,
    build_responses_delete_response_request,
    build_responses_get_response_request,
    build_responses_get_response_stream_request,
    build_responses_list_input_items_request,
    build_responses_list_responses_request,
)
from .._configuration import AgentsClientConfiguration

JSON = MutableMapping[str, Any]
_Unset: Any = object()
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, dict[str, Any]], Any]]


class ConversationsOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~azure.ai.agents.aio.AgentsClient`'s
        :attr:`conversations` attribute.
    """

    def __init__(self, *args, **kwargs) -> None:
        input_args = list(args)
        self._client: AsyncPipelineClient = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config: AgentsClientConfiguration = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize: Serializer = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize: Deserializer = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @overload
    async def create_conversation(
        self,
        *,
        content_type: str = "application/json",
        metadata: Optional[dict[str, str]] = None,
        items: Optional[list[_openai_models6.ItemParam]] = None,
        **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Create a conversation.

        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword metadata: Set of 16 key-value pairs that can be attached to an object. This can be
         useful for storing additional information about the object in a structured
         format, and querying for objects via API or the dashboard.

         Keys are strings with a maximum length of 64 characters. Values are strings
         with a maximum length of 512 characters. Default value is None.
        :paramtype metadata: dict[str, str]
        :keyword items: Initial items to include the conversation context.
         You may add up to 20 items at a time. Default value is None.
        :paramtype items: list[~openai.models.ItemParam]
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_conversation(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Create a conversation.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_conversation(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Create a conversation.

        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_conversation(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        metadata: Optional[dict[str, str]] = None,
        items: Optional[list[_openai_models6.ItemParam]] = None,
        **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Create a conversation.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword metadata: Set of 16 key-value pairs that can be attached to an object. This can be
         useful for storing additional information about the object in a structured
         format, and querying for objects via API or the dashboard.

         Keys are strings with a maximum length of 64 characters. Values are strings
         with a maximum length of 512 characters. Default value is None.
        :paramtype metadata: dict[str, str]
        :keyword items: Initial items to include the conversation context.
         You may add up to 20 items at a time. Default value is None.
        :paramtype items: list[~openai.models.ItemParam]
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_openai_models6.ConversationResource] = kwargs.pop("cls", None)

        if body is _Unset:
            body = {"items": items, "metadata": metadata}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_conversations_create_conversation_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ConversationResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def update_conversation(
        self,
        conversation_id: str,
        body: _openai_models6.UpdateConversationRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Update a conversation.

        :param conversation_id: The id of the conversation to update. Required.
        :type conversation_id: str
        :param body: Required.
        :type body: ~openai.models.UpdateConversationRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_conversation(
        self, conversation_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Update a conversation.

        :param conversation_id: The id of the conversation to update. Required.
        :type conversation_id: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_conversation(
        self, conversation_id: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Update a conversation.

        :param conversation_id: The id of the conversation to update. Required.
        :type conversation_id: str
        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def update_conversation(
        self,
        conversation_id: str,
        body: Union[_openai_models6.UpdateConversationRequest, JSON, IO[bytes]],
        **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Update a conversation.

        :param conversation_id: The id of the conversation to update. Required.
        :type conversation_id: str
        :param body: Is one of the following types: UpdateConversationRequest, JSON, IO[bytes]
         Required.
        :type body: ~openai.models.UpdateConversationRequest or JSON or IO[bytes]
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_openai_models6.ConversationResource] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_conversations_update_conversation_request(
            conversation_id=conversation_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ConversationResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_conversation(self, conversation_id: str, **kwargs: Any) -> _openai_models6.ConversationResource:
        """Retrieves a conversation.

        :param conversation_id: The id of the conversation to retrieve. Required.
        :type conversation_id: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_openai_models6.ConversationResource] = kwargs.pop("cls", None)

        _request = build_conversations_get_conversation_request(
            conversation_id=conversation_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ConversationResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_conversation(
        self, conversation_id: str, **kwargs: Any
    ) -> _openai_models6.DeletedConversationResource:
        """Deletes a conversation.

        :param conversation_id: The id of the conversation to delete. Required.
        :type conversation_id: str
        :return: DeletedConversationResource. The DeletedConversationResource is compatible with
         MutableMapping
        :rtype: ~openai.models.DeletedConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_openai_models6.DeletedConversationResource] = kwargs.pop("cls", None)

        _request = build_conversations_delete_conversation_request(
            conversation_id=conversation_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.DeletedConversationResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def list_conversations(
        self,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        agent_name: Optional[str] = None,
        agent_id: Optional[str] = None,
        **kwargs: Any
    ) -> AsyncItemPaged["_openai_models6.ConversationResource"]:
        """Returns the list of all conversations.

        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :keyword agent_name: Filter by agent name. If provided, only items associated with the
         specified agent will be returned. Default value is None.
        :paramtype agent_name: str
        :keyword agent_id: Filter by agent ID in the format ``name:version``. If provided, only items
         associated with the specified agent ID will be returned. Default value is None.
        :paramtype agent_id: str
        :return: An iterator like instance of ConversationResource
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~openai.models.ConversationResource]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_openai_models6.ConversationResource]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_conversations_list_conversations_request(
                    limit=limit,
                    order=order,
                    after=after,
                    before=before,
                    agent_name=agent_name,
                    agent_id=agent_id,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_openai_models6.ConversationResource], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)

    @overload
    async def create_conversation_items(
        self,
        conversation_id: str,
        *,
        items: list[_openai_models6.ItemParam],
        include: Optional[list[str]] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentsPagedResultItemResource:
        """Create items in a conversation with the given ID.

        :param conversation_id: The id of the conversation on which the item needs to be created.
         Required.
        :type conversation_id: str
        :keyword items: The items to add to the conversation. You may add up to 20 items at a time.
         Required.
        :paramtype items: list[~openai.models.ItemParam]
        :keyword include: Additional fields to include in the response.
         See the ``include`` parameter for listing Conversation items for more information. Default
         value is None.
        :paramtype include: list[str]
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentsPagedResultItemResource. The AgentsPagedResultItemResource is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentsPagedResultItemResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_conversation_items(
        self,
        conversation_id: str,
        body: JSON,
        *,
        include: Optional[list[str]] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentsPagedResultItemResource:
        """Create items in a conversation with the given ID.

        :param conversation_id: The id of the conversation on which the item needs to be created.
         Required.
        :type conversation_id: str
        :param body: Required.
        :type body: JSON
        :keyword include: Additional fields to include in the response.
         See the ``include`` parameter for listing Conversation items for more information. Default
         value is None.
        :paramtype include: list[str]
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentsPagedResultItemResource. The AgentsPagedResultItemResource is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentsPagedResultItemResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_conversation_items(
        self,
        conversation_id: str,
        body: IO[bytes],
        *,
        include: Optional[list[str]] = None,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentsPagedResultItemResource:
        """Create items in a conversation with the given ID.

        :param conversation_id: The id of the conversation on which the item needs to be created.
         Required.
        :type conversation_id: str
        :param body: Required.
        :type body: IO[bytes]
        :keyword include: Additional fields to include in the response.
         See the ``include`` parameter for listing Conversation items for more information. Default
         value is None.
        :paramtype include: list[str]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentsPagedResultItemResource. The AgentsPagedResultItemResource is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentsPagedResultItemResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_conversation_items(
        self,
        conversation_id: str,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        items: list[_openai_models6.ItemParam] = _Unset,
        include: Optional[list[str]] = None,
        **kwargs: Any
    ) -> _models2.AgentsPagedResultItemResource:
        """Create items in a conversation with the given ID.

        :param conversation_id: The id of the conversation on which the item needs to be created.
         Required.
        :type conversation_id: str
        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword items: The items to add to the conversation. You may add up to 20 items at a time.
         Required.
        :paramtype items: list[~openai.models.ItemParam]
        :keyword include: Additional fields to include in the response.
         See the ``include`` parameter for listing Conversation items for more information. Default
         value is None.
        :paramtype include: list[str]
        :return: AgentsPagedResultItemResource. The AgentsPagedResultItemResource is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentsPagedResultItemResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models2.AgentsPagedResultItemResource] = kwargs.pop("cls", None)

        if body is _Unset:
            if items is _Unset:
                raise TypeError("missing required argument: items")
            body = {"items": items}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_conversations_create_conversation_items_request(
            conversation_id=conversation_id,
            include=include,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentsPagedResultItemResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_conversation_item(
        self, conversation_id: str, item_id: str, **kwargs: Any
    ) -> _openai_models6.ItemResource:
        """Get a single item from a conversation with the given IDs.

        :param conversation_id: The ID of the conversation that contains the item. Required.
        :type conversation_id: str
        :param item_id: The id of the conversation item to retrieve. Required.
        :type item_id: str
        :return: ItemResource. The ItemResource is compatible with MutableMapping
        :rtype: ~openai.models.ItemResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_openai_models6.ItemResource] = kwargs.pop("cls", None)

        _request = build_conversations_get_conversation_item_request(
            conversation_id=conversation_id,
            item_id=item_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ItemResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_conversation_item(
        self, conversation_id: str, item_id: str, **kwargs: Any
    ) -> _openai_models6.ConversationResource:
        """Delete an item from a conversation with the given IDs.

        :param conversation_id: The id of the conversation on which the item needs to be deleted from.
         Required.
        :type conversation_id: str
        :param item_id: The id of the conversation item to delete. Required.
        :type item_id: str
        :return: ConversationResource. The ConversationResource is compatible with MutableMapping
        :rtype: ~openai.models.ConversationResource
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_openai_models6.ConversationResource] = kwargs.pop("cls", None)

        _request = build_conversations_delete_conversation_item_request(
            conversation_id=conversation_id,
            item_id=item_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ConversationResource, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def list_conversation_items(
        self,
        conversation_id: str,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        item_type: Optional[Union[str, _openai_models6.ItemType]] = None,
        **kwargs: Any
    ) -> AsyncItemPaged["_openai_models6.ItemResource"]:
        """List all items for a conversation with the given ID.

        :param conversation_id: The id of the conversation on which the items needs to be listed.
         Required.
        :type conversation_id: str
        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :keyword item_type: Filter by item type. If provided, only items of the specified type will be
         returned. Known values are: "message", "file_search_call", "function_call",
         "function_call_output", "computer_call", "computer_call_output", "web_search_call",
         "reasoning", "item_reference", "image_generation_call", "code_interpreter_call",
         "local_shell_call", "local_shell_call_output", "mcp_list_tools", "mcp_approval_request",
         "mcp_approval_response", "mcp_call", "structured_inputs", "structured_outputs",
         "semantic_event", and "workflow_action". Default value is None.
        :paramtype item_type: str or ~openai.models.ItemType
        :return: An iterator like instance of ItemResource
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~openai.models.ItemResource]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_openai_models6.ItemResource]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_conversations_list_conversation_items_request(
                    conversation_id=conversation_id,
                    limit=limit,
                    order=order,
                    after=after,
                    before=before,
                    item_type=item_type,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_openai_models6.ItemResource], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)


class ResponsesOperations:
    """
    .. warning::
        **DO NOT** instantiate this class directly.

        Instead, you should access the following operations through
        :class:`~azure.ai.agents.aio.AgentsClient`'s
        :attr:`responses` attribute.
    """

    def __init__(self, *args, **kwargs) -> None:
        input_args = list(args)
        self._client: AsyncPipelineClient = input_args.pop(0) if input_args else kwargs.pop("client")
        self._config: AgentsClientConfiguration = input_args.pop(0) if input_args else kwargs.pop("config")
        self._serialize: Serializer = input_args.pop(0) if input_args else kwargs.pop("serializer")
        self._deserialize: Deserializer = input_args.pop(0) if input_args else kwargs.pop("deserializer")

    @overload
    async def create_response(
        self,
        *,
        content_type: str = "application/json",
        metadata: Optional[dict[str, str]] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        user: Optional[str] = None,
        service_tier: Optional[Union[str, _openai_models6.ServiceTier]] = None,
        top_logprobs: Optional[int] = None,
        previous_response_id: Optional[str] = None,
        model: Optional[str] = None,
        reasoning: Optional[_openai_models6.Reasoning] = None,
        background: Optional[bool] = None,
        max_output_tokens: Optional[int] = None,
        max_tool_calls: Optional[int] = None,
        text: Optional[_openai_models6.CreateResponseRequestText] = None,
        tools: Optional[list[_openai_models6.Tool]] = None,
        tool_choice: Optional[Union[str, _openai_models6.ToolChoiceOptions, _openai_models6.ToolChoiceObject]] = None,
        prompt: Optional[_openai_models6.Prompt] = None,
        truncation: Optional[Literal["auto", "disabled"]] = None,
        input: Optional[Union[str, list[Union[_openai_models6.ImplicitUserMessage, _openai_models6.ItemParam]]]] = None,
        include: Optional[list[Union[str, _openai_models6.Includable]]] = None,
        parallel_tool_calls: Optional[bool] = None,
        store: Optional[bool] = None,
        instructions: Optional[str] = None,
        stream_parameter: Optional[bool] = None,
        conversation: Optional[Union[str, _openai_models6.CreateResponseRequestConversation1]] = None,
        agent: Optional[_models2.AgentReference] = None,
        **kwargs: Any
    ) -> _openai_models6.Response:
        """Creates a model response.

        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword metadata: Set of 16 key-value pairs that can be attached to an object. This can be
         useful for storing additional information about the object in a structured
         format, and querying for objects via API or the dashboard.

         Keys are strings with a maximum length of 64 characters. Values are strings
         with a maximum length of 512 characters. Default value is None.
        :paramtype metadata: dict[str, str]
        :keyword temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8
         will make the output more random, while lower values like 0.2 will make it more focused and
         deterministic.
         We generally recommend altering this or ``top_p`` but not both. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature, called nucleus sampling,
         where the model considers the results of the tokens with top_p probability
         mass. So 0.1 means only the tokens comprising the top 10% probability mass
         are considered.

         We generally recommend altering this or ``temperature`` but not both. Default value is None.
        :paramtype top_p: float
        :keyword user: A unique identifier representing your end-user, which can help OpenAI to monitor
         and detect abuse. `Learn more </docs/guides/safety-best-practices#end-user-ids>`_. Default
         value is None.
        :paramtype user: str
        :keyword service_tier: Note: service_tier is not applicable to Azure OpenAI. Known values are:
         "auto", "default", "flex", "scale", and "priority". Default value is None.
        :paramtype service_tier: str or ~openai.models.ServiceTier
        :keyword top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens
         to return at each token position, each with an associated log probability. Default value is
         None.
        :paramtype top_logprobs: int
        :keyword previous_response_id: The unique ID of the previous response to the model. Use this to
         create multi-turn conversations. Learn more about
         `conversation state </docs/guides/conversation-state>`_. Default value is None.
        :paramtype previous_response_id: str
        :keyword model: The model deployment to use for the creation of this response. Default value is
         None.
        :paramtype model: str
        :keyword reasoning: Default value is None.
        :paramtype reasoning: ~openai.models.Reasoning
        :keyword background: Whether to run the model response in the background.
         `Learn more </docs/guides/background>`_. Default value is None.
        :paramtype background: bool
        :keyword max_output_tokens: An upper bound for the number of tokens that can be generated for a
         response, including visible output tokens and `reasoning tokens </docs/guides/reasoning>`_.
         Default value is None.
        :paramtype max_output_tokens: int
        :keyword max_tool_calls: The maximum number of total calls to built-in tools that can be
         processed in a response. This maximum number applies across all built-in tool calls, not per
         individual tool. Any further attempts to call a tool by the model will be ignored. Default
         value is None.
        :paramtype max_tool_calls: int
        :keyword text: Configuration options for a text response from the model. Can be plain
         text or structured JSON data. Learn more:

         * [Text inputs and outputs](/docs/guides/text)
         * [Structured Outputs](/docs/guides/structured-outputs). Default value is None.
        :paramtype text: ~openai.models.CreateResponseRequestText
        :keyword tools: An array of tools the model may call while generating a response. You
         can specify which tool to use by setting the ``tool_choice`` parameter.

         The two categories of tools you can provide the model are:



         * **Built-in tools**: Tools that are provided by OpenAI that extend the
         model's capabilities, like file search.
         * **Function calls (custom tools)**: Functions that are defined by you,
         enabling the model to call your own code. Default value is None.
        :paramtype tools: list[~openai.models.Tool]
        :keyword tool_choice: How the model should select which tool (or tools) to use when generating
         a response. See the ``tools`` parameter to see how to specify which tools
         the model can call. Is either a Union[str, "_openai_models4.ToolChoiceOptions"] type or a
         ToolChoiceObject type. Default value is None.
        :paramtype tool_choice: str or ~openai.models.ToolChoiceOptions or
         ~openai.models.ToolChoiceObject
        :keyword prompt: Default value is None.
        :paramtype prompt: ~openai.models.Prompt
        :keyword truncation: The truncation strategy to use for the model response.

         * `auto`: If the context of this response and previous ones exceeds
         the model's context window size, the model will truncate the
         response to fit the context window by dropping input items in the
         middle of the conversation.
         * `disabled` (default): If a model response will exceed the context window
         size for a model, the request will fail with a 400 error. Is either a Literal["auto"] type or
         a Literal["disabled"] type. Default value is None.
        :paramtype truncation: str or str
        :keyword input: Text, image, or file inputs to the model, used to generate a response.

         Learn more:

         * [Text inputs and outputs](/docs/guides/text)
         * [Image inputs](/docs/guides/images)
         * [File inputs](/docs/guides/pdf-files)
         * [Conversation state](/docs/guides/conversation-state)
         * [Function calling](/docs/guides/function-calling). Is either a str type or a
         [Union["_openai_models4.ImplicitUserMessage", "_openai_models4.ItemParam"]] type. Default value
         is None.
        :paramtype input: str or list[~openai.models.ImplicitUserMessage or ~openai.models.ItemParam]
        :keyword include: Specify additional output data to include in the model response. Currently
         supported values are:

         * `code_interpreter_call.outputs`: Includes the outputs of python code execution
         in code interpreter tool call items.
         * `computer_call_output.output.image_url`: Include image urls from the computer call output.
         * `file_search_call.results`: Include the search results of
         the file search tool call.
         * `message.input_image.image_url`: Include image urls from the input message.
         * `message.output_text.logprobs`: Include logprobs with assistant messages.
         * `reasoning.encrypted_content`: Includes an encrypted version of reasoning
         tokens in reasoning item outputs. This enables reasoning items to be used in
         multi-turn conversations when using the Responses API statelessly (like
         when the `store` parameter is set to `false`, or when an organization is
         enrolled in the zero data retention program). Default value is None.
        :paramtype include: list[str or ~openai.models.Includable]
        :keyword parallel_tool_calls: Whether to allow the model to run tool calls in parallel. Default
         value is None.
        :paramtype parallel_tool_calls: bool
        :keyword store: Whether to store the generated model response for later retrieval via
         API. Default value is None.
        :paramtype store: bool
        :keyword instructions: A system (or developer) message inserted into the model's context.

         When using along with ``previous_response_id``, the instructions from a previous
         response will not be carried over to the next response. This makes it simple
         to swap out system (or developer) messages in new responses. Default value is None.
        :paramtype instructions: str
        :keyword stream_parameter: If set to true, the model response data will be streamed to the
         client
         as it is generated using `server-sent events
         <https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format>`_.
         See the `Streaming section below </docs/api-reference/responses-streaming>`_
         for more information. Default value is None.
        :paramtype stream_parameter: bool
        :keyword conversation: Is either a str type or a CreateResponseRequestConversation1 type.
         Default value is None.
        :paramtype conversation: str or ~openai.models.CreateResponseRequestConversation1
        :keyword agent: The agent to use for generating the response. Default value is None.
        :paramtype agent: ~azure.ai.agents.models.AgentReference
        :return: Response. The Response is compatible with MutableMapping
        :rtype: ~openai.models.Response
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_response(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.Response:
        """Creates a model response.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Response. The Response is compatible with MutableMapping
        :rtype: ~openai.models.Response
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_response(
        self, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.Response:
        """Creates a model response.

        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Response. The Response is compatible with MutableMapping
        :rtype: ~openai.models.Response
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_response(
        self,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        metadata: Optional[dict[str, str]] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        user: Optional[str] = None,
        service_tier: Optional[Union[str, _openai_models6.ServiceTier]] = None,
        top_logprobs: Optional[int] = None,
        previous_response_id: Optional[str] = None,
        model: Optional[str] = None,
        reasoning: Optional[_openai_models6.Reasoning] = None,
        background: Optional[bool] = None,
        max_output_tokens: Optional[int] = None,
        max_tool_calls: Optional[int] = None,
        text: Optional[_openai_models6.CreateResponseRequestText] = None,
        tools: Optional[list[_openai_models6.Tool]] = None,
        tool_choice: Optional[Union[str, _openai_models6.ToolChoiceOptions, _openai_models6.ToolChoiceObject]] = None,
        prompt: Optional[_openai_models6.Prompt] = None,
        truncation: Optional[Literal["auto", "disabled"]] = None,
        input: Optional[Union[str, list[Union[_openai_models6.ImplicitUserMessage, _openai_models6.ItemParam]]]] = None,
        include: Optional[list[Union[str, _openai_models6.Includable]]] = None,
        parallel_tool_calls: Optional[bool] = None,
        store: Optional[bool] = None,
        instructions: Optional[str] = None,
        stream_parameter: Optional[bool] = None,
        conversation: Optional[Union[str, _openai_models6.CreateResponseRequestConversation1]] = None,
        agent: Optional[_models2.AgentReference] = None,
        **kwargs: Any
    ) -> _openai_models6.Response:
        """Creates a model response.

        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword metadata: Set of 16 key-value pairs that can be attached to an object. This can be
         useful for storing additional information about the object in a structured
         format, and querying for objects via API or the dashboard.

         Keys are strings with a maximum length of 64 characters. Values are strings
         with a maximum length of 512 characters. Default value is None.
        :paramtype metadata: dict[str, str]
        :keyword temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8
         will make the output more random, while lower values like 0.2 will make it more focused and
         deterministic.
         We generally recommend altering this or ``top_p`` but not both. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature, called nucleus sampling,
         where the model considers the results of the tokens with top_p probability
         mass. So 0.1 means only the tokens comprising the top 10% probability mass
         are considered.

         We generally recommend altering this or ``temperature`` but not both. Default value is None.
        :paramtype top_p: float
        :keyword user: A unique identifier representing your end-user, which can help OpenAI to monitor
         and detect abuse. `Learn more </docs/guides/safety-best-practices#end-user-ids>`_. Default
         value is None.
        :paramtype user: str
        :keyword service_tier: Note: service_tier is not applicable to Azure OpenAI. Known values are:
         "auto", "default", "flex", "scale", and "priority". Default value is None.
        :paramtype service_tier: str or ~openai.models.ServiceTier
        :keyword top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens
         to return at each token position, each with an associated log probability. Default value is
         None.
        :paramtype top_logprobs: int
        :keyword previous_response_id: The unique ID of the previous response to the model. Use this to
         create multi-turn conversations. Learn more about
         `conversation state </docs/guides/conversation-state>`_. Default value is None.
        :paramtype previous_response_id: str
        :keyword model: The model deployment to use for the creation of this response. Default value is
         None.
        :paramtype model: str
        :keyword reasoning: Default value is None.
        :paramtype reasoning: ~openai.models.Reasoning
        :keyword background: Whether to run the model response in the background.
         `Learn more </docs/guides/background>`_. Default value is None.
        :paramtype background: bool
        :keyword max_output_tokens: An upper bound for the number of tokens that can be generated for a
         response, including visible output tokens and `reasoning tokens </docs/guides/reasoning>`_.
         Default value is None.
        :paramtype max_output_tokens: int
        :keyword max_tool_calls: The maximum number of total calls to built-in tools that can be
         processed in a response. This maximum number applies across all built-in tool calls, not per
         individual tool. Any further attempts to call a tool by the model will be ignored. Default
         value is None.
        :paramtype max_tool_calls: int
        :keyword text: Configuration options for a text response from the model. Can be plain
         text or structured JSON data. Learn more:

         * [Text inputs and outputs](/docs/guides/text)
         * [Structured Outputs](/docs/guides/structured-outputs). Default value is None.
        :paramtype text: ~openai.models.CreateResponseRequestText
        :keyword tools: An array of tools the model may call while generating a response. You
         can specify which tool to use by setting the ``tool_choice`` parameter.

         The two categories of tools you can provide the model are:



         * **Built-in tools**: Tools that are provided by OpenAI that extend the
         model's capabilities, like file search.
         * **Function calls (custom tools)**: Functions that are defined by you,
         enabling the model to call your own code. Default value is None.
        :paramtype tools: list[~openai.models.Tool]
        :keyword tool_choice: How the model should select which tool (or tools) to use when generating
         a response. See the ``tools`` parameter to see how to specify which tools
         the model can call. Is either a Union[str, "_openai_models4.ToolChoiceOptions"] type or a
         ToolChoiceObject type. Default value is None.
        :paramtype tool_choice: str or ~openai.models.ToolChoiceOptions or
         ~openai.models.ToolChoiceObject
        :keyword prompt: Default value is None.
        :paramtype prompt: ~openai.models.Prompt
        :keyword truncation: The truncation strategy to use for the model response.

         * `auto`: If the context of this response and previous ones exceeds
         the model's context window size, the model will truncate the
         response to fit the context window by dropping input items in the
         middle of the conversation.
         * `disabled` (default): If a model response will exceed the context window
         size for a model, the request will fail with a 400 error. Is either a Literal["auto"] type or
         a Literal["disabled"] type. Default value is None.
        :paramtype truncation: str or str
        :keyword input: Text, image, or file inputs to the model, used to generate a response.

         Learn more:

         * [Text inputs and outputs](/docs/guides/text)
         * [Image inputs](/docs/guides/images)
         * [File inputs](/docs/guides/pdf-files)
         * [Conversation state](/docs/guides/conversation-state)
         * [Function calling](/docs/guides/function-calling). Is either a str type or a
         [Union["_openai_models4.ImplicitUserMessage", "_openai_models4.ItemParam"]] type. Default value
         is None.
        :paramtype input: str or list[~openai.models.ImplicitUserMessage or ~openai.models.ItemParam]
        :keyword include: Specify additional output data to include in the model response. Currently
         supported values are:

         * `code_interpreter_call.outputs`: Includes the outputs of python code execution
         in code interpreter tool call items.
         * `computer_call_output.output.image_url`: Include image urls from the computer call output.
         * `file_search_call.results`: Include the search results of
         the file search tool call.
         * `message.input_image.image_url`: Include image urls from the input message.
         * `message.output_text.logprobs`: Include logprobs with assistant messages.
         * `reasoning.encrypted_content`: Includes an encrypted version of reasoning
         tokens in reasoning item outputs. This enables reasoning items to be used in
         multi-turn conversations when using the Responses API statelessly (like
         when the `store` parameter is set to `false`, or when an organization is
         enrolled in the zero data retention program). Default value is None.
        :paramtype include: list[str or ~openai.models.Includable]
        :keyword parallel_tool_calls: Whether to allow the model to run tool calls in parallel. Default
         value is None.
        :paramtype parallel_tool_calls: bool
        :keyword store: Whether to store the generated model response for later retrieval via
         API. Default value is None.
        :paramtype store: bool
        :keyword instructions: A system (or developer) message inserted into the model's context.

         When using along with ``previous_response_id``, the instructions from a previous
         response will not be carried over to the next response. This makes it simple
         to swap out system (or developer) messages in new responses. Default value is None.
        :paramtype instructions: str
        :keyword stream_parameter: If set to true, the model response data will be streamed to the
         client
         as it is generated using `server-sent events
         <https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format>`_.
         See the `Streaming section below </docs/api-reference/responses-streaming>`_
         for more information. Default value is None.
        :paramtype stream_parameter: bool
        :keyword conversation: Is either a str type or a CreateResponseRequestConversation1 type.
         Default value is None.
        :paramtype conversation: str or ~openai.models.CreateResponseRequestConversation1
        :keyword agent: The agent to use for generating the response. Default value is None.
        :paramtype agent: ~azure.ai.agents.models.AgentReference
        :return: Response. The Response is compatible with MutableMapping
        :rtype: ~openai.models.Response
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_openai_models6.Response] = kwargs.pop("cls", None)

        if body is _Unset:
            body = {
                "agent": agent,
                "background": background,
                "conversation": conversation,
                "include": include,
                "input": input,
                "instructions": instructions,
                "max_output_tokens": max_output_tokens,
                "max_tool_calls": max_tool_calls,
                "metadata": metadata,
                "model": model,
                "parallel_tool_calls": parallel_tool_calls,
                "previous_response_id": previous_response_id,
                "prompt": prompt,
                "reasoning": reasoning,
                "service_tier": service_tier,
                "store": store,
                "stream": stream_parameter,
                "temperature": temperature,
                "text": text,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_logprobs": top_logprobs,
                "top_p": top_p,
                "truncation": truncation,
                "user": user,
            }
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_responses_create_response_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.Response, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def create_response_stream(
        self, request: _openai_models6.CreateResponse, *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ResponseStreamEvent:
        """Creates a model response (streaming response).

        :param request: Required.
        :type request: ~openai.models.CreateResponse
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ResponseStreamEvent. The ResponseStreamEvent is compatible with MutableMapping
        :rtype: ~openai.models.ResponseStreamEvent
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_response_stream(
        self, request: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ResponseStreamEvent:
        """Creates a model response (streaming response).

        :param request: Required.
        :type request: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ResponseStreamEvent. The ResponseStreamEvent is compatible with MutableMapping
        :rtype: ~openai.models.ResponseStreamEvent
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_response_stream(
        self, request: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _openai_models6.ResponseStreamEvent:
        """Creates a model response (streaming response).

        :param request: Required.
        :type request: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: ResponseStreamEvent. The ResponseStreamEvent is compatible with MutableMapping
        :rtype: ~openai.models.ResponseStreamEvent
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_response_stream(
        self, request: Union[_openai_models6.CreateResponse, JSON, IO[bytes]], **kwargs: Any
    ) -> _openai_models6.ResponseStreamEvent:
        """Creates a model response (streaming response).

        :param request: Is one of the following types: CreateResponse, JSON, IO[bytes] Required.
        :type request: ~openai.models.CreateResponse or JSON or IO[bytes]
        :return: ResponseStreamEvent. The ResponseStreamEvent is compatible with MutableMapping
        :rtype: ~openai.models.ResponseStreamEvent
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_openai_models6.ResponseStreamEvent] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(request, (IOBase, bytes)):
            _content = request
        else:
            _content = json.dumps(request, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_responses_create_response_stream_request(
            content_type=content_type,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Content-Type"] = self._deserialize("str", response.headers.get("Content-Type"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ResponseStreamEvent, response.text())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_response(
        self,
        response_id: str,
        *,
        includables: Optional[list[Union[str, _openai_models6.Includable]]] = None,
        stream_parameter: Optional[bool] = None,
        starting_after: Optional[int] = None,
        **kwargs: Any
    ) -> _openai_models6.Response:
        """Retrieves a model response with the given ID.

        :param response_id: Required.
        :type response_id: str
        :keyword includables: Default value is None.
        :paramtype includables: list[str or ~openai.models.Includable]
        :keyword stream_parameter: Default value is None.
        :paramtype stream_parameter: bool
        :keyword starting_after: Default value is None.
        :paramtype starting_after: int
        :return: Response. The Response is compatible with MutableMapping
        :rtype: ~openai.models.Response
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_openai_models6.Response] = kwargs.pop("cls", None)

        _request = build_responses_get_response_request(
            response_id=response_id,
            includables=includables,
            stream_parameter=stream_parameter,
            starting_after=starting_after,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.Response, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_response_stream(
        self,
        response_id: str,
        *,
        includables: Optional[list[Union[str, _openai_models6.Includable]]] = None,
        starting_after: Optional[int] = None,
        **kwargs: Any
    ) -> _openai_models6.ResponseStreamEvent:
        """Retrieves a model response with the given ID (streaming response).

        :param response_id: The ID of the response to retrieve. Required.
        :type response_id: str
        :keyword includables: Default value is None.
        :paramtype includables: list[str or ~openai.models.Includable]
        :keyword starting_after: The sequence number of the event after which to start streaming.
         Default value is None.
        :paramtype starting_after: int
        :return: ResponseStreamEvent. The ResponseStreamEvent is compatible with MutableMapping
        :rtype: ~openai.models.ResponseStreamEvent
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        accept: Literal["text/event-stream"] = kwargs.pop("accept", _headers.pop("accept", "text/event-stream"))
        cls: ClsType[_openai_models6.ResponseStreamEvent] = kwargs.pop("cls", None)

        _request = build_responses_get_response_stream_request(
            response_id=response_id,
            includables=includables,
            starting_after=starting_after,
            accept=accept,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Content-Type"] = self._deserialize("str", response.headers.get("Content-Type"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.ResponseStreamEvent, response.text())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_response(self, response_id: str, **kwargs: Any) -> _models2.DeleteResponseResult:
        """Deletes a model response.

        :param response_id: The ID of the response to delete. Required.
        :type response_id: str
        :return: DeleteResponseResult. The DeleteResponseResult is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.DeleteResponseResult
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.DeleteResponseResult] = kwargs.pop("cls", None)

        _request = build_responses_delete_response_request(
            response_id=response_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.DeleteResponseResult, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def cancel_response(self, response_id: str, **kwargs: Any) -> _openai_models6.Response:
        """Cancels a model response.

        :param response_id: The ID of the response to cancel. Required.
        :type response_id: str
        :return: Response. The Response is compatible with MutableMapping
        :rtype: ~openai.models.Response
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_openai_models6.Response] = kwargs.pop("cls", None)

        _request = build_responses_cancel_response_request(
            response_id=response_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_openai_models6.Response, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def list_input_items(
        self,
        response_id: str,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        **kwargs: Any
    ) -> AsyncItemPaged["_openai_models6.ItemResource"]:
        """Returns a list of input items for a given response.

        :param response_id: Required.
        :type response_id: str
        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :return: An iterator like instance of ItemResource
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~openai.models.ItemResource]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_openai_models6.ItemResource]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_responses_list_input_items_request(
                    response_id=response_id,
                    limit=limit,
                    order=order,
                    after=after,
                    before=before,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_openai_models6.ItemResource], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)

    @distributed_trace
    def list_responses(
        self,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        agent_name: Optional[str] = None,
        agent_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        **kwargs: Any
    ) -> AsyncItemPaged["_openai_models6.Response"]:
        """Returns the list of all responses.

        Returns the list of all responses.

        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :keyword agent_name: Filter by agent name. If provided, only items associated with the
         specified agent will be returned. Default value is None.
        :paramtype agent_name: str
        :keyword agent_id: Filter by agent ID in the format ``name:version``. If provided, only items
         associated with the specified agent ID will be returned. Default value is None.
        :paramtype agent_id: str
        :keyword conversation_id: Filter by conversation ID. If provided, only responses associated
         with the specified conversation will be returned. Default value is None.
        :paramtype conversation_id: str
        :return: An iterator like instance of Response
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~openai.models.Response]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_openai_models6.Response]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_responses_list_responses_request(
                    limit=limit,
                    order=order,
                    after=after,
                    before=before,
                    agent_name=agent_name,
                    agent_id=agent_id,
                    conversation_id=conversation_id,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_openai_models6.Response], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)


class _AgentsClientOperationsMixin(
    ClientMixinABC[AsyncPipelineClient[HttpRequest, AsyncHttpResponse], AgentsClientConfiguration]
):

    @distributed_trace_async
    async def get_agent(self, agent_name: str, **kwargs: Any) -> _models2.AgentObject:
        """Retrieves the agent.

        Retrieves the agent.

        :param agent_name: The name of the agent to retrieve. Required.
        :type agent_name: str
        :return: AgentObject. The AgentObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentObject] = kwargs.pop("cls", None)

        _request = build_agents_get_agent_request(
            agent_name=agent_name,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_agent(self, agent_name: str, **kwargs: Any) -> _models2.DeleteAgentResponse:
        """Deletes an agent.

        Deletes an agent.

        :param agent_name: The name of the agent to delete. Required.
        :type agent_name: str
        :return: DeleteAgentResponse. The DeleteAgentResponse is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.DeleteAgentResponse
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.DeleteAgentResponse] = kwargs.pop("cls", None)

        _request = build_agents_delete_agent_request(
            agent_name=agent_name,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.DeleteAgentResponse, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def list_agents(
        self,
        *,
        kind: Optional[Union[str, _models2.AgentKind]] = None,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        **kwargs: Any
    ) -> AsyncItemPaged["_models2.AgentObject"]:
        """Returns the list of all agents.

        Returns the list of all agents.

        :keyword kind: Filter agents by kind. If not provided, all agents are returned. Known values
         are: "prompt", "hosted", "container_app", and "workflow". Default value is None.
        :paramtype kind: str or ~azure.ai.agents.models.AgentKind
        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :return: An iterator like instance of AgentObject
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~azure.ai.agents.models.AgentObject]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_models2.AgentObject]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_agents_list_agents_request(
                    kind=kind,
                    limit=limit,
                    order=order,
                    after=after,
                    before=before,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_models2.AgentObject], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)

    @overload
    async def create_agent_version(
        self,
        agent_name: str,
        body: _models2.CreateAgentVersionRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentVersionObject:
        """Create a new agent version.

        Create a new agent version.

        :param agent_name: The name of the agent to create/modify. Required.
        :type agent_name: str
        :param body: Required.
        :type body: ~azure.ai.agents.models.CreateAgentVersionRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentVersionObject. The AgentVersionObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentVersionObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_agent_version(
        self, agent_name: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models2.AgentVersionObject:
        """Create a new agent version.

        Create a new agent version.

        :param agent_name: The name of the agent to create/modify. Required.
        :type agent_name: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentVersionObject. The AgentVersionObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentVersionObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_agent_version(
        self, agent_name: str, body: IO[bytes], *, content_type: str = "application/json", **kwargs: Any
    ) -> _models2.AgentVersionObject:
        """Create a new agent version.

        Create a new agent version.

        :param agent_name: The name of the agent to create/modify. Required.
        :type agent_name: str
        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentVersionObject. The AgentVersionObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentVersionObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_agent_version(
        self, agent_name: str, body: Union[_models2.CreateAgentVersionRequest, JSON, IO[bytes]], **kwargs: Any
    ) -> _models2.AgentVersionObject:
        """Create a new agent version.

        Create a new agent version.

        :param agent_name: The name of the agent to create/modify. Required.
        :type agent_name: str
        :param body: Is one of the following types: CreateAgentVersionRequest, JSON, IO[bytes]
         Required.
        :type body: ~azure.ai.agents.models.CreateAgentVersionRequest or JSON or IO[bytes]
        :return: AgentVersionObject. The AgentVersionObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentVersionObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models2.AgentVersionObject] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_agents_create_agent_version_request(
            agent_name=agent_name,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentVersionObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_agent_version(
        self, agent_name: str, agent_version: str, **kwargs: Any
    ) -> _models2.AgentVersionObject:
        """Retrieves a specific version of an agent.

        Retrieves a specific version of an agent.

        :param agent_name: The name of the agent to retrieve. Required.
        :type agent_name: str
        :param agent_version: The version of the agent to retrieve. Required.
        :type agent_version: str
        :return: AgentVersionObject. The AgentVersionObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentVersionObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentVersionObject] = kwargs.pop("cls", None)

        _request = build_agents_get_agent_version_request(
            agent_name=agent_name,
            agent_version=agent_version,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentVersionObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_agent_version(
        self, agent_name: str, agent_version: str, **kwargs: Any
    ) -> _models2.DeleteAgentVersionResponse:
        """Deletes a specific version of an agent.

        Deletes a specific version of an agent.

        :param agent_name: The name of the agent to delete. Required.
        :type agent_name: str
        :param agent_version: The version of the agent to delete. Required.
        :type agent_version: str
        :return: DeleteAgentVersionResponse. The DeleteAgentVersionResponse is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.DeleteAgentVersionResponse
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.DeleteAgentVersionResponse] = kwargs.pop("cls", None)

        _request = build_agents_delete_agent_version_request(
            agent_name=agent_name,
            agent_version=agent_version,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.DeleteAgentVersionResponse, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def list_agent_versions(
        self,
        agent_name: str,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        **kwargs: Any
    ) -> AsyncItemPaged["_models2.AgentVersionObject"]:
        """Returns the list of versions of an agent.

        Returns the list of versions of an agent.

        :param agent_name: The name of the agent to retrieve versions for. Required.
        :type agent_name: str
        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :return: An iterator like instance of AgentVersionObject
        :rtype: ~azure.core.async_paging.AsyncItemPaged[~azure.ai.agents.models.AgentVersionObject]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_models2.AgentVersionObject]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_agents_list_agent_versions_request(
                    agent_name=agent_name,
                    limit=limit,
                    order=order,
                    after=after,
                    before=before,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_models2.AgentVersionObject], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)

    @overload
    async def create_or_update_agent_event_handler(
        self,
        agent_name: str,
        event_handler_name: str,
        body: _models2.AgentEventHandlerRequest,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentEventHandlerObject:
        """Create or update an event handler for an agent.

        Create/Update/Remove an event handler for an agent.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param event_handler_name: The name of the event handler to create/modify. Required.
        :type event_handler_name: str
        :param body: Required.
        :type body: ~azure.ai.agents.models.AgentEventHandlerRequest
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentEventHandlerObject. The AgentEventHandlerObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentEventHandlerObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_or_update_agent_event_handler(
        self,
        agent_name: str,
        event_handler_name: str,
        body: JSON,
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentEventHandlerObject:
        """Create or update an event handler for an agent.

        Create/Update/Remove an event handler for an agent.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param event_handler_name: The name of the event handler to create/modify. Required.
        :type event_handler_name: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentEventHandlerObject. The AgentEventHandlerObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentEventHandlerObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_or_update_agent_event_handler(
        self,
        agent_name: str,
        event_handler_name: str,
        body: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentEventHandlerObject:
        """Create or update an event handler for an agent.

        Create/Update/Remove an event handler for an agent.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param event_handler_name: The name of the event handler to create/modify. Required.
        :type event_handler_name: str
        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentEventHandlerObject. The AgentEventHandlerObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentEventHandlerObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_or_update_agent_event_handler(
        self,
        agent_name: str,
        event_handler_name: str,
        body: Union[_models2.AgentEventHandlerRequest, JSON, IO[bytes]],
        **kwargs: Any
    ) -> _models2.AgentEventHandlerObject:
        """Create or update an event handler for an agent.

        Create/Update/Remove an event handler for an agent.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param event_handler_name: The name of the event handler to create/modify. Required.
        :type event_handler_name: str
        :param body: Is one of the following types: AgentEventHandlerRequest, JSON, IO[bytes] Required.
        :type body: ~azure.ai.agents.models.AgentEventHandlerRequest or JSON or IO[bytes]
        :return: AgentEventHandlerObject. The AgentEventHandlerObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentEventHandlerObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models2.AgentEventHandlerObject] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_agents_create_or_update_agent_event_handler_request(
            agent_name=agent_name,
            event_handler_name=event_handler_name,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentEventHandlerObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_agent_event_handler(
        self, agent_name: str, event_handler_name: str, **kwargs: Any
    ) -> _models2.AgentEventHandlerObject:
        """Retrieves a specific event handler of an agent.

        Retrieves a specific event handler of an agent.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param event_handler_name: The name of the event handler to retrieve. Required.
        :type event_handler_name: str
        :return: AgentEventHandlerObject. The AgentEventHandlerObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentEventHandlerObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentEventHandlerObject] = kwargs.pop("cls", None)

        _request = build_agents_get_agent_event_handler_request(
            agent_name=agent_name,
            event_handler_name=event_handler_name,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentEventHandlerObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace
    def list_agent_event_handlers(
        self, agent_name: str, **kwargs: Any
    ) -> AsyncItemPaged["_models2.AgentEventHandlerObject"]:
        """Returns the list of event handlers of an agent.

        Returns the list of event handlers of an agent.

        :param agent_name: The name of the agent to retrieve event handlers for. Required.
        :type agent_name: str
        :return: An iterator like instance of AgentEventHandlerObject
        :rtype:
         ~azure.core.async_paging.AsyncItemPaged[~azure.ai.agents.models.AgentEventHandlerObject]
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[list[_models2.AgentEventHandlerObject]] = kwargs.pop("cls", None)

        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        def prepare_request(next_link=None):
            if not next_link:

                _request = build_agents_list_agent_event_handlers_request(
                    agent_name=agent_name,
                    api_version=self._config.api_version,
                    headers=_headers,
                    params=_params,
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            else:
                # make call to next link with the client's api-version
                _parsed_next_link = urllib.parse.urlparse(next_link)
                _next_request_params = case_insensitive_dict(
                    {
                        key: [urllib.parse.quote(v) for v in value]
                        for key, value in urllib.parse.parse_qs(_parsed_next_link.query).items()
                    }
                )
                _next_request_params["api-version"] = self._config.api_version
                _request = HttpRequest(
                    "GET", urllib.parse.urljoin(next_link, _parsed_next_link.path), params=_next_request_params
                )
                path_format_arguments = {
                    "endpoint": self._serialize.url(
                        "self._config.endpoint", self._config.endpoint, "str", skip_quote=True
                    ),
                }
                _request.url = self._client.format_url(_request.url, **path_format_arguments)

            return _request

        async def extract_data(pipeline_response):
            deserialized = pipeline_response.http_response.json()
            list_of_elem = _deserialize(list[_models2.AgentEventHandlerObject], deserialized.get("data", []))
            if cls:
                list_of_elem = cls(list_of_elem)  # type: ignore
            return None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            _request = prepare_request(next_link)

            _stream = False
            pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
                _request, stream=_stream, **kwargs
            )
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                error = _failsafe_deserialize(_models2.ApiError, response)
                raise HttpResponseError(response=response, model=error)

            return pipeline_response

        return AsyncItemPaged(get_next, extract_data)

    @distributed_trace_async
    async def delete_agent_event_handler(
        self, agent_name: str, event_handler_name: str, **kwargs: Any
    ) -> _models2.DeleteAgentEventHandlerResponse:
        """Deletes an agent event handler.

        Deletes an event handler of an agent.

        :param agent_name: The name of the agent to delete. Required.
        :type agent_name: str
        :param event_handler_name: The name of the event handler to delete. Required.
        :type event_handler_name: str
        :return: DeleteAgentEventHandlerResponse. The DeleteAgentEventHandlerResponse is compatible
         with MutableMapping
        :rtype: ~azure.ai.agents.models.DeleteAgentEventHandlerResponse
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.DeleteAgentEventHandlerResponse] = kwargs.pop("cls", None)

        _request = build_agents_delete_agent_event_handler_request(
            agent_name=agent_name,
            event_handler_name=event_handler_name,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.DeleteAgentEventHandlerResponse, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def start_agent_container(
        self,
        agent_name: str,
        agent_version: str,
        *,
        content_type: str = "application/json",
        min_replicas: Optional[int] = None,
        max_replicas: Optional[int] = None,
        **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Start containers for a specific version of an agent. If the container is already running, the
        operation will no-op.

        Start a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword min_replicas: The minimum number of replicas. Defaults to 1. Default value is None.
        :paramtype min_replicas: int
        :keyword max_replicas: The maximum number of replicas. Defaults to 1. Default value is None.
        :paramtype max_replicas: int
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def start_agent_container(
        self, agent_name: str, agent_version: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Start containers for a specific version of an agent. If the container is already running, the
        operation will no-op.

        Start a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def start_agent_container(
        self,
        agent_name: str,
        agent_version: str,
        body: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Start containers for a specific version of an agent. If the container is already running, the
        operation will no-op.

        Start a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def start_agent_container(
        self,
        agent_name: str,
        agent_version: str,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        min_replicas: Optional[int] = None,
        max_replicas: Optional[int] = None,
        **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Start containers for a specific version of an agent. If the container is already running, the
        operation will no-op.

        Start a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword min_replicas: The minimum number of replicas. Defaults to 1. Default value is None.
        :paramtype min_replicas: int
        :keyword max_replicas: The maximum number of replicas. Defaults to 1. Default value is None.
        :paramtype max_replicas: int
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models2.AgentContainerOperationObject] = kwargs.pop("cls", None)

        if body is _Unset:
            body = {"max_replicas": max_replicas, "min_replicas": min_replicas}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_agents_start_agent_container_request(
            agent_name=agent_name,
            agent_version=agent_version,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def update_agent_container(
        self,
        agent_name: str,
        agent_version: str,
        *,
        content_type: str = "application/json",
        min_replicas: Optional[int] = None,
        max_replicas: Optional[int] = None,
        **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Update containers for a specific version of an agent. If the container is not running, the
        operation will no-op.

        Update containers for a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword min_replicas: The minimum number of replicas. Default value is None.
        :paramtype min_replicas: int
        :keyword max_replicas: The maximum number of replicas. Default value is None.
        :paramtype max_replicas: int
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_agent_container(
        self, agent_name: str, agent_version: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Update containers for a specific version of an agent. If the container is not running, the
        operation will no-op.

        Update containers for a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_agent_container(
        self,
        agent_name: str,
        agent_version: str,
        body: IO[bytes],
        *,
        content_type: str = "application/json",
        **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Update containers for a specific version of an agent. If the container is not running, the
        operation will no-op.

        Update containers for a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :param body: Required.
        :type body: IO[bytes]
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def update_agent_container(
        self,
        agent_name: str,
        agent_version: str,
        body: Union[JSON, IO[bytes]] = _Unset,
        *,
        min_replicas: Optional[int] = None,
        max_replicas: Optional[int] = None,
        **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Update containers for a specific version of an agent. If the container is not running, the
        operation will no-op.

        Update containers for a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :param body: Is either a JSON type or a IO[bytes] type. Required.
        :type body: JSON or IO[bytes]
        :keyword min_replicas: The minimum number of replicas. Default value is None.
        :paramtype min_replicas: int
        :keyword max_replicas: The maximum number of replicas. Default value is None.
        :paramtype max_replicas: int
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models2.AgentContainerOperationObject] = kwargs.pop("cls", None)

        if body is _Unset:
            body = {"max_replicas": max_replicas, "min_replicas": min_replicas}
            body = {k: v for k, v in body.items() if v is not None}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IOBase, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=SdkJSONEncoder, exclude_readonly=True)  # type: ignore

        _request = build_agents_update_agent_container_request(
            agent_name=agent_name,
            agent_version=agent_version,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def stop_agent_container(
        self, agent_name: str, agent_version: str, **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Stop containers for a specific version of an agent. If the container is not running, or already
        stopped, the operation will no-op.

        Stop containers for a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentContainerOperationObject] = kwargs.pop("cls", None)

        _request = build_agents_stop_agent_container_request(
            agent_name=agent_name,
            agent_version=agent_version,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_agent_container(
        self, agent_name: str, agent_version: str, **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Delete container for a specific version of an agent. If the container doesn't exist, the
        operation will no-op.

        Delete container for a specific version of an agent.
        The operation is a long-running operation. Following the design guidelines for long-running
        operations in Azure REST APIs.
        `https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations
        <https://github.com/microsoft/api-guidelines/blob/vNext/azure/ConsiderationsForServiceDesign.md#action-operations>`_.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentContainerOperationObject] = kwargs.pop("cls", None)

        _request = build_agents_delete_agent_container_request(
            agent_name=agent_name,
            agent_version=agent_version,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [202]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        response_headers = {}
        response_headers["Operation-Location"] = self._deserialize("str", response.headers.get("Operation-Location"))

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_agent_container(
        self, agent_name: str, agent_version: str, **kwargs: Any
    ) -> _models2.AgentContainerObject:
        """Get containers for a specific version of an agent.

        The most basic operation.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :return: AgentContainerObject. The AgentContainerObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentContainerObject] = kwargs.pop("cls", None)

        _request = build_agents_get_agent_container_request(
            agent_name=agent_name,
            agent_version=agent_version,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentContainerObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_agent_container_operation(
        self, agent_name: str, operation_id: str, **kwargs: Any
    ) -> _models2.AgentContainerOperationObject:
        """Get the status of a container operation for an agent.

        The most basic operation.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param operation_id: The operation ID. Required.
        :type operation_id: str
        :return: AgentContainerOperationObject. The AgentContainerOperationObject is compatible with
         MutableMapping
        :rtype: ~azure.ai.agents.models.AgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentContainerOperationObject] = kwargs.pop("cls", None)

        _request = build_agents_get_agent_container_operation_request(
            agent_name=agent_name,
            operation_id=operation_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def list_agent_container_operations(
        self,
        agent_name: str,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        **kwargs: Any
    ) -> _models2.AgentsPagedResultAgentContainerOperationObject:
        """List container operations for an agent.

        list_agent_container_operations.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :return: AgentsPagedResultAgentContainerOperationObject. The
         AgentsPagedResultAgentContainerOperationObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentsPagedResultAgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentsPagedResultAgentContainerOperationObject] = kwargs.pop("cls", None)

        _request = build_agents_list_agent_container_operations_request(
            agent_name=agent_name,
            limit=limit,
            order=order,
            after=after,
            before=before,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentsPagedResultAgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def list_agent_version_container_operations(
        self,
        agent_name: str,
        agent_version: str,
        *,
        limit: Optional[int] = None,
        order: Optional[Literal["asc", "desc"]] = None,
        after: Optional[str] = None,
        before: Optional[str] = None,
        **kwargs: Any
    ) -> _models2.AgentsPagedResultAgentContainerOperationObject:
        """List container operations for a specific version of an agent.

        list_agent_version_container_operations.

        :param agent_name: The name of the agent. Required.
        :type agent_name: str
        :param agent_version: The version of the agent. Required.
        :type agent_version: str
        :keyword limit: A limit on the number of objects to be returned. Limit can range between 1 and
         100, and the
         default is 20. Default value is None.
        :paramtype limit: int
        :keyword order: Sort order by the ``created_at`` timestamp of the objects. ``asc`` for
         ascending order and``desc``
         for descending order. Is either a Literal["asc"] type or a Literal["desc"] type. Default value
         is None.
        :paramtype order: str or str
        :keyword after: A cursor for use in pagination. ``after`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include after=obj_foo in order to fetch the next page of the list. Default
         value is None.
        :paramtype after: str
        :keyword before: A cursor for use in pagination. ``before`` is an object ID that defines your
         place in the list.
         For instance, if you make a list request and receive 100 objects, ending with obj_foo, your
         subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         Default value is None.
        :paramtype before: str
        :return: AgentsPagedResultAgentContainerOperationObject. The
         AgentsPagedResultAgentContainerOperationObject is compatible with MutableMapping
        :rtype: ~azure.ai.agents.models.AgentsPagedResultAgentContainerOperationObject
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map: MutableMapping = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models2.AgentsPagedResultAgentContainerOperationObject] = kwargs.pop("cls", None)

        _request = build_agents_list_agent_version_container_operations_request(
            agent_name=agent_name,
            agent_version=agent_version,
            limit=limit,
            order=order,
            after=after,
            before=before,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        _request.url = self._client.format_url(_request.url, **path_format_arguments)

        _stream = kwargs.pop("stream", False)
        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            _request, stream=_stream, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            if _stream:
                try:
                    await response.read()  # Load the body in memory and close the socket
                except (StreamConsumedError, StreamClosedError):
                    pass
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            error = _failsafe_deserialize(_models2.ApiError, response)
            raise HttpResponseError(response=response, model=error)

        if _stream:
            deserialized = response.iter_bytes()
        else:
            deserialized = _deserialize(_models2.AgentsPagedResultAgentContainerOperationObject, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
