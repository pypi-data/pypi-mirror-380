"""
Data models and schemas for MCP as a Judge.

This module contains all Pydantic models used for data validation,
serialization, and API contracts.
"""

from typing import TYPE_CHECKING

from pydantic import BaseModel, Field

from mcp_as_a_judge.core.constants import MAX_TOKENS
from mcp_as_a_judge.models.task_metadata import TaskMetadata, TaskSize

if TYPE_CHECKING:  # Avoid import cycle at runtime
    from mcp_as_a_judge.workflow import WorkflowGuidance


class DesignPattern(BaseModel):
    """Design pattern specification for plan validation."""

    name: str = Field(
        description="Name of known design pattern required to be selected"
    )
    area: str = Field(
        description="Which part of the code will be solved using the pattern"
    )


class JudgeResponse(BaseModel):
    """Enhanced response model for all judge tool evaluations.

    This standardized response format ensures consistent feedback
    across all evaluation tools and includes task metadata and workflow guidance.
    """

    # Standard judge response fields
    approved: bool = Field(
        description="Whether the plan/code is approved for implementation"
    )
    required_improvements: list[str] = Field(
        default_factory=list,
        description="Specific improvements needed (empty if approved)",
    )
    feedback: str = Field(
        description="Detailed explanation of the decision and recommendations"
    )

    # Enhanced workflow fields (defaults added for robustness)
    current_task_metadata: TaskMetadata = Field(
        default_factory=lambda: TaskMetadata(
            title="Unknown Task",
            description="No metadata provided",
            user_requirements="",
            task_size=TaskSize.M,
        ),
        description="Current state of task metadata after operation",
    )

    # Use a lazy default factory to avoid importing workflow at module import time
    @staticmethod
    def _default_workflow_guidance() -> "WorkflowGuidance":
        from mcp_as_a_judge.workflow import WorkflowGuidance

        return WorkflowGuidance(
            next_tool=None,
            reasoning="Default guidance: insufficient context",
            preparation_needed=[],
            guidance="Provide required parameters and context",
        )

    workflow_guidance: "WorkflowGuidance" = Field(
        default_factory=_default_workflow_guidance,
        description="LLM-generated next steps and instructions",
    )


class ObstacleResolutionDecision(BaseModel):
    """Schema for eliciting user decision when agent encounters obstacles.

    Used by the raise_obstacle tool to capture user choices when
    the agent cannot proceed due to blockers or missing information.
    """

    chosen_option: str = Field(
        description="The option the user chooses from the provided list"
    )
    additional_context: str = Field(
        default="",
        description="Any additional context or modifications the user provides",
    )


# Note: RequirementsClarification and ObstacleResolution models have been replaced
# with dynamic model generation in _generate_dynamic_elicitation_model()
# This allows for context-specific elicitation fields generated by LLM


# NOTE: WorkflowGuidance is defined in `workflow/workflow_guidance.py` and imported above.
# This file intentionally does not redefine it to avoid duplication.


class ResearchValidationResponse(BaseModel):
    """Schema for research validation responses.

    Used by the validate_research_quality function to parse
    LLM responses about research quality and design alignment.
    """

    research_adequate: bool = Field(
        description="Whether the research is comprehensive enough"
    )
    design_based_on_research: bool = Field(
        description="Whether the design is properly based on research"
    )
    issues: list[str] = Field(
        default_factory=list, description="List of specific issues if any"
    )
    feedback: str = Field(
        description="Detailed feedback on research quality and design alignment"
    )


class TestOutputValidationResponse(BaseModel):
    """Schema for test output validation responses.

    Used by the validate_test_output function to parse
    LLM responses about test execution output quality.
    """

    looks_like_test_output: bool = Field(
        description="Whether the text appears to be genuine test execution output"
    )
    test_framework_detected: str = Field(
        description="The test framework detected (e.g., pytest, jest, junit, go test, etc.)"
    )
    has_test_results: bool = Field(
        description="Whether the output contains actual test results (pass/fail counts)"
    )
    has_execution_summary: bool = Field(
        description="Whether the output contains a test execution summary"
    )
    confidence_score: float = Field(
        description="Confidence score from 0.0 to 1.0 that this is genuine test output",
        ge=0.0,
        le=1.0,
    )
    issues: list[str] = Field(
        default_factory=list, description="List of specific issues if any"
    )
    feedback: str = Field(
        description="Detailed feedback on the test output quality and authenticity"
    )


class TestOutputValidationUserVars(BaseModel):
    """Variables for test output validation user prompt."""

    test_output: str = Field(description="The test execution output to validate")
    context: str = Field(description="Additional context about the test validation")


class ResearchAspect(BaseModel):
    """A major aspect that research must cover (LLM-extracted)."""

    name: str = Field(description="Canonical aspect name relevant to the task")
    synonyms: list[str] = Field(
        default_factory=list,
        description="List of synonymous terms or phrases that indicate coverage",
    )
    required: bool = Field(
        default=True,
        description="Whether coverage of this aspect is required for approval",
    )
    category: str | None = Field(
        default=None,
        description="Optional category, e.g., 'protocol', 'framework', 'deployment'",
    )
    rationale: str = Field(
        default="",
        description="Why this aspect is required for this task",
    )


class ResearchAspectsExtraction(BaseModel):
    """LLM-extracted research aspects that should be covered."""

    aspects: list[ResearchAspect] = Field(
        default_factory=list,
        description="List of aspects that research must cover for this task",
    )
    notes: str = Field(
        default="",
        description="Additional notes about coverage or prioritization",
    )


class ResearchAspectsUserVars(BaseModel):
    """Variables for research aspects extraction user prompt."""

    task_title: str = Field(description="Title of the coding task")
    task_description: str = Field(description="Detailed description of the task")
    user_requirements: str = Field(description="User requirements for the task")
    plan: str = Field(description="Implementation plan text (can be brief)")
    design: str = Field(description="Design summary (can be brief)")


class ResearchComplexityFactors(BaseModel):
    """Analysis factors for determining research complexity."""

    domain_specialization: str = Field(
        description="Level of domain specialization: 'general', 'specialized', 'highly_specialized'"
    )
    technology_maturity: str = Field(
        description="Maturity of required technologies: 'established', 'emerging', 'cutting_edge'"
    )
    integration_scope: str = Field(
        description="Scope of system integration: 'isolated', 'moderate', 'system_wide'"
    )
    existing_solutions: str = Field(
        description="Availability of existing solutions: 'abundant', 'limited', 'scarce'"
    )
    risk_level: str = Field(
        description="Implementation risk level: 'low', 'medium', 'high'"
    )


class ResearchRequirementsAnalysis(BaseModel):
    """LLM analysis of research requirements for a task."""

    expected_url_count: int = Field(
        description="Recommended number of research URLs for optimal coverage",
        ge=0,
        le=10,
    )
    minimum_url_count: int = Field(
        description="Minimum acceptable number of URLs for basic adequacy", ge=0, le=5
    )
    reasoning: str = Field(
        description="Detailed explanation of why these URL counts are appropriate"
    )
    complexity_factors: ResearchComplexityFactors = Field(
        description="Breakdown of complexity analysis factors"
    )
    quality_requirements: list[str] = Field(
        default_factory=list,
        description="Specific requirements for research source quality and types",
    )


class URLValidationResult(BaseModel):
    """Result of validating provided URLs against dynamic requirements."""

    adequate: bool = Field(
        description="Whether the provided URLs meet the dynamic requirements"
    )
    provided_count: int = Field(description="Number of URLs actually provided")
    expected_count: int = Field(description="Expected number of URLs based on analysis")
    minimum_count: int = Field(description="Minimum acceptable number of URLs")
    feedback: str = Field(
        description="Detailed feedback about URL adequacy and suggestions"
    )
    meets_quality_standards: bool = Field(
        description="Whether URLs meet quality requirements beyond just count"
    )


# Lightweight type aliases
ToolResponse = JudgeResponse
ElicitationResponse = str


# Prompt variable models
class SystemVars(BaseModel):
    """Unified system variables for all system prompts.

    This replaces all individual SystemVars models to reduce duplication.
    All system prompts use the same basic structure with response schema and token limits.
    """

    response_schema: str = Field(
        default="",
        description="JSON schema for the expected response format (optional)",
    )
    max_tokens: int = Field(
        default=MAX_TOKENS, description="Maximum tokens available for response"
    )
    task_size_definitions: str = Field(
        default="",
        description="Task size classifications and workflow routing rules (optional)",
    )
    plan_input_schema: str = Field(
        default="",
        description="JSON schema for judge_coding_plan input requirements (optional)",
    )
    plan_evaluation_criteria: str = Field(
        default="",
        description="Complete plan evaluation criteria and expectations (optional)",
    )
    workflow_guidance: str = Field(
        default="",
        description="Workflow guidance from task state to use as evaluation criteria (optional)",
    )
    plan_required_fields_json: str = Field(
        default="[]",
        description="JSON array of required fields for judge_coding_plan dynamic validation (optional)",
    )


class JudgeCodingPlanUserVars(BaseModel):
    """Variables for judge_coding_plan user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the coding task"
    )
    context: str = Field(description="Additional context about the task")
    plan: str = Field(description="The coding plan to be evaluated")
    design: str = Field(description="The design documentation")
    research: str = Field(description="Research findings and analysis")
    research_urls: list[str] = Field(
        default_factory=list,
        description="URLs from online research (conditional based on task needs)",
    )
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )

    # Domain focus and reuse/dependency planning (explicit inputs so plans don't get rejected)
    problem_domain: str = Field(
        default="",
        description="Concise statement of the problem domain being solved",
    )
    problem_non_goals: list[str] = Field(
        default_factory=list,
        description="Explicit non-goals/out-of-scope items to prevent scope creep",
    )
    # Library selection map and internal reuse map are free-form lists of objects
    # Using list[dict] to keep input flexible for different ecosystems
    library_plan: list[dict] = Field(
        default_factory=list,
        description=(
            "List of {purpose, selection, source: internal|external|custom, justification}"
        ),
    )
    internal_reuse_components: list[dict] = Field(
        default_factory=list,
        description="List of {path, purpose, notes} for reusable repo components",
    )

    # Conditional research fields
    research_required: bool = Field(
        default=False, description="Whether external research is required for this task"
    )
    research_scope: str = Field(
        default="none", description="Scope of research required (none, light, deep)"
    )
    research_rationale: str = Field(
        default="", description="Explanation of why research is or isn't required"
    )

    # Conditional internal research fields
    internal_research_required: bool = Field(
        default=False, description="Whether internal codebase research is needed"
    )
    related_code_snippets: list[str] = Field(
        default_factory=list, description="Related code snippets from the codebase"
    )

    # Conditional risk assessment fields
    risk_assessment_required: bool = Field(
        default=False, description="Whether risk assessment is needed"
    )
    identified_risks: list[str] = Field(
        default_factory=list, description="Areas that could be harmed by the changes"
    )
    risk_mitigation_strategies: list[str] = Field(
        default_factory=list, description="Strategies to mitigate identified risks"
    )

    # Dynamic URL requirements fields - NEW
    expected_url_count: int = Field(
        default=0,
        description="LLM-determined expected number of research URLs for this task",
    )
    minimum_url_count: int = Field(
        default=0, description="LLM-determined minimum acceptable URL count"
    )
    url_requirement_reasoning: str = Field(
        default="", description="LLM explanation of why specific URL count is needed"
    )

    # Design patterns enforcement fields
    design_patterns: list[DesignPattern] = Field(
        default_factory=list,
        description="List of design patterns to be used with their coverage areas",
    )


class JudgeResponseRepairUserVars(BaseModel):
    """Variables for repairing a non-JSON judge_coding_plan response."""

    raw_response: str = Field(
        description="Original text returned by the LLM that needs to be coerced into JSON"
    )
    task_metadata_json: str = Field(
        description=(
            "Current task metadata serialized as JSON; use this when updated metadata is"
            " not explicitly provided in the raw response"
        )
    )


class JudgeCodeChangeUserVars(BaseModel):
    """Variables for judge_code_change user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the code change"
    )
    file_path: str = Field(description="Path to the file being changed")
    change_description: str = Field(description="Description of what the change does")
    code_change: str = Field(
        description="Unified Git diff patch representing the code changes under review"
    )
    context: str = Field(description="Additional context about the code change")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class ResearchValidationUserVars(BaseModel):
    """Variables for research_validation user prompt."""

    user_requirements: str = Field(description="The user's requirements for the task")
    plan: str = Field(description="The proposed plan")
    design: str = Field(description="The design documentation")
    research: str = Field(description="Research findings to be validated")
    research_urls: list[str] = Field(
        default_factory=list,
        description="URLs from online research - count determined dynamically based on task complexity",
    )
    context: str = Field(description="Additional context about the research validation")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class WorkflowGuidanceUserVars(BaseModel):
    """Variables for workflow_guidance user prompt."""

    task_id: str = Field(description="Task ID")
    task_title: str = Field(description="Task title")
    task_description: str = Field(description="Description of the development task")
    user_requirements: str = Field(description="User requirements for the task")
    current_state: str = Field(description="Current task state")
    state_description: str = Field(description="Description of current state")
    current_operation: str = Field(description="Current operation being performed")
    task_size: str = Field(description="Task size classification")
    task_size_definitions: str = Field(description="Task size definitions")
    state_transitions: str = Field(description="Valid state transitions")
    tool_descriptions: str = Field(description="Available tool descriptions")
    conversation_context: str = Field(description="Conversation history context")
    operation_context: str = Field(description="Current operation context")
    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )
    plan_required_fields_json: str = Field(
        default="[]",
        description="JSON array of required fields for judge_coding_plan (when next_tool is judge_coding_plan)",
    )


class ValidationErrorUserVars(BaseModel):
    """Variables for validation_error user prompt."""

    validation_issue: str = Field(
        description="The specific validation issue that occurred"
    )
    context: str = Field(description="Additional context about the validation failure")


class DynamicSchemaUserVars(BaseModel):
    """Variables for dynamic_schema user prompt."""

    context: str = Field(
        description="Context about what information needs to be gathered"
    )
    information_needed: str = Field(
        description="Specific description of what information is needed from the user"
    )
    current_understanding: str = Field(
        description="What we currently understand about the situation"
    )


class ElicitationFallbackUserVars(BaseModel):
    """Variables for elicitation fallback user prompt template."""

    original_message: str = Field(
        description="The original elicitation message that was intended for the user"
    )
    required_fields: list[str] = Field(
        description="List of required field descriptions for the user to provide"
    )
    optional_fields: list[str] = Field(
        description="List of optional field descriptions for the user to provide"
    )


class ResearchRequirementsAnalysisUserVars(BaseModel):
    """Variables for research_requirements_analysis user prompt."""

    task_title: str = Field(description="Title of the coding task")
    task_description: str = Field(description="Detailed description of the task")
    user_requirements: str = Field(description="User requirements for the task")
    research_scope: str = Field(description="Current research scope (none/light/deep)")
    research_rationale: str = Field(
        description="Rationale for why research is needed at current scope"
    )
    context: str = Field(description="Additional context about the task and project")


class TestingEvaluationSystemVars(BaseModel):
    """Variables for testing evaluation system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class TestingEvaluationUserVars(BaseModel):
    """Variables for testing evaluation user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the coding task"
    )
    task_description: str = Field(
        description="Description of the coding task being tested"
    )
    modified_files: list[str] = Field(
        default_factory=list,
        description="List of implementation files that were modified",
    )
    test_summary: str = Field(description="Summary of the testing implementation")
    test_files: list[str] = Field(
        default_factory=list, description="List of test files that were created"
    )
    test_execution_results: str = Field(description="Results from running the tests")
    test_coverage_report: str = Field(description="Test coverage report data")
    test_types_implemented: list[str] = Field(
        default_factory=list,
        description="Types of tests implemented (unit, integration, e2e, etc.)",
    )
    testing_framework: str = Field(description="Testing framework used")
    performance_test_results: str = Field(
        description="Performance test results if applicable"
    )
    manual_test_notes: str = Field(
        description="Notes from manual testing if applicable"
    )
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )
