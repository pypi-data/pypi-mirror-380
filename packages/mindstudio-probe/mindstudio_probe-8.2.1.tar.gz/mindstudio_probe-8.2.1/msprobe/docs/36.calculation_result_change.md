# 模型计算结果改变原因分析

## 介绍
在模型训练场景下，使用seed_all接口同时固定随机性和打开计算，通信确定性计算，是能够保证模型跑两次得到的loss和gnorm结果完全一样。如果出现使能工具后loss或者gnorm出现偏差，可能是以下原因导致。

## 工具引入同步导致计算结果变化
工具采集统计量数据时，会涉及到将device上的tensor计算后的统计量信息通过item的时候传到cpu侧，再落盘到json文件中，item操作是一个同步的操作，可能会导致模型的计算结果出现变化。**一般的现象就是模型计算出现了NaN，但加了工具后问题不复现了。**

ASCEND_LAUNCH_BLOCKING 是一个环境变量，用于控制在 PyTorch 训练或在线推理场景中算子的执行模式。当设置为“1”时，算子将采用同步模式运行。因此如果出现加工具计算结果变化，可以设置ASCEND_LAUNCH_BLOCKING 为1，如果结果一样发生了变化，则说明是由于同步引起的结果改变。这个时候需要复现问题现象完成问题定位，推荐使用msprobe工具的异步dump功能，具体使用方式可查看[config配置](02.config_introduction.md)中的async_dump字段。

## Hook机制导致计算结果变化

pytorch/mindspore的hook机制会导致某些特殊场景下梯度计算的累加序产生变化，从而影响模型反向计算的gnorm结果。具体代码示例如下：
```python
import random, os
import numpy as np
import torch
from torch import nn


class Net(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.ln1 = nn.Linear(32, 32)
        self.bn1 = nn.BatchNorm1d(32)
        self.ln2 = nn.Linear(32, 32)

    def forward(self, x):
        x1 = self.ln1(x)

        x2 = self.bn1(x)
        x2 = self.ln2(x2)
        return x1 + x2


class BigNet(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.net1 = Net()
        self.net2 = Net()

    def forward(self, x):
        out1 = self.net1(x)
        out2 = self.net2(out1)
        return out1, out2


def my_backward_hook(module, grad_input, grad_output):
    pass


if __name__ == "__main__":
    os.environ["HCCL_DETERMINISTIC"] = 'true'

    seed = 1234
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)

    model = BigNet()
    model.net2.register_full_backward_hook(my_backward_hook)
    inputs = torch.randn(3, 32)

    out1, out2 = model(inputs)
    loss = out1.sum() + out2.sum()
    loss.backward()

    for name, param in model.named_parameters():
        print(f"{name}: {param.grad.mean()}")

```
执行一遍以上脚本，可以打印得到模型中各层的权重梯度，注释model.net2.register_full_backward_hook(my_backward_hook)后再执行一篇，可以看出bn层的权重梯度已经发生了变化。

**如果在msprobe L0，mix级别采集出现gnorm发生变化，可以尝试将采集级别改为L1，若L1级别gnorm不发生变化，则大概率是hook机制导致的梯度计算结果变化。**
