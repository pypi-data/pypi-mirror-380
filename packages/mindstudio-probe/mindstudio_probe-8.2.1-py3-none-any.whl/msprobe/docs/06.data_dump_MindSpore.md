
# msprobe 工具 MindSpore场景精度数据采集指南


## 1. 专业名词解释

* **静态图**：在编译时就确定网络结构，静态图模式拥有较高的训练性能，但难以调试。
* **动态图**：运行时动态构建网络，相较于静态图模式虽然易于调试，但难以高效执行。
* **高阶 API**：如 `mindspore.train.Model`，封装了训练过程的高级接口。
* **JIT（Just-In-Time 编译）**：MindSpore提供JIT（just-in-time）技术进一步进行性能优化。JIT模式会通过AST树解析的方式或者Python字节码解析的方式，将代码解析为一张中间表示图（IR，intermediate representation）。IR图作为该代码的唯一表示，编译器通过对该IR图的优化，来达到对代码的优化，提高运行性能。与动态图模式相对应，这种JIT的编译模式被称为静态图模式。
* **Primitive op**：MindSpore 中的基本算子，通常由 `mindspore.ops.Primitive` 定义，提供底层的算子操作接口。


## 2. 工具安装

请参见[《msprobe 工具安装指南》](./01.installation.md)。


## 3. 快速入门

以下通过一个简单的示例，展示如何在 MindSpore 中使用 msprobe 工具进行精度数据采集。

您可以参考 [动态图快速入门示例](data_dump_MindSpore/dynamic_graph_quick_start_example.md)  了解详细步骤。

## 4. 概述

msprobe 工具通过在训练脚本中添加 `PrecisionDebugger` 接口并启动训练的方式，采集模型在运行过程中的精度数据。该工具支持对MindSpore的静态图和动态图场景进行不同Level等级的精度数据采集。

dump "statistics"模式的性能膨胀大小"与"tensor"模式采集的数据量大小，可以参考[dump基线](data_dump_MindSpore/data_dump_MindSpore_baseline.md)。

**注意**：

* 因 MindSpore 框架自动微分机制的限制，dump 数据中可能会缺少原地操作模块/API 及其上一个模块/API 的反向数据。

* 使用msprobe工具后loss/gnorm发生变化：可能是工具中的item操作引入同步，pt/ms框架的hook机制等原因导致的，详见《工具导致计算结果变化》。

## 5. 场景介绍

### 5.1 静态图场景
在静态图场景下，msprobe 支持 **L0 Level** 和 **L2 Level** 的数据采集。且当 MindSpore 版本高于 2.5.0 时，若需采集 **L2 Level** 数据，必须使用编包时添加了`--include-mod=adump`选项的 mindstudio-probe whl 包进行 msprobe 工具安装。
- **L0 Level（Cell 级）** ：采集 `Cell` 对象的数据，适用于需要分析特定网络模块的情况。仅支持 2.7.0 及以上版本的 MindSpore 框架。

- **L2 Level（Kernel 级）** ：采集底层算子的输入输出数据，适用于深入分析算子级别的精度问题。

采集方式请参见[示例代码 > 静态图场景](#71-静态图场景)。详细介绍请参见[《config.json 配置文件介绍》](./02.config_introduction.md#11-通用配置)中的“level 参数”和[《config.json 配置示例》](./03.config_examples.md#2-mindspore-静态图场景) 中的“MindSpore 静态图场景”。

### 5.2 动态图场景
在动态图场景下，msprobe 支持 **L0** 、**L1** 、**mix** 、**L2**、 **debug**  的数据采集，具体分为以下几种情况：
- **使用高阶 API（如 `Model 高阶API`）** ：
  - 需要使用 `MsprobeStep` 回调类来控制数据采集的启停，适用于 **L0** 、**L1** 、**mix** 、**L2** 数据采集。

- **未使用高阶 API** ：
  - 手动在训练循环中调用 `start`、`stop`、`step` 等接口，适用于 **L0** 、**L1** 、**mix** 、**L2** 数据采集。

采集方式请参见[示例代码 > 动态图场景](#72-动态图场景)。

> **注意** ：动态图模式下，使用 `mindspore.jit` 装饰的部分实际以静态图模式执行，此时的 **Kernel 级（L2 Level）**  数据采集方式与静态图场景相同。
- **L0 Level（Cell 级）** ：采集 `Cell` 对象的数据，适用于需要分析特定网络模块的情况。

- **L1 Level（API 级）** ：采集 MindSpore API 的输入输出数据，适用于定位 API 层面的精度问题。

- **mix（模块级 + API 级）** ：在 `L0` 和 `L1` 级别的基础上同时采集模块级和 API 级数据，适用于需要分析模块和 API 层面精度问题的场景。

- **debug level （单点保存）**：单点保存网络中变量的正反向数据，适用于用户熟悉网络结构的场景。


详细介绍请参见[《config.json 配置文件介绍》](./02.config_introduction.md#11-通用配置)中的“level 参数”。


## 6 接口介绍

### 6.1 msprobe.mindspore.PrecisionDebugger

**功能说明**：通过加载 dump 配置文件的方式来确定 dump 操作的详细配置。

**原型**：

```Python
PrecisionDebugger(config_path=None, task=None, dump_path=None, level=None, step=None)
```

**参数说明**:

1. config_path：指定 dump 配置文件路径，string 类型。参数示例："./config.json"。未配置该路径时，默认使用 [config.json](../config.json) 文件的默认配置，配置选项含义可见 [config.json 介绍](./02.config_introduction.md)。
2. 其他参数均在 [config.json](../config.json) 文件中可配，详细配置可见 [config.json 介绍](./02.config_introduction.md)。

此接口的参数均不是必要，且优先级高于 [config.json](../config.json) 文件中的配置，但可配置的参数相比 config.json 较少。

#### 6.1.1 start

**功能说明**：启动精度数据采集。静态图场景下，必须在mindspore.communication.init 调用前添加。如果没有使用 [Model](https://www.mindspore.cn/tutorials/zh-CN/r2.3.1/advanced/model.html) 高阶 API 进行训练，则需要与 stop 函数一起添加在 for 循环内，否则只有需要传入model参数时，才使用该接口。

**原型**：

```Python
start(model=None, token_range=None)
```

**参数说明**:

1. model：指定需要采集数据的实例化模型，支持传入mindspore.nn.Cell、List[mindspore.nn.Cell]或Tuple[mindspore.nn.Cell] 类型，默认未配置。Cell级别（"L0" level）dump 与 "mix" level dump 时，必须传入 model 才可以采集 model 内的所有 Cell 对象数据，且若存在会进行图编译的 Cell 对象（例如被 `mindspore.jit` 装饰的 Cell），则必须在第一个 step 训练开始前调用 `start` 接口。API级别（"L1" level）dump 时，传入 model 可以采集 model 内包含 primitive op 对象在内的所有 API 数据，若不传入 model 参数，则只采集非 primitive op 的 API 数据。token_range不为None时，必须传入model参数。
<br>对于复杂模型，如果仅需要监控一部分(如model.A，model.A extends mindspore.nn.Cell)，传入需要监控的部分(如model.A)即可。  
注意：传入的当前层不会被dump，工具只会dump传入层的子层级。如传入了model.A，A本身不会被dump，而是会dump A.x, A.x.xx等。
2. token_range：指定推理模型采集时的token循环始末范围，支持传入[int, int]类型，代表[start, end]，范围包含边界，默认未配置。

#### 6.1.2 stop

**功能说明**：停止精度数据采集。在 **start** 函数之后的任意位置添加。若 **stop** 函数添加在反向计算代码之后，则会采集 **start** 和该函数之间的前反向数据。
若 **stop** 函数添加在反向计算代码之前，则需要将 [**step**](#613-step) 函数添加到反向计算代码之后，才能采集 **start** 和该函数之间的前反向数据，参考[**采集指定代码块的前反向数据**](#7213-采集指定代码块的前反向数据)。
**仅未使用 Model 高阶 API 的动态图场景支持。**

**注意**：**stop** 函数必须调用，否则可能导致精度数据落盘不全。

**原型**：

```Python
stop()
```

#### 6.1.3 step

**功能说明**：结束一个 step 的数据采集，完成所有数据落盘并更新 dump 参数。在一个 step 结束的位置添加，且必须在 **stop** 函数之后的位置调用。
该函数需要配合 **start** 和 **stop** 函数使用，尽量添加在反向计算代码之后，否则可能会导致反向数据丢失。
**仅未使用 Model 高阶 API 的动态图和静态图场景支持。**

**原型**：

```Python
step()
```

#### 6.1.4 forward_backward_dump_end

**功能说明**：停止精度数据采集。与 **stop** 函数功能相同，该函数在将来会被移除，建议使用 **stop** 函数。

**L1级别数据中的jit数据采集行为不受此接口影响。**

**仅未使用 Model 高阶 API 的动态图场景支持。**

**原型**：

```Python
forward_backward_dump_end()
```

#### 6.1.5 save

**功能说明**：单点保存网络执行过程中正反向数值，并以统计值/张量文件落盘。

**原型**：
```python
save(variable, name, save_backward=True)
```

**参数说明**:
| 参数名称        | 参数含义          |        支持数据类型    |   是否必选|
| ----------     | ------------------| ------------------- | ------------------- |
| variable       | 需要保存的变量     |dict, list, tuple, torch.tensor, int, float, str |  是  |
| name           | 指定的名称         | str                 | 是  |
| save_backward  | 是否保存反向数据   | boolean             | 否 |

具体使用样例可参考：[单点保存工具使用介绍](./28.debugger_save_instruction.md)。

#### 6.1.6 set_init_step

**功能说明**：设置起始step数，step数默认从0开始计数，使用该接口后step从指定值开始计数。该函数需要写在训练迭代的循环开始前，不能写在循环内。

**原型**：

```Python
set_init_step(step)
```

**参数说明**:

1.step: 指定的起始step数。


#### 6.1.7 register_custom_api

**功能说明**：注册用户自定义的api到工具，用于 L1 dump 。

**原型**：

```Python
debugger.register_custom_api(module, api_name, api_prefix)
```
**参数说明**:

以 torch.matmul api 为例

1.module: api 所属的包，即传入 torch。

2.api_name: api 名，string类型，即传入 "matmul"。

3.api_prefix: [dump.json](./27.dump_json_instruction.md) 中 api 名的前缀，可选，默认为包名的字符串格式， 即 "torch"。

#### 6.1.8 restore_custom_api

**功能说明**：恢复用户原有的自定义的api，取消 dump 。

**原型**：

```Python
debugger.restore_custom_api(module, api_name)
```
**参数说明**:

以 torch.matmul api 为例

1.module: api 所属的包，即传入 torch。

2.api_name: api 名，string类型，即传入 "matmul"。


### 6.2 msprobe.mindspore.MsprobeStep

**功能说明**：MindSpore Callback类，自动在每个step开始时调用start()接口，在每个step结束时调用stop()、step()接口。实现使用 Model 高阶 API 的动态图场景下 L0、L1、mix 级别，和静态图场景下 L0级别的精度数据采集控制，控制粒度为单个 **Step** ，而 PrecisionDebugger.start, PrecisionDebugger.stop 接口的控制粒度为任意训练代码段。

**原型**：

```Python
MsprobeStep(debugger)
```

**参数说明**:

1. debugger：PrecisionDebugger对象。

### 6.3 msprobe.mindspore.MsprobeInitStep

**功能说明**：MindSpore Callback 类，自动获取并设置初始 step 值。仅适用于静态图 O0/O1 模式的断点续训场景。

**原型**：

```Python
MsprobeInitStep()
```

### 6.4 msprobe.mindspore.seed_all

**功能说明**：用于固定网络中的随机性和开启确定性计算。

**原型**：
```python
seed_all(seed=1234, mode=False, rm_dropout=False)
```

**参数说明**:

1. seed: 随机性种子，默认值：1234，非必选。参数示例: seed=1000。该参数用于 random、numpy.random, mindspore.common.Initializer、mindspore.nn.probability.distribution的随机数生成以及 Python 中 str、bytes、datetime 对象的 hash 算法。

2. mode：确定性计算使能，可配置 True 或 False，默认值：False，非必选。参数示例：mode=True。该参数设置为 True 后，将会开启算子确定性运行模式与归约类通信算子（AllReduce、ReduceScatter、Reduce）的确定性计算。注意：确定性计算会导致API执行性能降低，建议在发现模型多次执行结果不同的情况下开启。

3. rm_dropout：控制dropout失效的开关。可配置 True 或 False，默认值：False，非必选。参数示例：rm_dropout=True。该参数设置为 True 后，将会使mindspore.ops.Dropout，mindspore.ops.Dropout2D，mindspore.ops.Dropout3D，mindspore.mint.nn.Dropout和mindspore.mint.nn.functional.dropout失效，以避免因随机dropout造成的网络随机性。建议在采集mindspore数据前开启。注意：通过rm_dropout控制dropout失效或生效需要在初始化Dropout实例前调用才能生效。

## 7. 示例代码

### 7.1 静态图场景

#### 7.1.1 L0 级别

**说明**: 静态图 L0 级别的Dump功能是基于mindspore.ops.TensorDump算子实现。在Ascend平台上的Graph模式下，可以通过设置环境变量 [MS_DUMP_SLICE_SIZE 和 MS_DUMP_WAIT_TIME](https://www.mindspore.cn/docs/zh-CN/r2.5.0/api_python/env_var_list.html) 解决在输出大Tesnor或输出Tensor比较密集场景下算子执行失败的问题。

##### 7.1.1.1 未使用 Model 高阶 API


```python
import mindspore as ms
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
debugger = PrecisionDebugger(config_path="./config.json")

# 模型、损失函数的定义以及初始化等操作
# ...
model = Network()
# 数据集迭代的地方往往是模型开始训练的地方
for data, label in data_loader:
    debugger.start(model) # 进行 L0 级别下Cell 对象的数据采集时调用
    # 如下是模型每个 step 执行的逻辑
    grad_net = ms.grad(model)(data)
    # ...
    debugger.step()         # 更新迭代数
```

##### 7.1.1.2 使用 Model 高阶 API


```python
import mindspore as ms
from mindspore.train import Model
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
from msprobe.mindspore.common.utils import MsprobeStep
debugger = PrecisionDebugger(config_path="./config.json")

# 模型、损失函数的定义以及初始化等操作
# ...

model = Network()
# 进行 L0 级别下 Cell 对象的数据采集时调用
debugger.start(model)
trainer = Model(model, loss_fn=loss_fn, optimizer=optimizer, metrics={'accuracy'})
trainer.train(1, train_dataset, callbacks=[MsprobeStep(debugger)])
```

#### 7.1.2 L2 级别

```python
import mindspore as ms
ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
debugger = PrecisionDebugger(config_path="./config.json")
debugger.start()
# 请勿将以上初始化流程置于模型实例化或 mindspore.communication.init 调用后
# 模型定义和训练代码
# ...
debugger.stop()
debugger.step()
```

### 7.2 动态图场景

#### 7.2.1 L0 ,L1, mix 级别

##### 7.2.1.1 未使用 Model 高阶 API


```python
import mindspore as ms
ms.set_context(mode=ms.PYNATIVE_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
debugger = PrecisionDebugger(config_path="./config.json")

# 模型、损失函数的定义以及初始化等操作
# ...
model = Network()
# 数据集迭代的地方往往是模型开始训练的地方
for data, label in data_loader:
    debugger.start()        # 进行 L1 级别下非 primitive op 采集时调用
    # debugger.start(model) # 进行 L0, mix 级别或 L1 级别下 primitive op 的数据采集时调用
    # 如下是模型每个 step 执行的逻辑
    grad_net = ms.grad(model)(data)
    # ...
    debugger.stop()         # 关闭数据 dump
    debugger.step()         # 更新迭代数
```

##### 7.2.1.2 使用 Model 高阶 API


```python
import mindspore as ms
from mindspore.train import Model
ms.set_context(mode=ms.PYNATIVE_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
from msprobe.mindspore.common.utils import MsprobeStep
debugger = PrecisionDebugger(config_path="./config.json")

# 模型、损失函数的定义以及初始化等操作
# ...

model = Network()
# 只有进行 L0 级别下 Cell 对象，mix 级别，L1 级别下 primitive op 的数据采集时才需要调用
# debugger.start(model)
trainer = Model(model, loss_fn=loss_fn, optimizer=optimizer, metrics={'accuracy'})
trainer.train(1, train_dataset, callbacks=[MsprobeStep(debugger)])
```

##### 7.2.1.3 采集指定代码块的前反向数据

```python
import mindspore as ms
from mindspore import set_device
from mindspore.train import Model
ms.set_context(mode=ms.PYNATIVE_MODE)

set_device("Ascend", 0)

from msprobe.mindspore import PrecisionDebugger
from msprobe.mindspore.common.utils import MsprobeStep
debugger = PrecisionDebugger(config_path="./config.json")

# 模型、损失函数的定义及初始化等操作
# ...
# 数据集迭代的位置一般为模型训练开始的位置
for data, label in data_loader:
    debugger.start()  # 开启数据dump
    # 如下是模型每个step执行的逻辑
    output = model(data)

    debugger.stop()  # 插入该函数到start函数之后，只dump start函数到该函数之间的前反向数据，可以支持start-stop-start-stop-step分段采集。
    # ...
    loss.backward()
    debugger.step()  # 结束一个step的dump
```

#### 7.2.2 L2 级别

##### 7.2.2.1 未使用 Model 高阶 API


```python
import mindspore as ms
ms.set_context(mode=ms.PYNATIVE_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
debugger = PrecisionDebugger(config_path="./config.json")
debugger.start()
# 请勿将以上初始化流程置于模型实例化或 mindspore.communication.init 调用后

# 模型、损失函数的定义以及初始化等操作
# ...
model = Network()
# 数据集迭代的地方往往是模型开始训练的地方
for data, label in data_loader:
    # 如下是模型每个 step 执行的逻辑
    grad_net = ms.grad(model)(data)
    # ...
```


##### 7.2.2.2 使用 Model 高阶 API


```python
import mindspore as ms
from mindspore.train import Model
ms.set_context(mode=ms.PYNATIVE_MODE, device_target="Ascend")

from msprobe.mindspore import PrecisionDebugger
debugger = PrecisionDebugger(config_path="./config.json")
debugger.start()
# 请勿将以上初始化流程置于模型实例化或 mindspore.communication.init 调用后

# 模型、损失函数的定义以及初始化等操作
# ...

model = Network()
trainer = Model(model, loss_fn=loss_fn, optimizer=optimizer, metrics={'accuracy'})
trainer.train(1, train_dataset)
```


#### 7.2.3 推理模型采集指定token_range
需要配合mindtorch套件改造原推理代码，套件包装后使用方式与torch一致，唯一区别为import的是msprobe.mindspore下的PrecisionDebugger。

```Python
from vllm import LLM, SamplingParams
from msprobe.mindspore import PrecisionDebugger, seed_all
# 在模型训练开始前固定随机性
seed_all()
# 请勿将PrecisionDebugger的初始化流程插入到循环代码中
debugger = PrecisionDebugger(config_path="./config.json", dump_path="./dump_path")
# 模型定义及初始化等操作
prompts = ["Hello, my name is"]
sampling_params = SamplingParams(temprature=0.8, top_p=0.95)
llm = LLM(model='...')
model = llm.llm_engine.model_executor.driver_worker.worker.model_runner.get_model()
# 开启数据dump, 指定采集推理模型逐字符循环推理中的第1~3次
debugger.start(model=model, token_range=[1,3])  
# 推理模型生成的逻辑
output = llm.generate(prompts, sampling_params=sampling_params)
# 关闭数据dump并落盘
debugger.stop()
debugger.step()
```

## 8. dump 结果文件介绍

### 8.1 静态图场景

训练结束后，数据将保存在 `dump_path` 指定的目录下。<br/>
L0 级别 dump 的目录结构与动态图场景下目录结构一致。<br/>
L2 级别 dump 的目录结构如下所示：

若jit_level=O2，MindSpore 版本不低于 2.5.0，且使用mindstudio-probe发布包或源码编包时添加了`--include-mod=adump`选项，目录结构示例如下：
```
├── dump_path
│   ├── acl_dump_{device_id}.json
│   ├── rank_0
│   |   ├── {timestamp}
│   |   │   ├── step_0
|   |   |   |    ├── AssignAdd.Default_network-TrainOneStepCell_optimzer-Gsd_AssignAdd-op0.0.10.1735011096403740.input.0.ND.INT32.npy
|   |   |   |    ├── Cast.Default_network-TrainOneStepCell_network-WithLossCell__backbone-Net_Cast-op0.9.10.1735011096426349.input.0.ND.FLOAT.npy
|   |   |   |    ├── GetNext.Default_GetNext-op0.0.11.17350110964032987.output.0.ND.FLOAT.npy
|   |   |   |    ...
|   |   |   |    ├── RefDAata.accum_bias1.6.10.1735011096424907.output.0.ND.FLOAT.npy
|   |   |   |    ├── Sub.Default_network-TrainOneStepCell_network-WithLossCell__backbone-Net_Sub-op0.10.10.1735011096427368.input.0.ND.BF16
|   |   |   |    └── mapping.csv
│   |   │   ├── step_1
|   |   |   |    ├── ...
|   |   |   ├── ...
|   |   ├── ...
|   |
│   ├── ...
|   |
│   └── rank_7
│       ├── ...
```
**说明**
1. 若配置文件中指定落盘npy格式，但是实际数据格式不在npy支持范围内(如bf16、int4等)，则该tensor会以原始码流落盘，并不会转换为npy格式。
2. 若原始文件全名长度超过255个字符，则文件基础名会被转换为长度为32位的随机数字字符串，原始文件名与转换后文件名的对应关系会保存在同目录下的`mapping.csv`文件中。
3. acl_dump_{device_id}.json 为在 Dump 接口调用过程中生成的中间文件，一般情况下无需关注。

其他场景下，除 kernel_kbyk_dump.json（jit_level=O0/O1）、kernel_graph_dump.json（jit_level=O2）等无需关注的中间文件外的其他 dump 结果文件请参见 MindSpore 官方文档中的[ Ascend 下 O0/O1 模式 Dump 数据对象目录和数据文件介绍](https://www.mindspore.cn/docs/zh-CN/r2.5.0/model_train/debug/dump.html#%E6%95%B0%E6%8D%AE%E5%AF%B9%E8%B1%A1%E7%9B%AE%E5%BD%95%E5%92%8C%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E4%BB%8B%E7%BB%8D)
### 8.2 动态图场景

dump 结果目录结构示例如下：

```lua
├── dump_path
│   ├── step0
│   |   ├── rank0
│   |   │   ├── dump_tensor_data
|   |   |   |    ├── MintFunctional.relu.0.backward.input.0.npy
|   |   |   |    ├── Mint.abs.0.forward.input.0.npy
|   |   |   |    ├── Functional.split.0.forward.input.0.npy       # 命名格式为{api_type}.{api_name}.{API调用次数}.{forward/backward}.{input/output}.{参数序号}, 其中，“参数序号”表示该API的第n个输入或输出，例如1，则为第一个参数，若该参数为list格式，则根据list继续排序，例如1.1，表示该API的第1个参数的第1个元素。
|   |   |   |    ├── Tensor.__add__.0.forward.output.0.npy
|   |   |   |    ...
|   |   |   |    ├── Jit.AlexNet.0.forward.input.0.npy
|   |   |   |    ├── Primitive.conv2d.Conv2d.0.forward.input.0.npy
|   |   |   |    ├── Cell.conv1.Conv2d.forward.0.parameters.weight.npy # 模块参数数据：命名格式为{Cell}.{cell_name}.{class_name}.forward.{调用次数}.parameters.{parameter_name}。
|   |   |   |    ├── Cell.conv1.Conv2d.parameters_grad.weight.npy      # 模块参数梯度数据：命名格式为{Cell}.{cell_name}.{class_name}.parameters_grad.{parameter_name}。因为同一模块的参数使用同一梯度进行更新，所以参数梯度文件名不包含调用次数。
|   |   |   |    └── Cell.relu.ReLU.forward.0.input.0.npy              # 命名格式为{Cell}.{cell_name}.{class_name}.{forward/backward}.{调用次数}.{input/output}.{参数序号}, 其中，“参数序号”表示该Cell的第n个参数，例如1，则为第一个参数，若该参数为list格式，则根据list继续排序，例如1.1，表示该Cell的第1个参数的第1个元素。
|   |   |   |                                                          # 当dump时传入的model参数为List[mindspore.nn.Cell]或Tuple[mindspore.nn.Cell]时，模块级数据的命名中包含该模块在列表中的索引index，命名格式为{Cell}.{index}.*，*表示以上三种模块级数据的命名格式，例如：Cell.0.relu.ReLU.forward.0.input.0.npy。
│   |   |   ├── dump.json
│   |   |   ├── stack.json
│   |   |   ├── dump_error_info.log
│   |   |   └── construct.json
│   |   ├── rank1
|   |   |   ├── dump_tensor_data
|   |   |   |   └── ...
│   |   |   ├── dump.json
│   |   |   ├── stack.json
│   |   |   ├── dump_error_info.log
|   |   |   └── construct.json
│   |   ├── ...
│   |   |
|   |   └── rank7
│   ├── step1
│   |   ├── ...
│   ├── step2
```

* `rank`：设备 ID，每张卡的数据保存在对应的 `rank{ID}` 目录下。非分布式场景下没有 rank ID，目录名称为 rank。
* `dump_tensor_data`：保存采集到的张量数据。
* `dump.json`： 保存API或Cell前反向数据的统计量信息。包含dump数据的API名称或Cell名称，各数据的dtype、 shape、max、min、mean、L2norm（L2范数，平方根）统计信息以及当配置summary_mode="md5"时的CRC-32数据。具体介绍可参考[dump.json文件说明](./27.dump_json_instruction.md#2-mindspore-场景下的-dumpjson-文件)。
* `dump_error_info.log`: 仅在dump工具报错时拥有此记录日志，用于记录dump错误日志。
* `stack.json`：API/Cell的调用栈信息。
* `construct.json`：根据model层级展示分层分级结构，level为L1时，construct.json内容为空。

dump 过程中，npy 文件在对应API或者模块被执行后就会落盘，而 json 文件则需要在正常执行 PrecisionDebugger.stop() 后才会写入完整数据，因此，程序异常终止时，被执行API对应的 npy 文件已被保存，但 json 文件中的数据可能丢失。

动态图场景下使用 `mindspore.jit` 装饰特定 Cell 或 function 时，被装饰的部分会被编译成**静态图**执行。

- config.json 文件配置 level 为 L0 或 mix，且 MindSpore 版本不低于 2.7.0 时， 若存在 construct 方法被 `mindspore.jit` 装饰的 Cell 对象，则 dump_path 下将生成 `graph` 与 `pynative` 目录，分别存放 construct 方法被 `mindspore.jit` 装饰的 Cell 对象的精度数据、其它Cell 或 API 对象的精度数据。示例如下：

```lua
├── dump_path
│   ├── graph
│   |   ├── step0
│   |   |   ├── rank0
│   |   │   |   ├── dump_tensor_data
|   |   |   |   |   ├── ...
│   |   |   |   ├── dump.json
│   |   |   |   ├── stack.json
│   |   |   |   └── construct.json
│   |   |   ├── ...
│   ├── pynative
│   |   ├── step0
│   |   |   ├── rank0
│   |   │   |   ├── dump_tensor_data
|   |   |   |   |   ├── ...
│   |   |   |   ├── dump.json
│   |   |   |   ├── stack.json
│   |   |   |   └── construct.json
│   |   |   ├── ...
```

**注意**：因为在被 `mindspore.jit` 装饰的 construct 方法前后插入的 Dump 算子既处于动态图模式，也处于静态图模式，所以最外层被装饰的 Cell 对象的精度数据将被重复采集。

- config.json 文件配置 level 为 L1 时， 若 `mindspore.jit` 的 `capture_mode` 参数设置为 ast（原 PSJit 场景）, 则被装饰的部分也作为 API 被 dump 到对应目录；若 `mindspore.jit` 的 `capture_mode` 参数设置为 bytecode（原 PIJit 场景）, 则被装饰的部分会被还原为动态图，按 API 粒度进行 dump。

- config.json 文件配置 level 为 L2 时， 仅会 dump 被 `mindspore.jit` 装饰部分的 kernel 精度数据，其结果目录同 jit_level 为 O0/O1 时的静态图 dump 结果相同。

npy文件名的前缀含义如下：

| 前缀           | 含义                           |
| -------------- |------------------------------|
| Tensor         | mindspore.Tensor API数据             |
| Functional     | mindspore.ops API数据                |
| Primitive      | mindspore.ops.Primitive API数据      |
| Mint           | mindspore.mint API数据              |
| MintFunctional | mindspore.mint.nn.functional API数据 |
| MintDistributed | mindspore.mint.distributed API数据 |
| Distributed    | mindspore.communication.comm_func API数据     |
| Jit            | 被"jit"装饰的模块或函数数据                |
| Cell           | mindspore.nn.Cell 类（模块）数据           |


## 9.补充说明

### 9.1 修改 API 支持列表

动态图 API 级 dump 时，本工具提供固定的 API 支持列表，仅支持对列表中的 API 进行精度数据采集。一般情况下，无需修改该列表，而是通过config.json中的scope/list字段进行 dump API 指定。若需要改变 API 支持列表，可以在 `msprobe/mindspore/dump/hook_cell/support_wrap_ops.yaml` 文件内手动修改，如下示例：

```yaml
ops:
  - adaptive_avg_pool1d
  - adaptive_avg_pool2d
  - adaptive_avg_pool3d
```
