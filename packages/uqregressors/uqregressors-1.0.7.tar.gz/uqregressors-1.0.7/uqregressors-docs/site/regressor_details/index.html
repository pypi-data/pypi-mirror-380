
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../examples/conformal_coverage_validation/">
      
      
        <link rel="next" href="../api/Bayesian/deep_ens/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Regressor Details - UQregressors</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      
  
  
    
    
  
  
  <style>:root{--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2024%2024%22%3E%3Cpath%20d%3D%22M13%2014h-2V9h2m0%209h-2v-2h2M1%2021h22L12%202z%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#uncertainty-estimation-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="UQregressors" class="md-header__button md-logo" aria-label="UQregressors" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            UQregressors
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Regressor Details
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="UQregressors" class="md-nav__button md-logo" aria-label="UQregressors" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    UQregressors
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quickstart
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Metrics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/saving_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Saving and Loading Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Validation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/conformal_coverage_validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Validating Coverage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Regressor Details
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Regressor Details
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#uncertainty-estimation-with-distributional-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Uncertainty Estimation with Distributional Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Uncertainty Estimation with Distributional Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-gaussian-process-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Standard Gaussian Process Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#black-box-matrix-multiplication-gaussian-process-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Black-Box Matrix Multiplication Gaussian Process Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monte-carlo-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Monte-Carlo Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Deep Ensemble
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#uncertainty-estimation-with-distribution-free-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Uncertainty Estimation with Distribution-free Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Uncertainty Estimation with Distribution-free Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#split-conformal-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Split Conformal Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-fold-cv-and-cv-minmax" class="md-nav__link">
    <span class="md-ellipsis">
      K-fold CV+ and CV-minmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conformal-quantile-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Conformal Quantile Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalized-conformal-prediction-conformal-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Normalized Conformal Prediction (Conformal Ensemble)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Bayesian
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            Bayesian
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Bayesian/deep_ens/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deep Ensembles
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Bayesian/dropout/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dropout
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Bayesian/gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gaussian Processes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Bayesian/bbmm_gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gaussian Process (Black Box Matrix-Matrix Inference)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Conformal
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Conformal
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Conformal/cqr/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Conformal Quantile Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Conformal/k_fold_cqr/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Fold Conformal Quantile Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Conformal/conformal_ens/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalized Conformal Ensemble
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Metrics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/plotting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Plotting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_6" >
        
          
          <label class="md-nav__link" for="__nav_4_6" id="__nav_4_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Utils
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_6">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Utils/activations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Activations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Utils/data_loader/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Loader
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Utils/file_manager/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    File Manager
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Utils/logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/Utils/torch_sklearn_utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Torch Sklearn Utils
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#uncertainty-estimation-with-distributional-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Uncertainty Estimation with Distributional Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Uncertainty Estimation with Distributional Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-gaussian-process-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Standard Gaussian Process Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#black-box-matrix-multiplication-gaussian-process-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Black-Box Matrix Multiplication Gaussian Process Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monte-carlo-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Monte-Carlo Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Deep Ensemble
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#uncertainty-estimation-with-distribution-free-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Uncertainty Estimation with Distribution-free Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Uncertainty Estimation with Distribution-free Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#split-conformal-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Split Conformal Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#k-fold-cv-and-cv-minmax" class="md-nav__link">
    <span class="md-ellipsis">
      K-fold CV+ and CV-minmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conformal-quantile-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Conformal Quantile Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#normalized-conformal-prediction-conformal-ensemble" class="md-nav__link">
    <span class="md-ellipsis">
      Normalized Conformal Prediction (Conformal Ensemble)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="uncertainty-estimation-methods">Uncertainty Estimation Methods</h1>
<h2 id="uncertainty-estimation-with-distributional-methods">Uncertainty Estimation with Distributional Methods</h2>
<p>For generating the uncertainty intervals returned by a function, we make a distinction between distributional methods and non-distributional methods. Distributional methods seek to return a distribution instead of a point prediction for each point, whereas non-distributional methods return an upper and lower bound with some confidence <span class="arithmatex">\(1-\alpha\)</span>. Many distributional methods exist, but the below methods were selected for their published performance on non-aerospace datasets.</p>
<h3 id="standard-gaussian-process-regression">Standard Gaussian Process Regression</h3>
<p>Gaussian process regression is perhaps one of the most established and widely used methods for regression with uncertainty quantification within the aerospace discipline. A brief introduction is given here to standard Gaussian process regression, with a full treatment in (<a href="http://www.gaussianprocess.org/gpml/">Rasmussen</a>). Gaussian processes model a distribution over candidate functions which fit the training data by assuming that the training outputs and the test outputs are drawn from a jointly distributed Multi-variate Gaussian. Neglecting measurement noise and assuming a zero mean prior for simplicity of explanation, we assume that the joint distribution of the training outputs, <span class="arithmatex">\(y_{tr}\)</span> and the testing outputs, <span class="arithmatex">\(y_{te}\)</span>, is given by:</p>
<div class="arithmatex">\[
\begin{bmatrix}
    f(X_{tr}) \\
    f(X_{te})
\end{bmatrix} = \mathcal{N}\left(0,
\begin{bmatrix}
    K &amp;  K_* \\
    K_*^{T} &amp; K_{**}
\end{bmatrix} \right)
\]</div>
<p>for covariance matrices given by some measure of similarity (kernel function: <span class="arithmatex">\(\text{cov}\)</span>) <span class="arithmatex">\(K=\text{cov}(X_{tr}, X_{tr}), K_* = \text{cov}(X_{tr}, X_{te}), K_{**} = \text{cov}(X_{te}, X_{te})\)</span>. After obtaining the data, <span class="arithmatex">\(f(X_{tr}) = y_{tr}\)</span>, we obtain the posterior distribution by conditioning this multivariate normal distribution:</p>
<div class="arithmatex">\[
y_{te} | y_{tr}, X_{tr}, X_{te} = \mathcal{N}\left(K_*K^{-1}y_{tr}, K_{**} - K_*K^{-1}K_*^T\right)
\]</div>
<p>As an intuitive explanation of this formula, the mean of this normal distribution is the outputs of the training data weighted by how similar their corresponding inputs are to the test input, while the variance is entirely dependent on this similarity. Similarity is determined by the kernel function mentioned above, which uses the kernel trick to evaluate the similarity of the input vectors in a much higher dimensional space to evaluate a rich measure of similarity. One of the most commonly used kernel functions is the Radial Basis Function (RBF), which is computed for the length scale parameter, <span class="arithmatex">\(l\)</span> as:</p>
<div class="arithmatex">\[
\text{cov}(x_i, x_j) = \exp\left(-\frac{||x_i-x_j||^2_2}{2l^2}\right).
\]</div>
<p>This kernel function is used for several desirable properties; namely its infinite dimensional feature space, exponentially decaying value of similarity, and positive definiteness. One limitation of this kernel function is that it implicitly assumes that the underlying function is smooth, and can give uninformative measures of similarity in high dimension, where the Euclidean distance between points tends to concentrate.</p>
<p>This kernel is also strongly dependent on a reasonable choice of the length-scale parameter <span class="arithmatex">\(l\)</span>, which is often optimized by maximizing the log-likelihood of the training data, <span class="arithmatex">\(y\)</span> with gradient based methods. The log-likelihood is written as:</p>
<div class="arithmatex">\[
\log p(y_{tr} | X_{tr}, l) = -\frac{1}{2}y_{tr}^T K^{-1}y_{tr} - \frac{1}{2} \log|K| - C_1
\]</div>
<p>The first term of this measures the relation between the output values, and the relation expected by the covariance matrix structure. For example, if the covariance matrix predicts that y values should be highly correlated and they are not, the loss will be large. The second term is a measurement complexity term, which tries to drive the outputs of the kernel covariance lower. In the case of the RBF kernel, this corresponds to prioritizing smooth functions which still have a good data-fit. The constant, <span class="arithmatex">\(C_1\)</span> is not important for the hyperparameter optimization. In practice, a measurement noise term, <span class="arithmatex">\(\sigma_n^2\)</span> is co-optimized with <span class="arithmatex">\(l\)</span>, but it is not included here for simplicity of explanation.</p>
<p>Gaussian process regression with hyperparameter optimization can often fit low-dimensional smooth functions extremely well, but also has a time complexity of <span class="arithmatex">\(O(n^3)\)</span> to fit data arising from the need to invert the <span class="arithmatex">\(n \times n\)</span> covariance matrix. This makes Gaussian process regression a natural choice for datasets with only a few number of points. After being trained, the time complexity to perform inference on <span class="arithmatex">\(m\)</span> points is <span class="arithmatex">\(O(n^2m)\)</span>, meaning that Gaussian process regression can quickly become impractical for optimization routines or other use cases where the number of predictions required is extremely large. Typically, standard Gaussian process regression is impractical for datasets beyond a few thousand points.</p>
<h3 id="black-box-matrix-multiplication-gaussian-process-regression">Black-Box Matrix Multiplication Gaussian Process Regression</h3>
<p>The key reason why Gaussian Process Regressions have a large training time complexity is the requirement to invert the matrix <span class="arithmatex">\(K\)</span>. This is typically done using the Cholesky decomposition, but there exist other methods of approximate inversion which can significantly reduce the training time complexity. In particular, the black-box matrix multiplication variation of a GP uses <span class="arithmatex">\(T\)</span> iterations of the conjugate gradients algorithm with a low rank preconditioner in order to compute an approximate inversion in <span class="arithmatex">\(O(n^2T)\)</span> time, giving an exact solution if <span class="arithmatex">\(T=n\)</span> (<a href="https://arxiv.org/abs/1809.11165">Gardner</a>). Additionally, they have optimized their methodology for parallel computing hardware, with packaged code available in the python repository GPytorch. Since all of the deep learning models to follow are implemented for GPU hardware using PyTorch, this allows for a fair comparison of training and inference time.</p>
<h3 id="monte-carlo-dropout">Monte-Carlo Dropout</h3>
<p>Monte-Carlo dropout is a method of uncertainty quantification for arbitrary neural networks, which is a neural network approximation of Gaussian processes. Critically, because of the deep learning architecture, it can avoid the large time complexity of Gaussian Processes such that it can scale to extremely large datasets. To train a neural network with dropout, at each iteration, each neuron in the hidden layers is assigned a binary variable with probability <span class="arithmatex">\(p_{drop}\)</span> (typically near 0.1 or 0.2) of being 0. Each neuron assigned a value of 0 is dropped for that training pass, meaning that it simply returns a value of zero. The parameters are trained to minimize the mean squared error loss in this stochastic setting, where each iteration has a different combination of parameters. To perform inference, we perform <span class="arithmatex">\(T\)</span> passes through the network with the same dropout methodology as during training to obtain <span class="arithmatex">\(T\)</span> different samples of function values at each point. If desired, a distribution (typically Gaussian) can be fit to the outputs at each point. The theoretical results of (<a href="https://arxiv.org/abs/1506.02142">Gal</a>) demonstrate that this approximates Bayesian Neural networks, where a distribution is placed over the weights within the neural network. However, Bayesian neural networks require a doubling of the parameters of a standard neural network, or one with dropout, and can also have undesirable optimization landscapes with poor convergence properties. The empirical results of (<a href="https://arxiv.org/abs/1506.02142">Gal</a>) demonstrate comparable or better performance of Monte-Carlo dropout as compared to Bayesian Neural Networks on a variety of datasets, which is why Monte-Carlo dropout is implemented over Bayesian Neural Networks for this comparison.</p>
<p>While Monte-Carlo dropout is an extremely efficient method of obtaining uncertainty distributions in addition to a mean prediction, the outputs are random, so they can be non-smooth, which can cause difficulties for optimization. Additionally, each training iteration optimizes different network parameters for a new random realization of the mean function, so the accuracy of this method may be less than that of traditional neural networks without dropout. Key benefits of Monte-Carlo dropout include that there are no required assumptions on the distribution of uncertainty, and that implementation requires minimum modification to standard neural networks.</p>
<h3 id="deep-ensemble">Deep Ensemble</h3>
<p>Deep Ensembles, proposed by (<a href="https://arxiv.org/abs/1612.01474">Lakshminarayanan</a>), are another relatively inexpensive approximation of Bayesian Neural Networks, which involve training several (i.e., an ensemble) of models and combining their outputs. Each regressor within the ensemble assumes that each point is an observation from a Gaussian distribution, and is trained to predict both the mean and standard deviation of an output given an input. During training, the negative log likelihood of the data given the predicted mean and standard deviation is minimized. During inference, the mean and variance from each regressor are combined by assuming that the overall distribution of uncertainty is also Gaussian with the mean and variance of the mixture. For <span class="arithmatex">\(K\)</span> models, the mean and standard deviation are calculated as:</p>
<div class="arithmatex">\[
\mu_*(x) = K^{-1}\sum_{k=1}^K \mu_{\theta_k}(x), \sigma_*^2(x)=K^{-1}\sum_{k=1}^K(\sigma_{\theta_m}^2(x) + \mu_{\theta_m}^2(x)) - \mu_*^2(x)
\]</div>
<p>When implementing this model, it is useful to let the outputs of each model predict the log of the standard deviation instead of the standard deviation itself for numerical stability and positive definiteness of the standard deviation. Additionally, one variant of this model is to randomly select a different subset of the training examples for every regressor to prioritize diversity in prediction and calibrate the uncertainty intervals towards difficulty of prediction. However, this was not necessary in order to produce well-calibrated intervals during the empirical studies by (<a href="https://arxiv.org/abs/1612.01474">Lakshminarayanan</a>). In these studies, Deep Ensembles were shown to perform similarly or slightly worse as compared to Monte-Carlo dropout on a variety of public regression datasets on the basis of RMSE, but to significantly outperform Monte-Carlo dropout on the basis of negative log likelihood evaluated on the test set. This implies that deep ensembles output well calibrated uncertainty distributions, as the log-likelihood will penalize both poor fit and either over or under confident uncertainty intervals. This is explained in the paper by the fact that deep ensembles explicitly optimize for negative log likelihood during training as opposed to mean squared error.</p>
<p>Deep ensembles are able to produce smooth uncertainty estimates in a well-calibrated sense, but assume that the uncertainty follows a known distribution. In many cases, a Gaussian distribution may be a good approximation to the distribution of uncertainty, unless prior knowledge about the problem indicates that a different distribution should be used. Additionally, Deep ensembles require fitting <span class="arithmatex">\(K\)</span> estimators, although they can be fit in parallel to accomplish the same running time as standard neural networks. Ensemble predictors are generally desirable, as they can reduce the variance of the output function found through training, particularly for sparse datasets in high dimensions. More detail on the variance reduction of ensemble predictors is described in Variance reduction with ensemble methods section below.</p>
<h2 id="uncertainty-estimation-with-distribution-free-methods">Uncertainty Estimation with Distribution-free Methods</h2>
<p>While assuming a Gaussian distribution on uncertainty is often a reasonable assumption, this may not hold for certain processes or certain datasets. As a simple example, if we consider the drag of an airfoil near stall, very small changes in the angle of attack can result in large drops in lift. Therefore, slight modeling errors will result in a non-Gaussian distribution of error, where errors are more likely to be distributed below the prediction, to have a long-tailed distribution, and perhaps even demonstrate bi-modality. This can become especially important if the observations come from the physical world, as in a wind tunnel test, which may have measurement noise which does not demonstrate a Gaussian distribution. For this reason, a method of quantifying uncertainty which does not assume a distribution on uncertainty is included in these comparisons. A method which is rising in popularity in the machine learning community to accomplish distribution-free uncertainty estimation is conformal prediction. An overview of the simplest method of conformal prediction, split conformal prediction, followed by a description of more sophisticated methods.</p>
<p>The key idea of conformal prediction is to return a set, <span class="arithmatex">\(\hat C (X_{te_i})\)</span>, for each test input, such that:</p>
<div class="arithmatex">\[
\mathbb{P}(y_{te_i} \in \hat C(X_{te_i})) \geq 1- \alpha,
\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is a user-specified error rate. In this way, the model developer has a <span class="arithmatex">\(1-\alpha\)</span> level of confidence that the returned intervals contain the true response values (<a href="https://www.alrw.net/">Vovk2022</a>). While several methods of conformal prediction exist, we explain the process with the simplest implementation: split conformal prediction.</p>
<h3 id="split-conformal-prediction">Split Conformal Prediction</h3>
<p>Conformal prediction returns statistically valid sets with minimal assumptions on the data and no assumptions on the predictor by leveraging ideas from rank statistics. First, the available data is partitioned into a training, <span class="arithmatex">\(S_1\)</span>, and calibration, <span class="arithmatex">\(S_2\)</span>, set. A point predictor, <span class="arithmatex">\(\hat{f}\)</span>, is fit on the training data, and absolute residuals are found on the calibration set. The absolute residuals are of the form:</p>
<div class="arithmatex">\[
R_i=|y_i-\hat{f}(X_i)|, \ i \in S_2.
\]</div>
<p>For a feature-response pair in the test set with residual <span class="arithmatex">\(R_{te_i}\)</span>, we expect that the rank (or the sorted position) of the calibration residuals and the test residual is evenly distributed. This assumes that the new residual is exchangeable with the calibration residuals. We can then form the rank-adjusted quantile:</p>
<div class="arithmatex">\[
\hat{q} = \text{the } \frac{\lceil(n_2+1)(1-\alpha)\rceil}{n_2} \text{ quantile of } R_i, \ i \in S_2,
\]</div>
<p>where the term <span class="arithmatex">\(\frac{(n_2 + 1)}{n_2}\)</span> is a finite sample correction such that:</p>
<div class="arithmatex">\[
\mathbb{P}(R_{te_i} \leq \hat{q})  \geq 1-\alpha.
\]</div>
<p>A prediction interval satisfying the above can then be formed as:</p>
<div class="arithmatex">\[
\hat C(X_{te_i}) = [\hat{f}(X_{te_i}) - \hat{q},\hat{f}(X_{te_i})+\hat{q}],
\]</div>
<p>The only assumption made about the data and the predictor is that the residuals of the calibration set and the residuals of new test points are exchangeable, meaning that their joint probability distribution is the same regardless of their permutation. Split conformal prediction is a simple method of generating statistically valid prediction intervals, but it requires partitioning the available data into a training and calibration set, which means that not all of the available data is used to train the model (low statistical efficiency) (<a href="https://arxiv.org/abs/2107.07511">Angelopoulos2023</a>).</p>
<p>The size of the calibration set also has a substantial impact on the performance of split conformal prediction. Although the average coverage is guaranteed to be <span class="arithmatex">\(1-\alpha\)</span>, the total coverage over different random splits of the data follows a beta distribution with approximate variance <span class="arithmatex">\(\frac{\alpha(1-\alpha)}{n_2+2}\)</span>. Therefore, if <span class="arithmatex">\(n_2\)</span> is small, a given split of the available data can substantially undercover. In practical implementations of split conformal prediction, it is recommended to use calibration sets of at least 100 feature response pairs. To remedy the low statistical efficiency of this method, ensemble regression can be used through the methodology of K-fold CV+.</p>
<h3 id="k-fold-cv-and-cv-minmax">K-fold CV+ and CV-minmax</h3>
<p>The exchangeability requirement mandates determining the residuals on a set of data not used to train the model. K-fold CV is a method of conformal prediction based on ideas from cross-validation, where the data is split into <span class="arithmatex">\(K\)</span> folds, and <span class="arithmatex">\(K\)</span> predictors are trained on each permutation of <span class="arithmatex">\(K-1\)</span> folds. For each predictor, calibration residuals are evaluated on the fold that is left out from training. In this way, a set of calibration residuals which are exchangeable with the residuals from unseen data can be generated while still using a majority of data for model training. The interval half-width can then be constructed as in split conformal prediction, using the combined set of calibration residuals from each fold (<a href="https://arxiv.org/abs/1905.02928">barber2020</a>).</p>
<p>Two methods of generating prediction intervals with K-fold CV provide statistical guarantees. CV+ centers the prediction interval around the average of the <span class="arithmatex">\(K\)</span> predictors:</p>
<div class="arithmatex">\[
\hat C(X_{n+1}) = [\hat{f}_{mean}(X_{n+1}) - \hat{q},\hat{f}_{mean} (X_{n+1})+\hat{q}],  \ \hat{f}_{mean} (X_{n+1}) = \frac{1}{K} \sum_{i=1}^K \hat{f}_i(X_{n+1}),
\]</div>
<p>whereas CV-minmax constructs bounds around the minimum and maximum of the <span class="arithmatex">\(K\)</span> predictors:</p>
<div class="arithmatex">\[
\hat C(X_{n+1}) = [\hat{f}_{low}(X_{n+1}) - \hat{q},\hat{f}_{high} (X_{n+1})+\hat{q}],  \ \hat{f}_{low} (X_{n+1}) = \text{min}(\hat{f}_i), \ \hat{f}_{high} (X_{n+1}) = \text{max}(\hat{f}_i), \ i \in K.
\]</div>
<p>In exchange for higher statistical efficiency, CV+ has a relaxed statistical guarantee. The average coverage guarantee of CV+ is <span class="arithmatex">\(1-2\alpha - \sqrt{\frac{2}{n_2}}\)</span>, while the average coverage guarantee of CV-minmax is <span class="arithmatex">\(1-\alpha\)</span>. In practice, however, CV+ generally has an average coverage of <span class="arithmatex">\(1-\alpha\)</span>, while the coverage of CV-minmax is typically larger than <span class="arithmatex">\(1-\alpha\)</span>.</p>
<p>Thus, while CV+ does not enjoy the same guarantee as split conformal prediction, it typically performs well in practice (<a href="https://arxiv.org/abs/1905.02928">barber2020</a>). CV-minmax always returns prediction intervals which are wider than CV+, and tends to overcover in practice.</p>
<p>An additional downside of generic split conformal prediction is that the interval half-width, <span class="arithmatex">\(\hat{q}\)</span>, is constant across the output space, meaning that the intervals will generally be too small for input values corresponding to outputs which are difficult to predict, and too large for easily predictable outputs, despite maintaining a <span class="arithmatex">\(1-\alpha\)</span> average coverage (<a href="https://arxiv.org/abs/1905.03222">Romano2019</a>). A constant width is uninformative when it comes to adaptive sampling, so different methods of conformal prediction modify the methodology to generate adaptive interval widths.</p>
<h3 id="conformal-quantile-regression">Conformal Quantile Regression</h3>
<p>Conformal quantile regression produces locally adaptive prediction intervals by wrapping conformal prediction around an interval predictor instead of a point predictor. Instead of fitting a predictor to the mean of the available data, two predictors are fit to the <span class="arithmatex">\(\alpha/2\)</span> and the <span class="arithmatex">\(1-\alpha/2\)</span> quantiles of the data. This alone does not provide any statistical guarantees, but the split or CV conformal prediction methods can then be applied to the quantile estimators to produce modifications to the interval width that satisfy the desired statistical guarantee (<a href="https://arxiv.org/abs/1905.03222">Romano2019</a>).</p>
<p>For many regression methods, fitting predictors to quantiles of the data can be done by modifying the loss function. In this work, neural networks are used with a tilted loss function to predict quantiles. The neural network predicts a quantile, <span class="arithmatex">\(\tau\)</span> by minimizing the loss, <span class="arithmatex">\(L\)</span> over all points, where:</p>
<div class="arithmatex">\[
L(X_i)=\max\left(-\tau (\hat{f}(X_i)-Y_i), (1-\tau)(\hat{f}(X_i)-Y_i)\right).
\]</div>
<p>A simple synthetic dataset to demonstrate the use of wrapping a quantile regressor with conformal prediction was created. A set of 1,000 data pairs was generated with:</p>
<div class="arithmatex">\[
y=0.1(\sin(2\pi x) + 0.4 \epsilon (0.1 + x)), \ x \in[0,1], \ \epsilon \sim \mathcal{N}(0, 1).
\]</div>
<p>Defining <span class="arithmatex">\(\alpha=0.1\)</span>, neural networks were fitted to the 0.05 and 0.95 quantiles using 250 training points. Split conformal prediction was used with a calibration set size of 250. Finally, the coverage was evaluated on a set of 500 test points. The prediction intervals and test points are shown in the figure below. The conformal prediction interval bounds are slightly larger than the prediction made by the quantile neural network, which ensures the statistical guarantee is met. The coverage was evaluated over 10,000 random splits of the data, shown in the histogram below. The histogram demonstrates that the average coverage over these trials meets the desired coverage of 0.9. The variance of the coverage could be reduced by increasing the number of calibration points used.</p>
<p><img alt="Prediction interval visualization" src="../sin_function.png" />
<img alt="Coverage evaluated over 10000 trials" src="../sin_function_coverage.png" /></p>
<p>By wrapping quantile regressors with CV+ or CV-minmax, statistically-valid and locally-adaptive prediction intervals can be generated with relatively high statistical efficiency compared to split conformal prediction. The results presented in this work use quantile regressors wrapped with CV+. K-fold CV+ enables locally adaptive interval widths with relatively high statistical efficiency, but will still struggle to produce well-calibrated with datasets with less than a couple hundred points, as the small calibration set size will result in a high-variance distribution of coverage and interval widths. Additionally, since optimization of the mean function does not occur directly during training, the RMSE of this method as compared to other methods may be higher.</p>
<h3 id="normalized-conformal-prediction-conformal-ensemble">Normalized Conformal Prediction (Conformal Ensemble)</h3>
<p>Using quantile regression is an effective method of obtaining locally adaptive interval widths when there are enough training points to give the model information about quantiles of the dataset. Another method of conformal prediction is known as normalized conformal prediction, which follows the same methodology as split conformal prediction, but constructs adaptive intervals based on normalized non-conformity scores. The key idea is that instead of choosing <span class="arithmatex">\(\hat{q} = \text{the } \frac{\lceil(n_2+1)(1-\alpha)\rceil}{n_2} \text{ quantile of } R_i, \ i \in S_2\)</span>, we choose</p>
<div class="arithmatex">\[
\hat{q} = \text{the } \frac{\lceil(n_2+1)(1-\alpha)\rceil}{n_2} \text{ quantile of } \frac{R_i}{\sigma(X_i)}, \ i \in S_2,
\]</div>
<p>where <span class="arithmatex">\(\sigma(X_i)\)</span> is an estimator of the difficulty of predicting <span class="arithmatex">\(y_i\)</span>. For points which are difficult to predict, <span class="arithmatex">\(\sigma(X_i)\)</span> should be large and vice versa, such that the normalized residuals, <span class="arithmatex">\(\frac{R_i}{\sigma(X_i)}\)</span>, are approximately equal. To construct the interval for a point in the test interval, we first compute <span class="arithmatex">\(\sigma(X_{te_i})\)</span>, and then construct the interval as:</p>
<div class="arithmatex">\[
\hat C(X_{te_i}) = [\hat{f}(X_{te_i}) - \hat{q}\sigma(X_{te_i}),\hat{f}(X_{te_i})+\hat{q}\sigma(X_{te_i})]
\]</div>
<p>In this way, if <span class="arithmatex">\(\sigma\)</span> is a well calibrated estimate of prediction difficulty, conformal prediction will return adaptive interval widths. While this method trains a mean regressor and can often be easier to implement than conformal quantile regression, it loses statistical guarantees without strong assumptions on the difficulty function, <span class="arithmatex">\(\sigma\)</span>. However, this method has been shown empirically to provide good coverage (<a href="https://arxiv.org/abs/2402.14080">Nolte</a>). Several methods exist to construct the difficulty function. One intuitive method is to train an ensemble of regressors, and to use the variance of the ensemble predictions at any point as an estimate of the prediction difficulty. This idea has close ties to the method of deep ensembles, where we assume that if models with different training trajectories provide vastly different outputs for a given input, then the uncertainty of the overall ensemble (the mean of their predictions) should also be large.</p>
<p>To fit with the conformal prediction methodology, we must split the training dataset into a training and calibration set in order to use this method. In contrast to conformal K-fold quantile regression, we cannot use the K-fold training split here to achieve high statistical efficiency because in order to evaluate the normalized residual, we must calculate <span class="arithmatex">\(\sigma\)</span> for each point in the calibration set, which must be representative of the ensemble variance of a test point. Therefore, before beginning training, we split the data into a training and calibration set (typically an 80-20 split), then train each of <span class="arithmatex">\(K\)</span> regressors on the training set. For each point in the calibration set and each regressor <span class="arithmatex">\(k\)</span>, we evaluate <span class="arithmatex">\(\mu_{\theta_k}\)</span>. We take the ensemble prediction <span class="arithmatex">\(\mu_*\)</span> to be the mean of the individual predictions, and <span class="arithmatex">\(\sigma(X_{cal_i})\)</span> to be the variance of the individual predictions. We then construct the normalized residuals for each point in the calibration set and store these values. To perform inference, we use the same methodology as on the calibration set to evaluate <span class="arithmatex">\(\mu_*(X_{te_i})\)</span>, and <span class="arithmatex">\(\sigma(X_{te_i})\)</span>, and construct the conformal interval as above.</p>
<p>Normalized conformal prediction is simple to implement, and can provide adaptive intervals satisfying empirical coverage, but suffers from low statistical efficiency as the K-fold methodology cannot be used. Therefore, this method will likely perform poorly on datasets smaller than a few hundred points.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>