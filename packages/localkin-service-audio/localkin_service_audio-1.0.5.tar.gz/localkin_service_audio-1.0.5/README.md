# LocalKin Service Audio ğŸµ

[![PyPI version](https://badge.fury.io/py/localkin-service-audio.svg)](https://pypi.org/project/localkin-service-audio/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/âš¡-uv-4c1d95)](https://github.com/astral-sh/uv)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Complete Voice AI Platform: STT, TTS & LLM Integration

**LocalKin Service Audio** is a complete **voice AI platform** featuring **Speech-to-Text (STT)**, **Text-to-Speech (TTS)**, and **Large Language Model (LLM) integration**. An intuitive **local audio** tool inspired by **Ollama's** simplicity - perfect for **voice-based conversational AI** with both CLI and modern web interface support.

## âœ¨ Key Features

- **ğŸš€ Fast Startup**: Instant application launch with lazy loading architecture
- **âš¡ Maximum Performance**: whisper.cpp integration for up to 50x faster transcription
- **ğŸ¯ Multiple STT Engines**: OpenAI Whisper, faster-whisper with VAD, whisper.cpp (C++), and Hugging Face models
- **ğŸ”Š Multiple TTS Engines**: Native OS TTS, SpeechT5, Bark, Kokoro, and XTTS models
- **ğŸŒ REST API Server**: Run models as API servers with automatic endpoints
- **ğŸ’» Modern Web Interface**: Beautiful, responsive web UI with file upload, voice selection, and dynamic model discovery
- **ğŸ¤– LLM Integration**: Voice-based conversational AI with Ollama models for intelligent responses
- **ğŸ­ Voice Selection**: Multiple voice options for TTS models (Kokoro, XTTS, SpeechT5)
- **ğŸ“„ File Upload Support**: Upload text files (.txt, .md, .rtf) for TTS synthesis
- **ğŸ” Dynamic API Discovery**: Automatically finds and uses running API servers
- **ğŸ“¦ Smart Model Management**: Auto-pull models when needed, intelligent caching
- **ğŸ’¾ Persistent Cache**: Local model storage with size tracking and cleanup
- **ğŸ”„ Auto-Pull**: Models automatically download when running if not cached
- **ğŸ“Š Real-Time Status**: Live model status tracking with emoji indicators
- **ğŸ” Process Monitoring**: `kin audio ps` shows all running servers and their status
- **ğŸ“ˆ Model Transparency**: STT/TTS commands display detailed model information and statistics
- **âš¡ Performance Optimized**: Memory-efficient with GPU acceleration support
- **ğŸ¨ Professional Results**: High-quality audio processing with fine-tuned control
- **ğŸŒ CLI & Web**: Both command-line interface and modern web interface
- **ğŸ”§ Modular Architecture**: Clean, maintainable codebase with separated concerns

## ğŸš€ Quick Start

### Recommended: Install with uv (Best for Kokoro TTS)

Using `uv` ensures you have proper Python environment with LZMA support for Kokoro TTS:

```bash
# Install uv (fast Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install LocalKin Service Audio
uv pip install localkin-service-audio

# Start using it!
kin --help

# Try Kokoro TTS (requires LZMA support)
kin audio run kokoro-82m --port 8001
```

### Alternative: Install with pip

If you prefer traditional pip (may have LZMA issues with pyenv):

```bash
# Install from PyPI
pip install localkin-service-audio

# or upgrade
pip uninstall localkin-service-audio
pip install localkin-service-audio --upgrade --no-cache-dir

# Start using it!
kin --help

# If you get LZMA errors with Kokoro, see troubleshooting below or use uv
```

**ğŸ’¡ Pro Tip:** If you encounter "Could not import module 'pipeline'" errors with Kokoro TTS, use the `uv` installation method or see [Troubleshooting](#kokoro-tts-could-not-import-module-pipeline-error).

### Basic Usage

```bash
# Check version and help
kin --version
kin --help

# List all available models with status
kin audio models

# Transcribe audio files
kin audio transcribe audio.wav                    # Auto-select best engine
kin audio transcribe audio.wav --engine whisper-cpp --model_size tiny  # Ultra-fast
kin audio transcribe audio.wav --engine faster --vad                   # With VAD

# Real-time listening with TTS
kin audio listen --engine whisper-cpp --tts --tts-model native

# Voice AI with streaming LLM responses
kin audio listen --llm ollama --tts --stream

# Synthesize speech
kin audio tts "Hello world" --model kokoro-82m

# Start web interface
kin web
```

## ğŸ¯ Supported Models

### STT Models - Speech-to-Text

#### ğŸš€ Ultra-Fast whisper.cpp Models (Recommended)
```bash
# Download models
python scripts/download_whisper_cpp_models.py --list
python scripts/download_whisper_cpp_models.py tiny base small

# Usage
kin audio transcribe audio.wav --engine whisper-cpp --model_size tiny  # 50x faster!
kin audio listen --engine whisper-cpp --model_size base               # Real-time
```

**Performance:** Up to 50x faster than OpenAI Whisper with low memory usage.

#### âš¡ Fast Whisper Models (Balanced)
```bash
# Usage (auto-downloads on first use)
kin audio transcribe audio.wav --engine faster --vad  # 4x-32x faster with VAD
kin audio transcribe audio.wav --model faster-whisper-tiny
```

#### ğŸ  Local Whisper Models (Compatible)
```bash
# Built-in, no download needed
kin audio transcribe audio.wav --engine openai --model_size base
kin audio transcribe audio.wav  # Auto-selects best available
```

### TTS Models - Text-to-Speech

#### ğŸš€ API Server Models (Recommended for Production)
```bash
# Start API server (loads once, instant responses)
kin audio run kokoro-82m --port 8001      # High quality, 320MB
kin audio run speecht5-tts --port 8002    # Fast, 130MB
kin audio run xtts-v2 --port 8003         # Voice cloning, 1.8GB

# Use API
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello world"}' \
     --output speech.wav
```

#### ğŸ’» CLI Models (Development/Testing)
```bash
# Direct CLI usage (loads on-demand)
kin audio tts "Hello world" --model kokoro-82m
kin audio tts "Hello world" --model native        # Fastest, built-in
```

### ğŸ¤– LLM Integration

#### Voice-Based Conversational AI
```bash
# Requires Ollama running
kin audio listen --llm ollama --tts                    # Full voice AI
kin audio listen --llm ollama --llm-model qwen3:14b    # Custom model
kin audio listen --llm ollama --tts --stream           # Streaming responses
kin audio listen --engine whisper-cpp --model_size small --tts --tts-model kokoro-82m --vad --llm ollama --llm-model deepseek-r1:14b --stream                               # Custom models with Streaming
```

**Streaming Mode**: Add `--stream` for real-time LLM responses that speak as they generate, creating more natural conversational flow.

**Conversation Context**: LLM maintains conversation history during your session, allowing for contextual follow-up questions and natural dialogue flow.

## ğŸ“¦ Installation & Setup

### Prerequisites
- **Python 3.10+** (required for optimal performance)
- **Ollama** (optional, for LLM integration)
- **FFmpeg** (for audio processing)

### Recommended: Install with uv (Best Compatibility)
```bash
# Install uv (fast Python package manager with proper environment handling)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install LocalKin Service Audio
uv pip install localkin-service-audio

# Verify installation
kin --version
```

**Why uv?** It ensures proper Python environment with LZMA support needed for Kokoro TTS, avoiding common pyenv-related issues.

### Alternative: Install from PyPI
```bash
# Install from PyPI (may have LZMA issues with pyenv)
pip install localkin-service-audio

# Verify installation
kin --version
```

### Install from Source (For Contributors/Advanced Users)
```bash
# Clone repository for development or advanced setup
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Install with uv
uv sync
```

### whisper.cpp Setup (Optional, for Maximum Performance)
```bash
# Build whisper.cpp (requires CMake, Make, C++ compiler)
./scripts/build_whisper_cpp.sh

# Download models
python scripts/download_whisper_cpp_models.py tiny base small
```

### Ollama Setup (Optional, for LLM Integration)
```bash
# Install Ollama
brew install ollama  # macOS
# OR visit https://ollama.ai for other platforms

# Start Ollama service
ollama serve

# Pull recommended models
ollama pull qwen3:14b
```

## âš¡ Performance & Benchmarks

### Engine Comparison

| Engine | Speed | Memory | VAD | GPU Support | Best For |
|--------|-------|--------|-----|-------------|----------|
| **whisper.cpp** | **50x** | **Low** | âŒ | âŒ | **Maximum Performance** |
| **faster-whisper** | 4x-32x | Medium | âœ… | âœ… | Balanced speed/quality |
| **OpenAI Whisper** | 1x | High | âŒ | âœ… | Compatibility |

### Hardware Recommendations

- **Basic Usage**: 8GB RAM, any CPU
- **High-Quality Models**: 16GB+ RAM, GPU recommended
- **whisper.cpp**: Works on any hardware, best performance
- **Real-time Applications**: Use whisper.cpp for lowest latency

## ğŸŒ REST API Reference

### Running API Servers
```bash
# STT API server
kin audio run whisper-cpp-tiny --port 8000

# TTS API servers
kin audio run kokoro-82m --port 8001
kin audio run speecht5-tts --port 8002
```

### API Endpoints

#### STT Endpoint
```bash
# Transcribe audio
curl -X POST "http://localhost:8000/transcribe" \
     -F "file=@audio.wav" \
     -F "language=en"
```

#### TTS Endpoints
```bash
# Synthesize speech
curl -X POST "http://localhost:8001/synthesize" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello world"}' \
     --output speech.wav
```

## ğŸ’¾ Cache Management

### Cache Commands
```bash
# Check cache status
kin audio cache info

# Clear specific model
kin audio cache clear whisper-tiny-hf

# Clear all cached models
kin audio cache clear
```

### Auto-Pull Behavior
Models are automatically downloaded when first used. No manual intervention required!

## ğŸ”§ Troubleshooting

### Common Issues

#### "Model not found" Error
```bash
# Check available models
kin audio models

# For Ollama models, ensure Ollama is running
ollama serve

# Pull the model first
kin audio pull whisper-base
```

#### Audio File Issues
```bash
# Ensure audio files are in supported formats: WAV, MP3, FLAC, OGG
# For best results, use 16-bit WAV at 16kHz sample rate
```

#### whisper.cpp Library Issues
```bash
# If you get library loading errors, rebuild whisper.cpp
./scripts/build_whisper_cpp.sh
```

#### Kokoro TTS "Could not import module 'pipeline'" Error
If you get this error when trying to use Kokoro TTS:
```
ERROR: Could not import module 'pipeline'. Are this object's requirements defined correctly?
```

This means your Python installation is missing LZMA compression support. Here are the solutions:

**Option 1: Use System Python (Recommended - Quickest)**
```bash
# System Python has LZMA support built-in
/usr/bin/python3 -m pip install --user localkin-service-audio

# Add to your PATH or create an alias
echo 'alias kin="/usr/bin/python3 -m localkin_service_audio.cli"' >> ~/.zshrc
source ~/.zshrc

# Now run normally
kin audio run kokoro-82m --port 8001
```

**Option 2: Use uv with System Python**
```bash
# Use uv with system Python (keeps LZMA support)
/usr/bin/python3 -m pip install uv

# Create a virtual environment with system Python
/usr/bin/python3 -m venv ~/.venv/localkin
source ~/.venv/localkin/bin/activate

# Install with uv
uv pip install localkin-service-audio

# Run kokoro TTS
kin audio run kokoro-82m --port 8001
```

**Option 3: Use SpeechT5 Instead (No Python changes needed)**
```bash
# SpeechT5 works without LZMA - use with your current Python
kin audio run speecht5-tts --port 8001
```

**Option 4: Fix pyenv Python (Advanced - requires reinstalling all packages)**
```bash
# Only if you really want to fix pyenv Python
# WARNING: This removes all installed packages!

# Install LZMA library first
brew install xz

# List your packages to reinstall later
pip freeze > requirements_backup.txt

# Reinstall Python with LZMA support
pyenv uninstall 3.10.0
pyenv install 3.10.0

# Reinstall packages
pip install -r requirements_backup.txt
```

## ğŸ¤ Contributing

### Development Setup
```bash
# Clone repository
git clone https://github.com/LocalKinAI/localkin-service-audio.git
cd localkin-service-audio

# Install in development mode
uv sync --dev

# Run tests
uv run pytest

# Run linting
uv run ruff check .
```

### Adding New Models
See the [model configuration guide](docs/model-configuration.md) for details on adding new STT/TTS models.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI** for the Whisper model
- **ggerganov** for whisper.cpp
- **SYSTRAN** for faster-whisper
- **Hugging Face** for model hosting
- **Ollama** for the inspiration and local AI ecosystem

---

**ğŸ‰ Ready to get started with local audio AI? Install LocalKin Service Audio and choose your preferred interface!**
