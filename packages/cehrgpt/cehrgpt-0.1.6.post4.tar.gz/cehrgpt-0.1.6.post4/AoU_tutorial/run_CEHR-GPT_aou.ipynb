{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1,2,4 maximum cpu 96 ram 624\n",
    "# step 3,5 training cpu 32 ram 120 enable gpu nvidia tesla v100 gpus 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Download omop table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir linear_prob\n",
    "!mkdir aou_cehrgpt\n",
    "!mkdir allofus_omop_v8\n",
    "!pip install cehrbert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "# Get the BigQuery curated dataset for the current workspace context.\n",
    "CDR = os.environ['WORKSPACE_CDR']\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "local_folder = \"allofus_omop_v8\"\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    " \n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('BigQuery with Spark') \\\n",
    "    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.24.2') \\\n",
    "    .config('spark.driver.memory', '24g') \\\n",
    "    .config('spark.executor.cores', '32') \\\n",
    "    .config('spark.executor.memory', '10g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if we set those properties correctly\n",
    "print(\"spark.driver.memory: \" + spark.conf.get(\"spark.driver.memory\"))\n",
    "print(\"spark.executor.cores: \" + spark.conf.get(\"spark.executor.cores\"))\n",
    "print(\"spark.executor.memory: \" + spark.conf.get(\"spark.executor.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for omop_table in [\n",
    "    \"person\", \"visit_occurrence\", \"drug_exposure\", \n",
    "    \"procedure_occurrence\", \"condition_occurrence\", \"measurement\",\n",
    "    \"concept\", \"concept_ancestor\", \"concept_relationship\", \"death\"\n",
    "]:\n",
    "\n",
    "    print(f\"converting {omop_table} now\")\n",
    "    omop_table_df = spark.read.format('bigquery') \\\n",
    "    .option('table', f'{CDR}.{omop_table}') \\\n",
    "    .load()\n",
    "    omop_table_df.write.mode(\"overwrite\").parquet(os.path.join(local_folder, omop_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########SPARK process for preparing training data\n",
    "```bash\n",
    "\n",
    "# 1. Set all configs in SPARK_SUBMIT_OPTIONS \n",
    "export SPARK_SUBMIT_OPTIONS=\"--master local[16] --driver-memory 16g --executor-memory 36g --executor-cores 4 --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.coalescePartitions.enabled=true --conf spark.serializer=org.apache.spark.serializer.KryoSerializer\"\n",
    "\n",
    "```\n",
    "\n",
    "# 2. do this to find where python sitepackages lives:\n",
    "\n",
    "ls -al ~/.\n",
    "\n",
    "\n",
    "# 3. Generate a list of concepts to use (qualified_concept_list folder - concept list should sit under allofus_omop_v8)\n",
    "\n",
    "```bash\n",
    "spark-submit $SPARK_SUBMIT_OPTIONS ~/.local/lib/python3.10/site-packages/cehrbert_data/apps/generate_included_concept_list.py \\\n",
    "    -i allofus_omop_v8 \\\n",
    "    -o allofus_omop_v8 \\\n",
    "    --min_num_of_patients 100\n",
    "```\n",
    "\n",
    "# 4. You need to make train and test folders in patient_sequence folder before generating training data.\n",
    "SEQ_DIR    = Path(\"allofus_omop_v8/patient_sequence/patient_sequence\")\n",
    "TRAIN_DIR  = SEQ_DIR / \"train\"\n",
    "TEST_DIR   = SEQ_DIR / \"test\"\n",
    "\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 5. run full download (first go to workspace folder by cd...) - generate training data\n",
    "\n",
    "```bash\n",
    "spark-submit $SPARK_SUBMIT_OPTIONS ~/.local/lib/python3.10/site-packages/cehrbert_data/apps/generate_training_data.py \\\n",
    "    --input_folder allofus_omop_v8 \\\n",
    "    --output_folder allofus_omop_v8/patient_sequence \\\n",
    "    -iv \\\n",
    "    -ip \\\n",
    "    --gpt_patient_sequence \\\n",
    "    --include_concept_list \\\n",
    "    --include_inpatient_hour_token \\\n",
    "    --att_type day \\\n",
    "    --inpatient_att_type day \\\n",
    "    --should_construct_artificial_visits \\\n",
    "    --disconnect_problem_list_records \\\n",
    "    --include_death \\\n",
    "    --domain_table_list condition_occurrence procedure_occurrence drug_exposure\n",
    "```\n",
    "\n",
    "# If there is memory error: To increase SPARK_EXECUTOR_MEMORY reduce SPARK_MASTER local[64] to local[16]. Since you have 2 cores, you have 8 ***available. 8xSPARK_EXECUTOR_MEMORY should not exceed RAM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. CEHR-GPT training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p /home/jupyter/workspaces/ehrcancermodelevaluation/allofus_omop_v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r {my_bucket}/allofus_omop_v8 /home/jupyter/workspaces/ehrcancermodelevaluation/allofus_omop_v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "# Get the BigQuery curated dataset for the current workspace context.\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    " \n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('BigQuery with Spark') \\\n",
    "    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.24.2') \\\n",
    "    .config('spark.driver.memory', '24g') \\\n",
    "    .config('spark.executor.cores', '32') \\\n",
    "    .config('spark.executor.memory', '10g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir aou_cehrgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/knatarajan-lab/cehrgpt.git\n",
    "# pip install .\n",
    "# training loss needs to be under 3-4\n",
    "# with learning rate 0.0002, training loss was 6.7\n",
    "# reduced learning rate to 5e-5 and added logging_step 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################training\n",
    "export CEHR_GPT_MODEL_DIR=/home/jupyter/workspaces/ehrcancermodelevaluation/aou_cehrgpt\n",
    "export CEHR_GPT_DATA_DIR=/home/jupyter/workspaces/ehrcancermodelevaluation/allofus_omop_v8/allofus_omop_v8\n",
    "export TRANSFORMERS_VERBOSITY=info\n",
    "\n",
    "nohup python -u -m cehrgpt.runners.hf_cehrgpt_pretrain_runner \\\n",
    "  --model_name_or_path $CEHR_GPT_MODEL_DIR \\\n",
    "  --tokenizer_name_or_path $CEHR_GPT_MODEL_DIR \\\n",
    "  --output_dir $CEHR_GPT_MODEL_DIR \\\n",
    "  --data_folder \"$CEHR_GPT_DATA_DIR/patient_sequence/patient_sequence/train\" \\\n",
    "  --dataset_prepared_path \"$CEHR_GPT_DATA_DIR/dataset_prepared\" \\\n",
    "  --do_train true --seed 42 \\\n",
    "  --dataloader_num_workers 16 --dataloader_prefetch_factor 8 \\\n",
    "  --hidden_size 768 --num_hidden_layers 14 --max_position_embeddings 2048 \\\n",
    "  --evaluation_strategy epoch --save_strategy epoch \\\n",
    "  --warmup_steps 500 --weight_decay 0.01 \\\n",
    "  --num_train_epochs 10 --learning_rate 5e-5 \\\n",
    "  --use_early_stopping --early_stopping_threshold 0.001 \\\n",
    "  --load_best_model_at_end \\\n",
    "  --report_to none \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --per_device_eval_batch_size 2 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --sample_packing \\\n",
    "  --max_tokens_per_batch 3072 \\\n",
    "  --logging_steps 10 &> nohup.out &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_time must be string\n",
    "meds_data = read_df_from_bucket('meds_all_data_rev')\n",
    "meds_data['prediction_time'] = meds_data['prediction_time'].astype(str)\n",
    "meds_data.to_parquet('meds_data/meds_data.parquet', index = False)\n",
    "\n",
    "# # alternatively, run\n",
    "# python -u -m cehrbert_data.tools.convert_prediction_time_to_str \\\n",
    "#     -i meds_data \\\n",
    "#     -o meds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id          int64\n",
       "prediction_time    object\n",
       "boolean_value       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "meds_data = pd.read_parquet('meds_data/meds_data.parquet')\n",
    "meds_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit $SPARK_SUBMIT_OPTIONS ~/.local/lib/python3.10/site-packages/cehrbert_data/tools/extract_features.py \\\n",
    "    -c year3_gpt_sequence \\\n",
    "    -i allofus_omop_v8 \\\n",
    "    -o gpt_seq \\\n",
    "    -dl 1985-01-01 \\\n",
    "    -du 2023-12-31 \\\n",
    "    --cohort_dir meds_data/ \\\n",
    "    --person_id_column subject_id \\\n",
    "    --index_date_column prediction_time \\\n",
    "    --label_column boolean_value \\\n",
    "    -ip \\\n",
    "    --gpt_patient_sequence \\\n",
    "    --att_type day \\\n",
    "    --inpatient_att_type day \\\n",
    "    -iv \\\n",
    "    --ehr_table_list condition_occurrence procedure_occurrence drug_exposure \\\n",
    "    --include_concept_list \\\n",
    "    --patient_splits_folder allofus_omop_v8/patient_splits/ \\\n",
    "    --cache_events \\\n",
    "    --should_construct_artificial_visits \\\n",
    "    --disconnect_problem_list_records \\\n",
    "    --observation_window 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps aux | grep extract_features.py | grep -v grep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup python -u -m cehrgpt.tools.linear_prob.compute_cehrgpt_features /home/jupyter/workspaces/ehrcancermodelevaluation/config_pat_emb.yaml &> config_pat_emb.yaml.out &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Linear probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup python -u -m cehrgpt.tools.linear_prob.train_with_cehrgpt_features \\\n",
    "--features_data_dir \"/home/jupyter/workspaces/ehrcancermodelevaluation/aou_cehrgpt/linear_prob\" \\\n",
    "--output_dir \"/home/jupyter/workspaces/ehrcancermodelevaluation/aou_cehrgpt/linear_prob\" \\\n",
    "&> \"linear_prob.out\" &"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
