model_name_or_path: "test_results"
tokenizer_name_or_path: "test_results"

data_folder: "sample_data/pretrain"
dataset_prepared_path: "test_dataset_prepared"
validation_split_percentage: 0.05
validation_split_num: 10
preprocessing_num_workers: 4
preprocessing_batch_size: 1000
streaming: true

#Tokenizer
vocab_size: 50000
min_frequency: 0

do_train: true
overwrite_output_dir: false
resume_from_checkpoint: # path to the checkpoint folder
seed: 42

num_hidden_layers: 6
hidden_size: 768
n_head: 12
max_position_embeddings: 1024

# torch dataloader configs
dataloader_num_workers: 4
dataloader_prefetch_factor: 2

output_dir: "test_results"
save_strategy: "steps"
evaluation_strategy: "no"
learning_rate: 0.00005
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1
num_train_epochs: 1
# When streaming is set to True, max_steps needs to be provided
max_steps: 1000
save_steps: 500

warmup_steps: 100
weight_decay: 0.01
logging_dir: "./logs"
logging_steps: 100
save_total_limit: 5
load_best_model_at_end: false
metric_for_best_model: "eval_loss"
greater_is_better: false

report_to: "none"
