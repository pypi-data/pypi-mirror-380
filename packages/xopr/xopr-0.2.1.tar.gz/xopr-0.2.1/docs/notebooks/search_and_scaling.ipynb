{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf5e1cb",
   "metadata": {},
   "source": [
    "---\n",
    "title: Searching by region and scaling up\n",
    "description: Demonstration of how to find flight segments by geographic region and scale your workflows\n",
    "date: 2025-08-07\n",
    "---\n",
    "\n",
    "This notebook demonstrates:\n",
    "* Loading radar frames intersecting a geographic region (in this case an ice shelf)\n",
    "* Simple processing on focused radar data\n",
    "* How to use [Dask clusters](https://docs.dask.org/en/stable/deploying.html) to scale up your analysis, optionally using [Coiled](https://coiled.io/) to parallelize your analysis in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a040c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely\n",
    "import scipy.constants\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "import xopr.opr_access\n",
    "import xopr.geometry\n",
    "\n",
    "hvplot.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful projections\n",
    "epsg_3031 = ccrs.Stereographic(central_latitude=-90, true_scale_latitude=-71)\n",
    "latlng = ccrs.PlateCarree()\n",
    "features = gf.ocean.options(scale='50m').opts(projection=epsg_3031) * gf.coastline.options(scale='50m').opts(projection=epsg_3031)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish an OPR session\n",
    "# You'll probably want to set a cache directory if you're running this locally to speed\n",
    "# up subsequent requests. You can do other things like customize the STAC API endpoint,\n",
    "# but you shouldn't need to do that for most use cases.\n",
    "opr = xopr.opr_access.OPRConnection(cache_dir=\"/tmp\")\n",
    "\n",
    "# Or you can open a connection without a cache directory (for example, if you're parallelizing\n",
    "# this on a cloud cluster without persistent storage).\n",
    "#opr = xopr.OPRConnection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679adb83",
   "metadata": {},
   "source": [
    "xopr includes a helper module `xopr.geometry` with some useful utilities. You can call `get_antarctic_regions` to select one or more regions from the [MEaSUREs Antarctic Boundaries](https://nsidc.org/data/nsidc-0709/versions/2) dataset.\n",
    "\n",
    "Before we dive in, let's look at a few examples. Note that the GeoJSON returned is `EPSG:4326` (`WGS84`), so they look a bit weird. In the examples below, we reproject them to a more familiar `EPSG:3031` for the previews.\n",
    "\n",
    "You can select any (combination) of the three regions: `Peninsula`, `West`, and `East` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = xopr.geometry.get_antarctic_regions(regions='West', merge_regions=True)\n",
    "xopr.geometry.project_geojson(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62d1e9",
   "metadata": {},
   "source": [
    "The MEaSUREs dataset categorizes regions as `GR` (grounded), `FL` (floating), or `IS` (island). You can select by those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07276f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = xopr.geometry.get_antarctic_regions(type='FL', merge_regions=True)\n",
    "xopr.geometry.project_geojson(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b79db",
   "metadata": {},
   "source": [
    "You can also select by name. Note that you can pass a list to any of these parameters, as shown here.\n",
    "\n",
    "By default `merge_regions=True` and the return value is a single GeoJSON geometry. If you set `merge_regions=False`, you'll instead get back a GeoDataFrame with information about the regions included.\n",
    "\n",
    "```{tip}\n",
    "If you're trying to figure out what parameters to use to find your region, the [user guide for the MEaSUREs data product](https://nsidc.org/sites/default/files/nsidc-0709-v002-userguide.pdf) may be useful.\n",
    "\n",
    "[Quantarctica](https://www.npolar.no/quantarctica/) also includes this layer under the \"glaciology\" category if you want to be able to browse around and see the regions.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xopr.geometry.get_antarctic_regions(name=['LarsenD', 'LarsenE'], merge_regions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6299247",
   "metadata": {},
   "source": [
    "For the rest of this notebook, we're going to look at just the Dotson ice shelf, which we select as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a115110",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = xopr.geometry.get_antarctic_regions(name=\"Dotson\", type=\"FL\", merge_regions=True)\n",
    "xopr.geometry.project_geojson(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c3c5f",
   "metadata": {},
   "source": [
    "Here we see a new way of searching for radar frames by geometry. Simply passing our GeoJSON region gives us every available frame that intersects it.\n",
    "\n",
    "`data_product` controls what you get back. If you set this to `CSARP_standard` (or another `CSARP_` standard data product), you'll get back a list of loaded radar frames. If you set it to `None`, you'll get back the actual STAC items. That's helpful here because we're going to distribute the process of analyzing these frames, so we don't actually want to load any data just yet.\n",
    "\n",
    "Passing `max_items=10` limits the search to a maximum of 10 items to keep this running quickly on the GitHub Actions runners, but feel free to experiment with removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd964ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stac_items = opr.search_by_geometry(region, data_product=None, max_items=10)\n",
    "stac_items = opr.query_frames(geometry=region, max_items=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a3646",
   "metadata": {},
   "source": [
    "Plotting a map should look familiar. Here we've also overlaid the region we're searching.\n",
    "\n",
    "You'll notice that the region doesn't quite line up with the GeoViews `coastline` feature. That's expected. The coastline feature is a relatively low resolution global data product that you shouldn't treat as a real grounding line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1273de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a map of our loaded data over the selected region on an EPSG:3031 projection\n",
    "\n",
    "# Create a GeoViews object for the selected region\n",
    "region_gv = gv.Polygons(region, crs=latlng).opts(\n",
    "    color='green',\n",
    "    line_color='black',\n",
    "    fill_alpha=0.5,\n",
    "    projection=epsg_3031,\n",
    ")\n",
    "# Plot the frame geometries\n",
    "frame_lines = []\n",
    "for item in stac_items.geometry:\n",
    "    frame_lines.append(gv.Path(item, crs=latlng).opts(\n",
    "        line_width=2,\n",
    "        projection=epsg_3031\n",
    "    ))\n",
    "\n",
    "(features * region_gv * gv.Overlay(frame_lines)).opts(projection=epsg_3031)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb309c1b",
   "metadata": {},
   "source": [
    "Now that we've picked out some data, we're going to define some functions to do some simple analysis on it. We won't explain every detail of the code below, but basically it's picking out the surface and bed reflection powers and giving us back a `Dataset` with those powers in decibels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb185dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layer_peak_power(radar_ds, layer_twtt, margin_twtt):\n",
    "    \"\"\"\n",
    "    Extract the peak power of a radar layer within a specified margin around the layer's two-way travel time (TWTT).\n",
    "\n",
    "    Parameters:\n",
    "    - radar_ds: xarray Dataset containing radar data.\n",
    "    - layer_twtt: The two-way travel time of the layer to extract.\n",
    "    - margin_twtt: The margin around the layer's TWTT to consider for peak power extraction.\n",
    "\n",
    "    Returns:\n",
    "    - A DataArray containing the peak power values for the specified layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure that layer_twtt.slow_time matches the radar_ds slow_time\n",
    "    t_start = np.minimum(radar_ds.slow_time.min(), layer_twtt.slow_time.min())\n",
    "    t_end = np.maximum(radar_ds.slow_time.max(), layer_twtt.slow_time.max())\n",
    "    layer_twtt = layer_twtt.sel(slow_time=slice(t_start, t_end))\n",
    "    radar_ds = radar_ds.sel(slow_time=slice(t_start, t_end))\n",
    "    #layer_twtt = layer_twtt.interp(slow_time=radar_ds.slow_time, method='nearest')\n",
    "    layer_twtt = layer_twtt.reindex(slow_time=radar_ds.slow_time, method='nearest', tolerance=pd.Timedelta(seconds=1), fill_value=np.nan)\n",
    "    \n",
    "    # Calculate the start and end TWTT for the margin\n",
    "    start_twtt = layer_twtt - margin_twtt\n",
    "    end_twtt = layer_twtt + margin_twtt\n",
    "    \n",
    "    # Extract the data within the specified TWTT range\n",
    "    data_within_margin = radar_ds.where((radar_ds.twtt >= start_twtt) & (radar_ds.twtt <= end_twtt), drop=True)\n",
    "\n",
    "    power_dB = 10 * np.log10(np.abs(data_within_margin.Data))\n",
    "\n",
    "    # Find the twtt index corresponding to the peak power\n",
    "    peak_twtt_index = power_dB.argmax(dim='twtt')\n",
    "    # Convert the index to the actual TWTT value\n",
    "    peak_twtt = power_dB.twtt[peak_twtt_index]\n",
    "\n",
    "    # Calculate the peak power in dB\n",
    "    peak_power = power_dB.isel(twtt=peak_twtt_index)\n",
    "\n",
    "    # Remove unnecessary dimensions\n",
    "    peak_twtt = peak_twtt.drop_vars('twtt')\n",
    "    peak_power = peak_power.drop_vars('twtt')\n",
    "    \n",
    "    return peak_twtt, peak_power\n",
    "\n",
    "def surface_bed_reflection_power(stac_item, opr=xopr.opr_access.OPRConnection()):\n",
    "\n",
    "    frame = opr.load_frame(stac_item, data_product='CSARP_standard')\n",
    "    frame = frame.resample(slow_time='5s').mean()\n",
    "\n",
    "    try:\n",
    "        layers = opr.get_layers_db(frame, include_geometry=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving layers: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Re-pick surface and bed layers to ensure we're getting the peaks\n",
    "    speed_of_light_in_ice = scipy.constants.c / np.sqrt(3.17)  # Speed of light in ice (m/s)\n",
    "    layer_selection_margin_twtt = 50 / speed_of_light_in_ice # approx 50 m margin in ice\n",
    "    surface_repicked_twtt, surface_power = extract_layer_peak_power(frame, layers[1]['twtt'], layer_selection_margin_twtt)\n",
    "    bed_repicked_twtt, bed_power = extract_layer_peak_power(frame, layers[2]['twtt'], layer_selection_margin_twtt)\n",
    "\n",
    "    # Create a dataset from surface_repicked_twtt, bed_repicked_twtt, surface_power, and bed_power\n",
    "\n",
    "    reflectivity_dataset = xr.merge([\n",
    "        surface_repicked_twtt.rename('surface_twtt'),\n",
    "        bed_repicked_twtt.rename('bed_twtt'),\n",
    "        surface_power.rename('surface_power_dB'),\n",
    "        bed_power.rename('bed_power_dB'),\n",
    "        ])\n",
    "\n",
    "    flight_line_metadata = frame.drop_vars(['Data', 'Surface', 'Bottom'])\n",
    "    reflectivity_dataset = xr.merge([reflectivity_dataset, flight_line_metadata])\n",
    "\n",
    "    reflectivity_dataset = reflectivity_dataset.drop_dims(['twtt'])  # Remove the twtt dimension since everything has been flattened\n",
    "\n",
    "    attributes_to_copy = ['season', 'segment', 'doi', 'ror', 'funder_text']\n",
    "    reflectivity_dataset.attrs = {attr: frame.attrs[attr] for attr in attributes_to_copy if attr in frame.attrs}\n",
    "\n",
    "    return reflectivity_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf9f56",
   "metadata": {},
   "source": [
    "Let's try out our analysis function on one frame. The plot below shows surface and bed power for a single frame of radar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflectivity = surface_bed_reflection_power(stac_items.iloc[0], opr=opr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "reflectivity['surface_power_dB'].plot(ax=ax, x='slow_time', label='Surface')\n",
    "reflectivity['bed_power_dB'].plot(ax=ax, x='slow_time', label='Bed')\n",
    "ax.set_ylabel('Power [dB]')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a6aeb",
   "metadata": {},
   "source": [
    "Ok, that works, so now let's scale up. You could just write a loop, but this is a parallelizable task, so we want to share the load out across whatever computational resources you might have.\n",
    "\n",
    "This is good time to pause and note a few things:\n",
    "\n",
    "```{admonition} This is getting too complicated!\n",
    "We're about to jump into how to parallelize your workflows. This part is *completely optional*. If you're happily doing your analysis without thinking about this, you can feel free to ignore the rest of the tutorial.\n",
    "\n",
    "But if you're curious or you'd like to run things faster...\n",
    "```\n",
    "\n",
    "If you're still with us, this would probably be a good time to briefly read up on [Dask](https://www.dask.org/). In short, Dask gives us a way to write code that looks pretty close to standard Python code but also easily distribute jobs across available compute infrastructure.\n",
    "\n",
    "The defaults in this notebook will simply parallelize this workflow across however many CPU cores you have on your machine. We'll also discuss using [Coiled](https://coiled.io/) to scale your workflows into the cloud.\n",
    "\n",
    "There are many more options, which you can read about in [the Dask documentation](https://docs.dask.org/en/stable/deploying.html). These options allow you to distribute your workflow across various cloud services or using any HPC resources you may have acesss to.\n",
    "\n",
    "Getting started, we'll create a Dask `LocalCluster`, which is just going to let us distribute the workload across the CPUs on our machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80808473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import LocalCluster\n",
    "\n",
    "client = LocalCluster().get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f2147",
   "metadata": {},
   "source": [
    "We could have alternatively created any cluster we want. For example, this is how you'd create a Coiled cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# import coiled\n",
    "# cluster = coiled.Cluster(n_workers=10)\n",
    "# client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee06e26",
   "metadata": {},
   "source": [
    "Now we're going to use `client.map` to run our function across whatever resources are in the `client` object.\n",
    "\n",
    "Note that if you're using a cloud cluster, you probably don't want to pass your local `opr` object. Since you don't have shared storage anyway, it'll be better to let it load a default `OPRConnection()` object on each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_list = [row for _, row in stac_items.iterrows()]\n",
    "futures = client.map(surface_bed_reflection_power, stac_list, opr=opr)\n",
    "\n",
    "# Process results as they complete, capturing exceptions\n",
    "results = []\n",
    "for future in dask.distributed.as_completed(futures):\n",
    "    try:\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da21d40",
   "metadata": {},
   "source": [
    "Great! If that ran successfully, results should now be a list of `Dataset`'s. Now we're back to normal code to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2af483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoViews object for the selected region\n",
    "region_gv = gv.Polygons(region, crs=latlng).opts(\n",
    "    line_color='black',\n",
    "    fill_alpha=0,\n",
    "    projection=epsg_3031,\n",
    ")\n",
    "\n",
    "data_lines = []\n",
    "for ds in results:\n",
    "    ds['bed_minus_surf'] = ds['bed_power_dB'] - ds['surface_power_dB']\n",
    "    ds = ds.dropna(dim='slow_time')\n",
    "    ds = xopr.geometry.project_dataset(ds, target_crs='EPSG:3031')\n",
    "    sc = ds.hvplot.scatter(x='x', y='y', c='bed_minus_surf',\n",
    "                           hover_cols=['surface_power_dB', 'bed_power_dB'],\n",
    "                           cmap='turbo', size=3)\n",
    "    data_lines.append(sc)\n",
    "\n",
    "(features * region_gv * gv.Overlay(data_lines)).opts(aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea49367",
   "metadata": {},
   "source": [
    "If you're using a cloud or HPC cluster, it's good practice to explicitly close it. In most cases, it'll have an auto-timeout, but we'll just close ours to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0e761",
   "metadata": {},
   "source": [
    "Congrats! If you made it this far, you've learned how to parallelize your radar analysis workflows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c2ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xopr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
