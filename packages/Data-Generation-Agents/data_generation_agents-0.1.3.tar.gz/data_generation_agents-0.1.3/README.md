# ğŸš€ Synthetic Data Generation Pipeline

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/synthetic-data-pipeline.svg)](https://badge.fury.io/py/synthetic-data-pipeline)

An AI-powered synthetic data generation pipeline that leverages web search, content scraping, and advanced language models to create high-quality, contextually rich datasets for machine learning and AI training.

## âœ¨ Features

- **ğŸŒ Intelligent Web Search**: Uses Tavily API for comprehensive web search across multiple queries
- **ğŸ“Š Advanced Content Scraping**: Robust web scraping with ScraperAPI integration and anti-bot measures
- **ğŸ§  AI-Powered Topic Extraction**: Gemini-based topic extraction from scraped content
- **ğŸ¯ Flexible Data Generation**: Generate diverse data types including QA pairs, product reviews, conversations, and more
- **ğŸŒ Multi-Language Support**: Generate data in multiple languages with automatic language detection
- **ğŸ’¾ Stateful Pipeline**: Resume interrupted processes with intelligent checkpointing
- **ğŸ”§ Highly Configurable**: Customizable parameters for queries, results, and generation strategies
- **ğŸ·ï¸ Domain Categories**: Optional specification of categories within domains for more targeted and precise data generation
- **ğŸ“ˆ Scalable Architecture**: Agent-based architecture for modular and extensible functionality

## ğŸ—ï¸ Architecture

The pipeline consists of several specialized AI agents working in coordination:

1. **ğŸ“ Query Parser Agent**: Analyzes user queries to extract requirements, including optional categories within the specified domain
2. **ğŸ” Query Refiner Agent**: Generates multiple refined search queries
3. **ğŸŒ Web Search Agent**: Performs comprehensive web searches
4. **ğŸ” Filtration Agent**: Filters and validates search results
5. **ğŸ•·ï¸ Web Scraping Agent**: Extracts content from web pages
6. **ğŸ·ï¸ Topic Extraction Agent**: Identifies relevant topics from content
7. **ğŸ¯ Synthetic Data Generator Agent**: Creates synthetic data based on extracted topics

## ğŸ“Š Pipeline Flowchart

![Pipeline Flowchart](pipeline.png)

## âš ï¸ Important: Topic Extraction Requirement

**CRITICAL**: The data generation phase will **NOT begin** until the topic extraction stage identifies enough unique topics to support your requested data volume.

### How It Works:

The pipeline calculates the required number of topics using this formula:

```
Required Topics = Number of Requested Rows / ROWS_PER_SUBTOPIC
```

**Example**:
- If you request **1000 rows** of data
- And `ROWS_PER_SUBTOPIC = 5` (default)
- The pipeline needs to extract at least **200 unique topics** before data generation begins

### What This Means For You:

1. **Larger Datasets Need More Topics**: Requesting 5000 rows requires significantly more topics than 500 rows
2. **Search Configuration Matters**: If the pipeline can't extract enough topics, you may need to:
   - Increase `REFINED_QUERIES_COUNT` (generate more search queries)
   - Increase `SEARCH_RESULTS_PER_QUERY` (scrape more web pages)
   - Broaden your domain/categories (provide more diverse content sources)
3. **Pipeline Will Wait**: The data generation agent will not start until sufficient topics are extracted
4. **Monitor Progress**: Watch the topic extraction output to see how many topics have been identified

### Configuration Tips:

For large datasets (1000+ rows), consider adjusting these parameters:

```python
generate_synthetic_data(
    "Generate 5000 English QA pairs about machine learning...",
    refined_queries_count=40,      # More search queries
    search_results_per_query=8,    # More results per query
    rows_per_subtopic=5            # Or increase this to need fewer topics
)
```

**Pro Tip**: If you're generating a very large dataset and topic extraction is taking too long, you can increase `ROWS_PER_SUBTOPIC` to reduce the number of required topics (but this may reduce data diversity).

## ğŸš€ Quick Start

### Installation

```bash
pip install Data_Generation_Agents
```

### Basic Usage

```python
from Data_Generation_Agents import generate_synthetic_data

generate_synthetic_data(
    "prompt"
)
```

### CLI Usage

```bash
synthetic-data "prompt"
```

## âš™ï¸ Configuration

### Environment Setup

Create a `.env` file in your project directory:

```env
# Required API Keys
GEMINI_API_KEY=your_gemini_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
SCRAPERAPI_API_KEY=your_scraper_api_key_here

# Output Configuration
OUTPUT_DIR=/path/to/your/output

# Optional: Pipeline Configuration
REFINED_QUERIES_COUNT=30
SEARCH_RESULTS_PER_QUERY=5
ROWS_PER_SUBTOPIC=5
LOG_LEVEL=INFO
```

### ğŸ”‘ API Keys Setup

1. **ğŸ¤– Gemini API**: Get your key from [Google AI Studio](https://makersuite.google.com/app/apikey)
2. **ğŸ” Tavily API**: Sign up at [Tavily](https://tavily.com/) for web search capabilities
3. **ğŸ•·ï¸ ScraperAPI**: Register at [ScraperAPI](https://www.scraperapi.com/) for robust web scraping


## ğŸ”„ Stateful Pipeline Example

The pipeline's checkpointing system ensures you never lose progress. Here's how it works in practice:

```python
# First run - generates 1000 samples, but gets interrupted after 400
generate_synthetic_data("Generate 1000 English QA pairs about Python programming in categories: basics, advanced topics, libraries......")

# Output shows:
# âœ… Query parsed successfully
# âœ… 30 refined queries generated
# âœ… Web search completed (150 results)
# âœ… Content scraping completed
# âœ… Topic extraction completed (45 topics found)
# â¸ï¸  Data generation: 400/1000 samples (INTERRUPTED)

# Second run with the SAME query - automatically resumes from checkpoint
generate_synthetic_data("Generate 1000 English QA pairs about Python programming in categories: basics, advanced topics, libraries......")

# Output shows:
# â­ï¸  Skipping completed stages (query_parsed, query_refined, web_searched, etc.)
# ğŸ”„ Resuming data generation from topic 41/45
# âœ… Data generation: 1000/1000 samples completed
```

**File Structure Created**:
```
data/
â””â”€â”€ 7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2d3e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9/
    â”œâ”€â”€ pipeline_state.json           # {"status": "data_generated", "checkpoint": {...}}
    â”œâ”€â”€ refined_queries.json          # 30 search queries
    â”œâ”€â”€ search_results.json           # 150 web search results
    â”œâ”€â”€ scraped_content.json          # Scraped web pages
    â”œâ”€â”€ all_chunks.json               # Processed content chunks
    â”œâ”€â”€ all_extracted_topics.json     # 45 extracted topics
    â””â”€â”€ synthetic_data.json           # 1000 generated QA pairs
```

## ğŸ¯ Advanced Configuration

### Custom Parameters

```python
generate_synthetic_data(
    user_query="Your data generation request",
    refined_queries_count=40,      # More search queries for broader coverage
    search_results_per_query=8,    # More results per query
    rows_per_subtopic=6,           # More data per topic
    gemini_model_name="gemini-1.5-pro"  # Use different Gemini model
)
```

### Domain-Specific Generation

The pipeline automatically detects and adapts to various domains:
- **Technical**: Programming, software development, cloud computing
- **Medical**: Healthcare, pharmaceuticals, medical devices
- **Legal**: Law, regulations, compliance
- **Financial**: Banking, fintech, investment
- **E-commerce**: Products, reviews, customer service
- **Education**: Learning materials, tutorials, assessments

Users can optionally specify categories within the domain in the natural language query (e.g., "about machine learning in categories: supervised, unsupervised, reinforcement"). The query parser will detect and define the categories variable accordingly.

## ğŸ“Š Output Structure

Generated data is saved in structured JSON format with the following structure:

```json
{
    "data_type": "QA pairs",
    "content": {
        "question": "What is machine learning?",
        "answer": "Machine learning is a subset of artificial intelligence..."
    },
    "source_topics": ["machine learning"],
    "generation_timestamp": "2024-01-15T10:30:00Z"
}
```

## ğŸ”§ Development

### Local Development Setup

```bash
# Clone the repository
git clone https://github.com/Omar-YYoussef/Data_Gen_Agent
cd Data_Gen_Agent

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -e .

# Install development dependencies
pip install -e ".[dev]"
```

### Project Structure

```
src/synthetic_data_pipeline/
â”œâ”€â”€ agents/                    # AI agents for different pipeline stages
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py          # Base agent class with common functionality
â”‚   â”œâ”€â”€ query_parser_agent.py  # Parses user queries to extract requirements
â”‚   â”œâ”€â”€ query_refiner_agent.py # Generates multiple refined search queries
â”‚   â”œâ”€â”€ web_search_agent.py    # Performs comprehensive web searches
â”‚   â”œâ”€â”€ filtration_agent.py    # Filters and validates search results
â”‚   â”œâ”€â”€ web_scraping_agent.py  # Extracts content from web pages
â”‚   â”œâ”€â”€ topic_extraction_agent.py # Identifies relevant topics from content
â”‚   â””â”€â”€ synthetic_data_generator_agent.py # Creates synthetic data
â”œâ”€â”€ services/                  # External service integrations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gemini_service.py      # Google Gemini API integration
â”‚   â”œâ”€â”€ tavily_service.py      # Tavily web search API integration
â”‚   â”œâ”€â”€ scraping_service.py    # Web scraping utilities
â”‚   â””â”€â”€ chunking_service.py    # Content chunking for processing
â”œâ”€â”€ workflows/                 # Workflow orchestration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main_workflow.py       # Main workflow coordination
â”œâ”€â”€ models/                    # Data models and schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_schemas.py        # Pydantic models for data structures
â”œâ”€â”€ utils/                     # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_handler.py        # JSON file operations
â”‚   â””â”€â”€ pipeline_state_manager.py # State management and checkpointing
â”œâ”€â”€ config/                    # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py            # Application settings and environment variables
â”œâ”€â”€ __init__.py                # Package initialization
â”œâ”€â”€ __main__.py                # CLI entry point
â””â”€â”€ main.py                    # Main pipeline orchestration
```


## ğŸŒŸ Key Features Deep Dive

### ğŸ”„ Stateful Pipeline with Intelligent Checkpointing

**Resume from Any Point**: The pipeline automatically saves its progress and can resume from exactly where it left off if interrupted. Each query is hashed using SHA-256 to create a unique identifier, and all progress is saved in a dedicated directory structure.

**How It Works**:
```python
# If you run the same query again, it automatically resumes from the last checkpoint
generate_synthetic_data("Generate 5000 English QA pairs about machine learning in categories: supervised learning, unsupervised learning, deep learning, use (Question, Answer) as the columns.")

# The pipeline creates a unique directory based on the query hash:
# data/
# â””â”€â”€ a1b2c3d4e5f6.../  # SHA-256 hash of your query
#     â”œâ”€â”€ pipeline_state.json      # Main state file
#     â”œâ”€â”€ refined_queries.json     # Saved search queries
#     â”œâ”€â”€ search_results.json      # Web search results
#     â”œâ”€â”€ scraped_content.json     # Scraped web content
#     â”œâ”€â”€ all_chunks.json          # Content chunks
#     â”œâ”€â”€ all_extracted_topics.json # Extracted topics
#     â””â”€â”€ synthetic_data.json      # Generated data
```

**Checkpoint Levels**:
- ğŸ `initial` (0) - Pipeline initialized
- ğŸ¬ `initialized` (1) - Setup completed
- ğŸ“ `query_parsed` (2) - Query analysis completed
- ğŸ” `query_refined` (3) - Search queries generated
- ğŸŒ `web_searched` (4) - Web search completed
- ğŸ•·ï¸ `web_scraped` (5) - Content scraping completed
- ğŸ“¦ `content_gathered` (6) - Content processing completed
- ğŸ·ï¸ `topics_extracted` (7) - Topic extraction completed
- ğŸ¯ `data_generated` (8) - Synthetic data generation completed
- âœ… `completed` (9) - Pipeline fully completed

**Benefits**:
- **No Lost Progress**: Never lose hours of work due to interruptions
- **Incremental Processing**: Add more data to existing datasets
- **Resource Efficiency**: Skip completed stages when resuming
- **Debugging**: Easy to inspect intermediate results

### Intelligent Query Processing
- Automatically extracts sample count, data type, language, domain, and optional categories from natural language queries
- Supports implicit and explicit parameter specification
- Handles complex multi-requirement queries

### Robust Web Scraping
- Content quality filtering and validation
- Language detection and filtering
- Error handling and retry mechanisms

### Advanced Topic Extraction
- Contextual topic identification from web content
- Hierarchical topic organization

### Flexible Data Generation
- Support for multiple data structures and formats
- Context-aware content generation

## ğŸ“ˆ Performance & Scalability

- **Concurrent Processing**: Parallel execution of web searches and content scraping
- **Checkpoint System**: Resume interrupted processes without losing progress
- **Rate Limiting**: Intelligent API rate limiting to prevent quota exhaustion

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **API Key Errors**
   ```
   Error: Invalid API key
   Solution: Verify your API keys in the .env file
   ```

2. **Rate Limiting**
   ```
   Error: Rate limit exceeded
   Solution: wait before retrying
   ```

### Debug Mode

Enable detailed logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Your pipeline code here
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Google Gemini](https://ai.google.dev/) for advanced language model capabilities
- [Tavily](https://tavily.com/) for comprehensive web search
- [ScraperAPI](https://www.scraperapi.com/) for robust web scraping

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/Omar-YYoussef/Data_Gen_Agent/issues)
- **Email**: omarjooo595@gmail.com
- **Documentation**: [Project Wiki](https://github.com/Omar-YYoussef/Data_Gen_Agent/wiki)

---

**Made with â¤ï¸ by [Omar Youssef](https://github.com/Omar-YYoussef)**