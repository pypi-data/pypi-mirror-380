import csv
import io
import os
import tempfile
import time
from contextlib import contextmanager

import numpy as np
import pandas as pd
import psycopg2
import yaml

from hmcis_packs.logger.logger_config import setup_logger

logger = setup_logger(__name__)


class ReallyOptimizedDatabaseClient:
    def __init__(self, config_path=None):
        if config_path is None:
            user_home = os.path.expanduser("~")
            config_path = os.path.join(user_home, "db_config.yaml")

        with open(config_path, 'r', encoding='utf-8') as f:
            self.db_config = yaml.safe_load(f)

    @contextmanager
    def get_speed_optimized_connection(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        conn = psycopg2.connect(**self.db_config)

        try:
            with conn.cursor() as cur:
                # –¢–æ–ª—å–∫–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ú–û–ñ–ù–û –º–µ–Ω—è—Ç—å –≤–æ –≤—Ä–µ–º—è —Å–µ—Å—Å–∏–∏
                try:
                    cur.execute("SET synchronous_commit = OFF")  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∫–æ–º–º–∏—Ç—ã
                    cur.execute("SET maintenance_work_mem = '512MB'")  # –ü–∞–º—è—Ç—å –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π
                    cur.execute("SET work_mem = '256MB'")  # –ü–∞–º—è—Ç—å –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–æ–∫
                    cur.execute("SET temp_buffers = '64MB'")  # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
                    # cur.execute("SET commit_delay = 1000")           # –ó–∞–¥–µ—Ä–∂–∫–∞ –∫–æ–º–º–∏—Ç–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
                    cur.execute("SET commit_siblings = 5")  # –ú–∏–Ω–∏–º—É–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏

                    # –≠—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö
                    try:
                        cur.execute("SET checkpoint_completion_target = 0.9")
                    except psycopg2.Error:
                        pass

                    logger.info("‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ PostgreSQL")
                except psycopg2.Error as e:
                    logger.warning(f"‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ PostgreSQL –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã: {e}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª–∏—Å—å

                conn.commit()

            yield conn

        finally:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (—Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å)
            try:
                with conn.cursor() as cur:
                    cur.execute("SET synchronous_commit = ON")
                    conn.commit()
            except:
                pass
            conn.close()

    @contextmanager
    def get_safe_optimized_connection(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ (–±–µ–∑ server-level –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)"""
        conn = psycopg2.connect(**self.db_config)

        try:
            with conn.cursor() as cur:
                # –¢–æ–ª—å–∫–æ session-level –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç
                cur.execute("SET synchronous_commit = OFF")  # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∫–æ–º–º–∏—Ç—ã
                cur.execute("SET maintenance_work_mem = '256MB'")  # –ü–∞–º—è—Ç—å –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π
                cur.execute("SET work_mem = '128MB'")  # –ü–∞–º—è—Ç—å –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–æ–∫
                logger.info("‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PostgreSQL")

            yield conn

        finally:
            try:
                with conn.cursor() as cur:
                    cur.execute("SET synchronous_commit = ON")
            except:
                pass
            conn.close()

    def _get_optimized_pg_type(self, pandas_dtype):
        """–ë—ã—Å—Ç—Ä—ã–π –º–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ PostgreSQL —Ç–∏–ø–∞–º–∏"""
        dtype_str = str(pandas_dtype).lower()

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ datetime64[ns] - –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–∏–ø–∞ pandas –¥–ª—è –¥–∞—Ç
        if 'datetime64' in dtype_str:
            if '[ns]' in dtype_str:  # datetime64[ns] - –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã
                return 'TIMESTAMP WITHOUT TIME ZONE'
            elif '[us]' in dtype_str:  # datetime64[us] - –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã
                return 'TIMESTAMP WITHOUT TIME ZONE'
            elif '[ms]' in dtype_str:  # datetime64[ms] - –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                return 'TIMESTAMP WITHOUT TIME ZONE'
            else:
                return 'TIMESTAMP WITHOUT TIME ZONE'

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º pandas API –¥–ª—è datetime —Ç–∏–ø–æ–≤ (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ)
        if pd.api.types.is_datetime64_any_dtype(pandas_dtype):
            return 'TIMESTAMP WITHOUT TIME ZONE'

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è timezone-aware datetime
        if hasattr(pandas_dtype, 'tz') and pandas_dtype.tz is not None:
            return 'TIMESTAMP WITH TIME ZONE'

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã
        if 'int8' in dtype_str or 'int16' in dtype_str:
            return 'SMALLINT'
        elif 'int32' in dtype_str:
            return 'INTEGER'
        elif 'int64' in dtype_str or 'int' in dtype_str:
            return 'BIGINT'
        elif 'float32' in dtype_str:
            return 'REAL'
        elif 'float64' in dtype_str or 'float' in dtype_str:
            return 'DOUBLE PRECISION'  # –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º NUMERIC
        elif 'bool' in dtype_str:
            return 'BOOLEAN'
        elif 'datetime' in dtype_str or 'timestamp' in dtype_str:
            return 'TIMESTAMP WITHOUT TIME ZONE'
        elif 'object' in dtype_str:
            return 'TEXT'
        else:
            return 'TEXT'

    def save_with_temp_file_copy(self,
                                 df: pd.DataFrame,
                                 table_name: str,
                                 schema: str = 'IFRS Reports') -> None:
        """
        –°–ê–ú–´–ô –ë–´–°–¢–†–´–ô –º–µ—Ç–æ–¥: –≤—Ä–µ–º–µ–Ω–Ω—ã–π TSV —Ñ–∞–π–ª + PostgreSQL COPY
        """
        start_time = time.time()
        total_rows = len(df)

        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        df_clean = df.dropna(axis=1, how='all')  # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        df_clean = df_clean.loc[:, df_clean.columns.str.strip() != '']  # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∏–º–µ–Ω–∞–º–∏

        logger.info(f"üöÄ COPY —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {total_rows:,} —Å—Ç—Ä–æ–∫...")

        with self.get_speed_optimized_connection() as conn:
            with conn.cursor() as cursor:
                try:
                    prep_start = time.time()

                    # 1. –ë–´–°–¢–†–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã (–±–µ–∑ –ø—Ä–æ–≤–µ—Ä–æ–∫)
                    try:
                        cursor.execute(f'CREATE SCHEMA "{schema}"')
                    except psycopg2.Error:
                        pass  # –°—Ö–µ–º–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

                    # 2. –ë–´–°–¢–†–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                    column_definitions = []
                    for col_name, dtype in zip(df_clean.columns, df_clean.dtypes):
                        pg_type = self._get_optimized_pg_type(dtype)
                        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
                        safe_col_name = col_name.replace('"', '""')
                        column_definitions.append(f'"{safe_col_name}" {pg_type}')

                    # –î–†–û–ü–ê–ï–ú —Ç–∞–±–ª–∏—Ü—É –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–æ–∫ (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º TRUNCATE)
                    cursor.execute(f'DROP TABLE IF EXISTS "{schema}"."{table_name}"')

                    # –°–æ–∑–¥–∞–µ–º UNLOGGED —Ç–∞–±–ª–∏—Ü—É (–ë–ï–ó WAL –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è = —Å—É–ø–µ—Ä –±—ã—Å—Ç—Ä–æ)
                    create_table_sql = f'''
                    CREATE UNLOGGED TABLE "{schema}"."{table_name}" (
                        {', '.join(column_definitions)}
                    )
                    '''
                    cursor.execute(create_table_sql)

                    prep_time = time.time() - prep_start
                    logger.info(f"‚ö° –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {prep_time:.2f}—Å")

                    # 3. –ë–´–°–¢–†–ê–Ø –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    data_start = time.time()

                    # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ None –í–ï–ö–¢–û–†–ù–û (—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–±)
                    df_for_export = df_clean.where(pd.notnull(df_clean), None)

                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π TSV —Ñ–∞–π–ª
                    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.tsv',
                                                     encoding='utf-8') as tmp_file:
                        writer = csv.writer(tmp_file, delimiter='\t', quoting=csv.QUOTE_MINIMAL,
                                            lineterminator='\n', quotechar='"')

                        # –ü–∏—à–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º DataFrame.to_csv –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
                        for row in df_for_export.values:
                            processed_row = []
                            for val in row:
                                if val is None:
                                    processed_row.append('')  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è NULL
                                elif isinstance(val, str):
                                    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                                    processed_row.append(val.replace('\t', ' ').replace('\r\n', ' ').replace('\n', ' '))
                                else:
                                    processed_row.append(str(val))
                            writer.writerow(processed_row)

                        temp_file_path = tmp_file.name

                    data_time = time.time() - data_start
                    logger.info(f"‚ö° –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {data_time:.2f}—Å")

                    # 4. –°–£–ü–ï–†-–ë–´–°–¢–†–´–ô COPY –∏–∑ —Ñ–∞–π–ª–∞
                    copy_start = time.time()

                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è COPY
                    column_names = []
                    columns_sql = ', '.join(column_names)

                    # COPY –∫–æ–º–∞–Ω–¥–∞ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
                    copy_command = f'''
                    COPY "{schema}"."{table_name}" ({columns_sql})
                    FROM '{temp_file_path}'
                    WITH (
                        FORMAT CSV,
                        DELIMITER E'\\t',
                        NULL '',
                        HEADER false,
                        QUOTE '"'
                    )
                    '''

                    cursor.execute(copy_command)

                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    try:
                        os.unlink(temp_file_path)
                    except:
                        pass

                    copy_time = time.time() - copy_start
                    logger.info(f"‚ö° COPY –∏–∑ —Ñ–∞–π–ª–∞: {copy_time:.2f}—Å")

                    # 5. –û–î–ò–ù –∫–æ–º–º–∏—Ç –≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ
                    commit_start = time.time()
                    conn.commit()
                    commit_time = time.time() - commit_start
                    logger.info(f"‚ö° –ö–æ–º–º–∏—Ç: {commit_time:.2f}—Å")

                    # 6. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ –æ–±—ã—á–Ω—É—é (—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º) –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                    cursor.execute(f'ALTER TABLE "{schema}"."{table_name}" SET LOGGED')
                    conn.commit()

                    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    total_time = time.time() - start_time
                    rows_per_second = total_rows / total_time if total_time > 0 else 0

                    logger.info(f"‚úÖ COPY –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
                    logger.info(f"   üìä –°—Ç—Ä–æ–∫: {total_rows:,}")
                    logger.info(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}—Å")
                    logger.info(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_second:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
                    logger.info(f"   üéØ –¶–µ–ª—å –¥–ª—è 900K: {900000 / rows_per_second:.1f}—Å")

                except Exception as e:
                    conn.rollback()
                    logger.error(f"üíÄ –û—à–∏–±–∫–∞ COPY –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                    # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    try:
                        if 'temp_file_path' in locals():
                            os.unlink(temp_file_path)
                    except:
                        pass
                    raise

    def save_with_execute_values_turbo(self,
                                       df: pd.DataFrame,
                                       table_name: str,
                                       schema: str = 'IFRS Reports',
                                       batch_size: int = 50000) -> None:
        """
        –¢—É—Ä–±–æ-–≤–µ—Ä—Å–∏—è execute_values —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        """
        start_time = time.time()
        total_rows = len(df)

        df_clean = df.dropna(axis=1, how='all')
        df_clean = df_clean.loc[:, df_clean.columns.str.strip() != '']

        logger.info(f"‚ö° Execute_values –¢–£–†–ë–û: {total_rows:,} —Å—Ç—Ä–æ–∫...")

        with self.get_speed_optimized_connection() as conn:
            with conn.cursor() as cursor:
                try:
                    # –ë—ã—Å—Ç—Ä–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ö–µ–º—ã –∏ —Ç–∞–±–ª–∏—Ü—ã
                    try:
                        cursor.execute(f'CREATE SCHEMA "{schema}"')
                    except psycopg2.Error:
                        pass

                    column_definitions = []
                    for col_name, dtype in zip(df_clean.columns, df_clean.dtypes):
                        pg_type = self._get_optimized_pg_type(dtype)
                        safe_col_name = col_name.replace('"', '""')
                        column_definitions.append(f'"{safe_col_name}" {pg_type}')

                    cursor.execute(f'DROP TABLE IF EXISTS "{schema}"."{table_name}"')

                    create_table_sql = f'''
                    CREATE UNLOGGED TABLE "{schema}"."{table_name}" (
                        {', '.join(column_definitions)}
                    )
                    '''
                    cursor.execute(create_table_sql)

                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –í–ï–ö–¢–û–†–ù–û
                    df_for_insert = df_clean.where(pd.notnull(df_clean), None)
                    all_records = [tuple(row) for row in df_for_insert.values]

                    # –ú–ï–ì–ê execute_values —Å –æ–≥—Ä–æ–º–Ω—ã–º–∏ –±–∞—Ç—á–∞–º–∏
                    column_names = [f'"{col.replace('"', '""')}"' for col in df_clean.columns]
                    columns_sql = ', '.join(column_names)

                    insert_sql = f'''
                    INSERT INTO "{schema}"."{table_name}" ({columns_sql}) VALUES %s
                    '''

                    insert_start = time.time()

                    # –í—Å—Ç–∞–≤–ª—è–µ–º –û–ì–†–û–ú–ù–´–ú–ò –±–∞—Ç—á–∞–º–∏
                    from psycopg2.extras import execute_values

                    if len(all_records) <= batch_size:
                        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–º–Ω–æ–≥–æ - –æ–¥–Ω–∏–º –º–∞—Ö–æ–º
                        execute_values(
                            cursor,
                            insert_sql,
                            all_records,
                            template=None,
                            page_size=len(all_records),  # –í–µ—Å—å –±–∞—Ç—á —Å—Ä–∞–∑—É
                            fetch=False
                        )
                    else:
                        # –ë–æ–ª—å—à–∏–º–∏ –∫—É—Å–∫–∞–º–∏
                        for i in range(0, len(all_records), batch_size):
                            batch = all_records[i:i + batch_size]
                            execute_values(
                                cursor,
                                insert_sql,
                                batch,
                                template=None,
                                page_size=len(batch),
                                fetch=False
                            )
                            logger.info(f"üì¶ –ë–∞—Ç—á {i // batch_size + 1}: {len(batch):,} —Å—Ç—Ä–æ–∫")

                    insert_time = time.time() - insert_start
                    logger.info(f"‚ö° Execute_values: {insert_time:.2f}—Å")

                    # –ö–æ–º–º–∏—Ç –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
                    conn.commit()
                    cursor.execute(f'ALTER TABLE "{schema}"."{table_name}" SET LOGGED')
                    conn.commit()

                    total_time = time.time() - start_time
                    rows_per_second = total_rows / total_time if total_time > 0 else 0

                    logger.info(f"‚úÖ Execute_values –¢–£–†–ë–û –∑–∞–≤–µ—Ä—à–µ–Ω:")
                    logger.info(f"   üìä –°—Ç—Ä–æ–∫: {total_rows:,}")
                    logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {total_time:.2f}—Å")
                    logger.info(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_second:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")

                except Exception as e:
                    conn.rollback()
                    logger.error(f"üíÄ –û—à–∏–±–∫–∞ Execute_values –¢–£–†–ë–û: {e}")
                    raise

    def save_with_copy_from_stringio(self,
                                     df: pd.DataFrame,
                                     table_name: str,
                                     schema: str = 'IFRS Reports') -> None:
        """
        COPY —á–µ—Ä–µ–∑ StringIO (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≤–∞—à–µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞)
        """
        start_time = time.time()
        total_rows = len(df)

        df_clean = df.dropna(axis=1, how='all')
        df_clean = df_clean.loc[:, df_clean.columns.str.strip() != '']

        logger.info(f"üìù COPY —á–µ—Ä–µ–∑ StringIO: {total_rows:,} —Å—Ç—Ä–æ–∫...")

        with self.get_speed_optimized_connection() as conn:
            with conn.cursor() as cursor:
                try:
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
                    try:
                        cursor.execute(f'CREATE SCHEMA "{schema}"')
                    except psycopg2.Error:
                        pass

                    column_definitions = []
                    for col_name, dtype in zip(df_clean.columns, df_clean.dtypes):
                        pg_type = self._get_optimized_pg_type(dtype)
                        safe_col_name = col_name.replace('"', '""')
                        column_definitions.append(f'"{safe_col_name}" {pg_type}')

                    cursor.execute(f'DROP TABLE IF EXISTS "{schema}"."{table_name}"')

                    create_table_sql = f'''
                    CREATE UNLOGGED TABLE "{schema}"."{table_name}" (
                        {', '.join(column_definitions)}
                    )
                    '''
                    cursor.execute(create_table_sql)

                    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ StringIO
                    data_start = time.time()

                    # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–º–µ–Ω–∞ NaN
                    df_copy = df_clean.fillna('')  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ \N (–±—ã—Å—Ç—Ä–µ–µ)

                    # –°–æ–∑–¥–∞–µ–º StringIO —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±—É—Ñ–µ—Ä–∞
                    output = io.StringIO()

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º pandas to_csv —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    df_copy.to_csv(
                        output,
                        sep='\t',
                        header=False,
                        index=False,
                        na_rep='',
                        quoting=csv.QUOTE_MINIMAL,
                        lineterminator='\n'
                    )
                    output.seek(0)

                    data_time = time.time() - data_start
                    logger.info(f"‚ö° –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ StringIO: {data_time:.2f}—Å")

                    # COPY –∏–∑ StringIO
                    copy_start = time.time()

                    column_names = [f'"{col.replace('"', '""')}"' for col in df_clean.columns]
                    columns_sql = ', '.join(column_names)

                    copy_command = f'''
                    COPY "{schema}"."{table_name}" ({columns_sql})
                    FROM STDIN
                    WITH (
                        FORMAT CSV,
                        DELIMITER E'\\t',
                        NULL '',
                        HEADER false
                    )
                    '''

                    cursor.copy_expert(copy_command, output)

                    copy_time = time.time() - copy_start
                    logger.info(f"‚ö° COPY StringIO: {copy_time:.2f}—Å")

                    conn.commit()
                    cursor.execute(f'ALTER TABLE "{schema}"."{table_name}" SET LOGGED')
                    conn.commit()

                    total_time = time.time() - start_time
                    rows_per_second = total_rows / total_time if total_time > 0 else 0

                    logger.info(f"‚úÖ COPY StringIO –∑–∞–≤–µ—Ä—à–µ–Ω:")
                    logger.info(f"   üìä –°—Ç—Ä–æ–∫: {total_rows:,}")
                    logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {total_time:.2f}—Å")
                    logger.info(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_second:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")

                except Exception as e:
                    conn.rollback()
                    logger.error(f"üíÄ –û—à–∏–±–∫–∞ COPY StringIO: {e}")
                    raise

    def save_with_copy_from_stringio_safe(self,
                                          df: pd.DataFrame,
                                          table_name: str,
                                          schema: str = 'IFRS Reports') -> None:
        """
        –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –≤–µ—Ä—Å–∏—è COPY —á–µ—Ä–µ–∑ StringIO (–±–µ–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫)
        """
        start_time = time.time()
        total_rows = len(df)

        df_clean = df.dropna(axis=1, how='all')
        df_clean = df_clean.loc[:, df_clean.columns.str.strip() != '']

        logger.info(f"üìù –ë–ï–ó–û–ü–ê–°–ù–´–ô COPY —á–µ—Ä–µ–∑ StringIO: {total_rows:,} —Å—Ç—Ä–æ–∫...")

        with self.get_safe_optimized_connection() as conn:
            with conn.cursor() as cursor:
                try:
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
                    try:
                        cursor.execute(f'CREATE SCHEMA IF NOT EXISTS "{schema}"')
                    except psycopg2.Error:
                        pass

                    column_definitions = []
                    for col_name, dtype in zip(df_clean.columns, df_clean.dtypes):
                        pg_type = self._get_optimized_pg_type(dtype)
                        safe_col_name = col_name.replace('"', '""')
                        column_definitions.append(f'"{safe_col_name}" {pg_type}')

                    cursor.execute(f'DROP TABLE IF EXISTS "{schema}"."{table_name}"')

                    # –û–±—ã—á–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ (–Ω–µ UNLOGGED –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
                    create_table_sql = f'''
                    CREATE TABLE "{schema}"."{table_name}" (
                        {', '.join(column_definitions)}
                    )
                    '''
                    cursor.execute(create_table_sql)

                    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ StringIO
                    data_start = time.time()

                    # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–º–µ–Ω–∞ NaN
                    df_copy = df_clean.fillna('')  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ \N

                    # –°–æ–∑–¥–∞–µ–º StringIO —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±—É—Ñ–µ—Ä–∞
                    output = io.StringIO()

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º pandas to_csv —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    df_copy.to_csv(
                        output,
                        sep='\t',
                        header=False,
                        index=False,
                        na_rep='',
                        quoting=csv.QUOTE_MINIMAL,
                        lineterminator='\n',
                        escapechar='\\',
                        doublequote=True
                    )
                    output.seek(0)

                    data_time = time.time() - data_start
                    logger.info(f"‚ö° –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ StringIO: {data_time:.2f}—Å")

                    # COPY –∏–∑ StringIO
                    copy_start = time.time()

                    column_names = [f'"{col.replace('"', '""')}"' for col in df_clean.columns]
                    columns_sql = ', '.join(column_names)

                    copy_command = f'''
                    COPY "{schema}"."{table_name}" ({columns_sql})
                    FROM STDIN
                    WITH (
                        FORMAT CSV,
                        DELIMITER E'\\t',
                        NULL '',
                        HEADER false,
                        QUOTE '"',
                        ESCAPE '\\'
                    )
                    '''

                    cursor.copy_expert(copy_command, output)

                    copy_time = time.time() - copy_start
                    logger.info(f"‚ö° COPY StringIO: {copy_time:.2f}—Å")

                    conn.commit()

                    total_time = time.time() - start_time
                    rows_per_second = total_rows / total_time if total_time > 0 else 0

                    logger.info(f"‚úÖ –ë–ï–ó–û–ü–ê–°–ù–´–ô COPY StringIO –∑–∞–≤–µ—Ä—à–µ–Ω:")
                    logger.info(f"   üìä –°—Ç—Ä–æ–∫: {total_rows:,}")
                    logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {total_time:.2f}—Å")
                    logger.info(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_second:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
                    logger.info(f"   üéØ –î–ª—è 900K —Å—Ç—Ä–æ–∫ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è: {(900000 / rows_per_second):.1f}—Å")

                except Exception as e:
                    conn.rollback()
                    logger.error(f"üíÄ –û—à–∏–±–∫–∞ –ë–ï–ó–û–ü–ê–°–ù–û–ì–û COPY StringIO: {e}")
                    raise

    def save_ultra_safe(self,
                        df: pd.DataFrame,
                        table_name: str,
                        schema: str = 'IFRS Reports') -> None:
        """
        –£–õ–¨–¢–†–ê-–ë–ï–ó–û–ü–ê–°–ù–ê–Ø –≤–µ—Ä—Å–∏—è –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π PostgreSQL
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–µ–∑–¥–µ, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ
        """
        start_time = time.time()
        total_rows = len(df)

        df_clean = df.dropna(axis=1, how='all')
        df_clean = df_clean.loc[:, df_clean.columns.str.strip() != '']

        logger.info(f"üîí –£–õ–¨–¢–†–ê-–ë–ï–ó–û–ü–ê–°–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞: {total_rows:,} —Å—Ç—Ä–æ–∫...")

        with self.get_safe_optimized_connection() as conn:
            with conn.cursor() as cursor:
                try:
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
                    cursor.execute(f'CREATE SCHEMA IF NOT EXISTS "{schema}"')

                    column_definitions = []
                    for col_name, dtype in zip(df_clean.columns, df_clean.dtypes):
                        pg_type = self._get_optimized_pg_type(dtype)
                        safe_col_name = col_name.replace('"', '""')
                        column_definitions.append(f'"{safe_col_name}" {pg_type}')

                    cursor.execute(f'DROP TABLE IF EXISTS "{schema}"."{table_name}"')

                    # –û–±—ã—á–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ (–±–µ–∑ UNLOGGED)
                    create_table_sql = f'''
                    CREATE TABLE "{schema}"."{table_name}" (
                        {', '.join(column_definitions)}
                    )
                    '''
                    cursor.execute(create_table_sql)

                    # –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ StringIO
                    data_start = time.time()
                    df_copy = df_clean.fillna('')

                    output = io.StringIO()
                    df_copy.to_csv(
                        output,
                        sep='\t',
                        header=False,
                        index=False,
                        na_rep='',
                        lineterminator='\n'
                    )
                    output.seek(0)

                    data_time = time.time() - data_start
                    logger.info(f"‚ö° –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {data_time:.2f}—Å")

                    # –ü—Ä–æ—Å—Ç–æ–π COPY
                    copy_start = time.time()

                    column_names = [f'"{col.replace('"', '""')}"' for col in df_clean.columns]
                    columns_sql = ', '.join(column_names)

                    copy_command = f'''
                    COPY "{schema}"."{table_name}" ({columns_sql})
                    FROM STDIN
                    WITH CSV DELIMITER E'\\t' NULL ''
                    '''

                    cursor.copy_expert(copy_command, output)

                    copy_time = time.time() - copy_start
                    logger.info(f"‚ö° COPY –æ–ø–µ—Ä–∞—Ü–∏—è: {copy_time:.2f}—Å")

                    conn.commit()

                    total_time = time.time() - start_time
                    rows_per_second = total_rows / total_time if total_time > 0 else 0

                    logger.info(f"‚úÖ –£–õ–¨–¢–†–ê-–ë–ï–ó–û–ü–ê–°–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
                    logger.info(f"   üìä –°—Ç—Ä–æ–∫: {total_rows:,}")
                    logger.info(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {total_time:.2f}—Å")
                    logger.info(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_second:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
                    logger.info(f"   üéØ –î–ª—è 900K —Å—Ç—Ä–æ–∫ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è: {(900000 / rows_per_second):.1f}—Å")

                except Exception as e:
                    conn.rollback()
                    logger.error(f"üíÄ –û—à–∏–±–∫–∞ –£–õ–¨–¢–†–ê-–ë–ï–ó–û–ü–ê–°–ù–û–ô –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                    raise

    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def fetch_data(self, query: str, params: tuple = None, schema: str = None):
        conn = None
        try:
            conn = psycopg2.connect(**self.db_config)
            with conn.cursor() as cur:
                cur.execute(query, params)
                return cur.fetchall()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ë–î: {e}")
            return []
        finally:
            if conn:
                conn.close()


def benchmark_all_methods(test_df, client):
    """–ü–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ (–≤–∫–ª—é—á–∞—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏)"""

    methods = [
        ('üõ°Ô∏è  –ë–ï–ó–û–ü–ê–°–ù–´–ô COPY StringIO',
         lambda: client.save_with_copy_from_stringio_safe(test_df.copy(), 'benchmark_safe_stringio')),
        ('‚ö° Execute_values –¢–£–†–ë–û',
         lambda: client.save_with_execute_values_turbo(test_df.copy(), 'benchmark_exec_turbo')),
        ('üöÄ COPY —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª', lambda: client.save_with_temp_file_copy(test_df.copy(), 'benchmark_temp_file')),
        ('üìù COPY —á–µ—Ä–µ–∑ StringIO (–ø–æ–ª–Ω—ã–π)',
         lambda: client.save_with_copy_from_stringio(test_df.copy(), 'benchmark_stringio')),
    ]

    results = []

    for method_name, method_func in methods:
        print(f"\n{'=' * 60}")
        print(f"üß™ –¢–ï–°–¢: {method_name}")
        print(f"{'=' * 60}")

        try:
            start = time.time()
            method_func()
            elapsed = time.time() - start
            rows_per_sec = len(test_df) / elapsed

            result = {
                'method': method_name,
                'time': elapsed,
                'rows_per_sec': rows_per_sec,
                'estimate_900k': (900000 / rows_per_sec) if rows_per_sec > 0 else 0
            }
            results.append(result)

            print(f"‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢:")
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_sec:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
            print(f"   üìä –û—Ü–µ–Ω–∫–∞ –¥–ª—è 900K —Å—Ç—Ä–æ–∫: {result['estimate_900k']:.1f} —Å–µ–∫")

        except Exception as e:
            print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")
            results.append({
                'method': method_name,
                'error': str(e)
            })

    # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n{'=' * 60}")
    print("üèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print(f"{'=' * 60}")

    successful = [r for r in results if 'error' not in r]
    if successful:
        best = min(successful, key=lambda x: x['time'])
        print(f"ü•á –õ–£–ß–®–ò–ô: {best['method']}")
        print(f"   ‚è±Ô∏è  {best['time']:.2f}—Å –¥–ª—è {len(test_df):,} —Å—Ç—Ä–æ–∫")
        print(f"   üìä –û—Ü–µ–Ω–∫–∞ –¥–ª—è 900K: {best['estimate_900k']:.1f}—Å")

        print(f"\nüìà –í–°–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        for result in sorted(successful, key=lambda x: x['time']):
            print(f"   {result['method']}: {result['time']:.2f}—Å ({result['rows_per_sec']:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫)")

    return results


# –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def quick_save_to_db(df: pd.DataFrame,
                     table_name: str,
                     schema: str = 'IFRS Reports',
                     method: str = 'safe') -> None:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è DataFrame –≤ –ë–î

    Args:
        df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        table_name: –ò–º—è —Ç–∞–±–ª–∏—Ü—ã
        schema: –°—Ö–µ–º–∞ –ë–î
        method: –ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ('safe', 'fast', 'turbo', 'temp_file')
    """
    client = ReallyOptimizedDatabaseClient()

    if method == 'safe':
        client.save_with_copy_from_stringio_safe(df, table_name, schema)
    elif method == 'fast':
        client.save_with_copy_from_stringio(df, table_name, schema)
    elif method == 'turbo':
        client.save_with_execute_values_turbo(df, table_name, schema)
    elif method == 'temp_file':
        client.save_with_temp_file_copy(df, table_name, schema)
    else:
        raise ValueError("method –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º –∏–∑: 'safe', 'fast', 'turbo', 'temp_file'")


if __name__ == '__main__':
    # –¢–µ—Å—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞ –≤–∞—à–∏ XML
    print("üèóÔ∏è  –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö...")

    # –†–∞–∑–º–µ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ—Ç–æ–º —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä—É–µ–º –Ω–∞ 900K)
    test_size = 50000  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

    test_data = {
        'Delivery_Number': [f'DEL_{i:08d}' for i in range(test_size)],
        'Customer_Code': [f'CUST_{i % 10000:05d}' for i in range(test_size)],
        'Customer_Name': [f'Customer Company {i % 1000}' for i in range(test_size)],
        'Item_Code': [f'ITEM_{i % 50000:06d}' for i in range(test_size)],
        'Material_Number': [f'MAT_{i % 20000:07d}' for i in range(test_size)],
        'Material_Description': [f'Material Description for item {i}' for i in range(test_size)],
        'CAC_Code': [f'CAC_{i % 100:03d}' for i in range(test_size)],
        'SAC_Code': [f'SAC_{i % 200:03d}' for i in range(test_size)],
        'Segment_Code': [f'SEG_{i % 50:02d}' for i in range(test_size)],
        'Batch_Number': [f'BATCH_{i % 5000:06d}' for i in range(test_size)],
        'Country_Code': np.random.choice(['US', 'DE', 'CN', 'JP', 'KR', 'IN'], test_size),
        'Quantity': np.random.randint(1, 1000, test_size),
        'Unit_Price': np.random.uniform(0.01, 999.99, test_size),
        'Total_Amount': np.random.uniform(1.0, 50000.0, test_size),
        'Delivery_Date': pd.date_range('2024-01-01', periods=test_size, freq='5min'),
        'Status': np.random.choice(['ACTIVE', 'PENDING', 'SHIPPED', 'DELIVERED'], test_size)
    }

    test_df = pd.DataFrame(test_data)
    print(f"üìä –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame: {len(test_df):,} —Å—Ç—Ä–æ–∫ √ó {len(test_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    print(f"üíæ –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {test_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìã –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
    for col, dtype in zip(test_df.columns, test_df.dtypes):
        print(f"   {col}: {dtype}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
    client = ReallyOptimizedDatabaseClient()

    # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
    print(f"\nüöÄ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ (—Ç–æ–ª—å–∫–æ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–µ—Ç–æ–¥)")
    print(f"{'=' * 60}")

    try:
        start_time = time.time()
        quick_save_to_db(test_df.copy(), 'quick_test', method='ultra_safe')
        elapsed = time.time() - start_time
        rows_per_sec = len(test_df) / elapsed
        estimate_900k = 900000 / rows_per_sec

        print(f"‚úÖ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ –ó–ê–í–ï–†–®–ï–ù:")
        print(f"   üìä –°—Ç—Ä–æ–∫: {len(test_df):,}")
        print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   üöÑ –°–∫–æ—Ä–æ—Å—Ç—å: {rows_per_sec:,.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫")
        print(f"   üéØ –û—Ü–µ–Ω–∫–∞ –¥–ª—è 900K —Å—Ç—Ä–æ–∫: {estimate_900k:.1f} —Å–µ–∫—É–Ω–¥")

        if estimate_900k < 60:
            print(f"   üéâ –û–¢–õ–ò–ß–ù–û! –í–∞—à–∏ 900K —Å—Ç—Ä–æ–∫ –∑–∞–≥—Ä—É–∑—è—Ç—Å—è –º–µ–Ω–µ–µ —á–µ–º –∑–∞ –º–∏–Ω—É—Ç—É!")
        elif estimate_900k < 180:
            print(f"   üëç –•–û–†–û–®–û! –í–∞—à–∏ 900K —Å—Ç—Ä–æ–∫ –∑–∞–≥—Ä—É–∑—è—Ç—Å—è –∑–∞ {estimate_900k / 60:.1f} –º–∏–Ω—É—Ç—ã")
        else:
            print(f"   ‚ö†Ô∏è  –ú–µ–¥–ª–µ–Ω–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥ –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ PostgreSQL")

    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê –ë–´–°–¢–†–û–ì–û –¢–ï–°–¢–ê: {e}")

    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –ø–æ–ª–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ
    print(f"\n‚ùì –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤? (y/n): ", end="")
    try:
        user_input = input().lower()
        if user_input in ['y', 'yes', '–¥–∞', '–¥']:
            print(f"\nüöÄ –ü–û–õ–ù–´–ô –ë–ï–ù–ß–ú–ê–†–ö")
            benchmark_results = benchmark_all_methods(test_df, client)
        else:
            print("üëç –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–ø—É—â–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ quick_save_to_db() –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")
    except:
        print("üëç –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–ø—É—â–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ quick_save_to_db() –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–∞—à–∏—Ö XML –¥–∞–Ω–Ω—ã—Ö:
"""
# –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ XML –≤ DataFrame:
from optimized_database_client import ReallyOptimizedDatabaseClient, quick_save_to_db

# –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±:
quick_save_to_db(df, 'xml_data', schema='XML_Import', method='safe')

# –ò–ª–∏ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º:
client = ReallyOptimizedDatabaseClient()
client.save_with_copy_from_stringio_safe(df, 'xml_deliveries', schema='XML_Import')

# –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ (–µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ PostgreSQL):
client.save_with_temp_file_copy(df, 'xml_deliveries', schema='XML_Import')
"""
