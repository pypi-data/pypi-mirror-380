{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "from xwords import XWords, XWordsSolver\n",
    "import copy\n",
    "import os\n",
    "_dir_    = '../../../data/crossword_puzzle_screenshots/'\n",
    "_files_  = os.listdir(_dir_)\n",
    "\n",
    "_puzzles_ = {}\n",
    "\n",
    "_ENTRIES_    = '_entries.txt'\n",
    "_GEOMETRIES_ = '_geometries.txt'\n",
    "_BLOCKERS_   = '_blockers.txt'\n",
    "_ANSWERS_    = '_answers.txt'\n",
    "for _file_ in _files_:\n",
    "    if _file_.endswith(_ENTRIES_):\n",
    "        _prefix_ = _file_[:-len(_ENTRIES_)]\n",
    "        if _prefix_+_GEOMETRIES_ in _files_ and _prefix_+_BLOCKERS_ in _files_:\n",
    "            if _prefix_+_ANSWERS_ in _files_: xwords = XWords(rt, _dir_+_file_, _dir_+_prefix_+_GEOMETRIES_, _dir_+_prefix_+_BLOCKERS_, _dir_+_prefix_+_ANSWERS_)\n",
    "            else:                             xwords = XWords(rt, _dir_+_file_, _dir_+_prefix_+_GEOMETRIES_, _dir_+_prefix_+_BLOCKERS_, None)\n",
    "            _puzzles_[_prefix_] = xwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadResultsAndAnswers():\n",
    "    df_results_list = []\n",
    "    df_answers_list = []\n",
    "    _files_ = os.listdir(_dir_)\n",
    "    for _file_ in _files_:\n",
    "        if   _file_.endswith('_xwords_answers.parquet'): df_answers_list.append(pd.read_parquet(_dir_ + _file_))\n",
    "        elif _file_.endswith('_xwords_results.parquet'): df_results_list.append(pd.read_parquet(_dir_ + _file_))\n",
    "    return pd.concat(df_results_list), pd.concat(df_answers_list)\n",
    "df_results, df_answers = loadResultsAndAnswers()\n",
    "print(f'{len(df_results)=} {len(df_answers)=}')\n",
    "print(set(df_results['puzzle_prefix']))\n",
    "df_results.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib.util\n",
    "import inspect\n",
    "import requests\n",
    "def getClassesFromFile(file_path):\n",
    "    classes = []\n",
    "    module_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    spec        = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module      = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    for name, obj in inspect.getmembers(module):\n",
    "        if inspect.isclass(obj) and obj.__module__ == module.__name__: classes.append(obj)\n",
    "    return classes\n",
    "# https://stackoverflow.com/questions/79372940/how-to-get-a-list-of-models-available-in-ollama-using-langchain\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
    "def getInstalledModels() -> list:\n",
    "    thelist = requests.get(OLLAMA_URL+\"/api/tags\")\n",
    "    jsondata = thelist.json()\n",
    "    result = list()\n",
    "    for model in jsondata[\"models\"]: \n",
    "        _model_ = model[\"model\"]\n",
    "        if _model_.endswith(\":latest\"): _model_ = _model_[:-len(':latest')]\n",
    "        result.append(_model_)\n",
    "    return result\n",
    "all_models = set(getInstalledModels()) - set(['nomic-embed-text']) # remove embedding models\n",
    "all_models = all_models - set(['qwen3:0.6b']) # this model won't finish... \n",
    "print(f'total models: {len(all_models)} | ', all_models) # all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lu = {} # resets the answers seen so far... don't do this or it loses all the work done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _prefix_ in _puzzles_.keys():\n",
    "    _models_ = all_models - set(df_answers.query('puzzle_prefix == @_prefix_')['model'])\n",
    "    print(f'{_prefix_}: remaining models: {len(_models_)} | ', _models_)\n",
    "    if len(_models_) == 0: continue\n",
    "\n",
    "    _models_ = list(_models_) # [:10] # just do a few models...  this isn't all that stable...\n",
    "\n",
    "    for _model_ in _models_:\n",
    "        print(_model_)\n",
    "        for _filename_ in os.listdir():\n",
    "            if _filename_.endswith('.py') == False: continue\n",
    "            for _class_ in getClassesFromFile(_filename_):\n",
    "                if issubclass(_class_, XWordsSolver):\n",
    "                    if _prefix_         not in results_lu:           results_lu[_prefix_]                   = {}    \n",
    "                    if _class_.__name__ not in results_lu[_prefix_]: results_lu[_prefix_][_class_.__name__] = {}\n",
    "                    print(_class_)\n",
    "                    if _model_ in results_lu[_prefix_][_class_.__name__]: \n",
    "                        print('skipping ', _model_, _class_.__name__)\n",
    "                        continue\n",
    "                    xwords_copy = copy.deepcopy(_puzzles_[_prefix_])\n",
    "                    _instance_ = _class_(xwords=xwords_copy, model=_model_)\n",
    "                    answer_lu, request_stats, num_of_llm_requests = _instance_.solve()\n",
    "                    char_level_acc = xwords_copy.characterLevelAccuracy()\n",
    "                    results_lu[_prefix_][_class_.__name__][_model_] = (answer_lu, request_stats, num_of_llm_requests, char_level_acc, xwords_copy)\n",
    "                    print(f'\\n{_class_.__name__} {_model_} {char_level_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tiles_ = []\n",
    "for _prefix_ in results_lu.keys():\n",
    "    for _algo_ in results_lu[_prefix_].keys():\n",
    "        for _model_ in _models_:\n",
    "            if _model_ not in results_lu[_prefix_][_algo_]: _xwords_ = copy.deepcopy(xwords)\n",
    "            else:                                           _xwords_ = results_lu[_prefix_][_algo_][_model_][4]\n",
    "            _tiles_.append(_xwords_.smallMultipleSVG())\n",
    "rt.table(_tiles_, per_row=len(_models_), spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_answers_ = {'algorithm':[], 'model':[], 'clue_number':[], 'orientation':[], 'answer':[], 'puzzle_prefix':[]}\n",
    "_df_results_ = {'algorithm':[], 'model':[], 'char_level_accuracy':[], 'num_of_llm_requests':[], 'time':[], 'prompt_tokens':[], 'output_tokens':[], 'puzzle_prefix':[]}\n",
    "for _prefix_ in results_lu.keys():\n",
    "    for _algo_ in results_lu[_prefix_].keys():\n",
    "        for _model_ in results_lu[_prefix_][_algo_].keys():\n",
    "            _tuple_ = results_lu[_prefix_][_algo_][_model_]\n",
    "            _time_sum_, _prompt_sum_, _output_sum_ = 0.0, 0, 0\n",
    "            for x in _tuple_[1]:\n",
    "                _time_sum_   += x[2]\n",
    "                _prompt_sum_ += x[3]\n",
    "                _output_sum_ += x[4]\n",
    "            _num_of_llm_requests_ = _tuple_[2]\n",
    "            _char_level_accuracy_ = _tuple_[3]\n",
    "            for _clue_ in _tuple_[0].keys():\n",
    "                _df_answers_['algorithm'].append(_algo_)\n",
    "                _df_answers_['model'].append(_model_)\n",
    "                _df_answers_['clue_number'].append(_clue_[0])\n",
    "                _df_answers_['orientation'].append(_clue_[1])\n",
    "                _df_answers_['answer'].append(_tuple_[0][_clue_])\n",
    "                _df_answers_['puzzle_prefix'].append(_prefix_)\n",
    "            _df_results_['algorithm'].append(_algo_)\n",
    "            _df_results_['model'].append(_model_)\n",
    "            _df_results_['char_level_accuracy'].append(_char_level_accuracy_)\n",
    "            _df_results_['num_of_llm_requests'].append(_num_of_llm_requests_)\n",
    "            _df_results_['time'].append(_time_sum_)\n",
    "            _df_results_['prompt_tokens'].append(_prompt_sum_)\n",
    "            _df_results_['output_tokens'].append(_output_sum_)\n",
    "            _df_results_['puzzle_prefix'].append(_prefix_)\n",
    "df_answers_new = pd.DataFrame(_df_answers_)\n",
    "df_results_new = pd.DataFrame(_df_results_)\n",
    "\n",
    "if len(df_answers_new) > 0:\n",
    "    _filename_ = _dir_ + '20250519m_xwords_answers.parquet'\n",
    "    if os.path.exists(_filename_): raise Exception('file already exists')\n",
    "    df_answers_new.to_parquet(_filename_)\n",
    "    _filename_ = _dir_ + '20250519m_xwords_results.parquet'\n",
    "    if os.path.exists(_filename_): raise Exception('file already exists')\n",
    "    df_results_new.to_parquet(_filename_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results, df_answers = loadResultsAndAnswers()\n",
    "\n",
    "_algorithms_ = list(set(df_results['algorithm']))\n",
    "_colors_     = rt.co_mgr.brewerColors(scale_type='qualitative', n=len(_algorithms_), alt=1)\n",
    "for i in range(len(_algorithms_)): rt.co_mgr.str_to_color_lu[_algorithms_[i]] = _colors_[i]\n",
    "\n",
    "parms = {'color_by':'algorithm', 'w':384, 'h':384}\n",
    "rt.tile([rt.xy       (df_results, x_field='time', y_field='char_level_accuracy', dot_size='large', **parms),\n",
    "         rt.histogram(df_results, bin_by='algorithm',           count_by='char_level_accuracy', color_by='algorithm', h=384, w=256),\n",
    "         rt.histogram(df_results, bin_by=['model','algorithm'], count_by='char_level_accuracy', **parms)], spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers['answer_lower'] = df_answers['answer'].str.lower()\n",
    "_orientation_, _clue_num_ = 'down', 65\n",
    "rt.tile([rt.histogram(df_answers.query('orientation == @_orientation_ and clue_number == @_clue_num_'), bin_by='answer_lower', color_by='algorithm', bar_h=20, w=384, h=670),\n",
    "         xwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Best possible w/ the small models... if the correct answer were chosen...\n",
    "# ... 0.60 character level accuracy for cogito:70b\n",
    "#\n",
    "_sorter_ = []\n",
    "for _model_ in set(df_answers['model']):\n",
    "    xwords.clearAll()\n",
    "    for _tuple_ in xwords.entries:\n",
    "        _clue_num_, _orientation_ = _tuple_\n",
    "        _df_ = df_answers.query('clue_number == @_clue_num_ and orientation == @_orientation_ and model == @_model_')\n",
    "        if len(_df_) == 0: continue\n",
    "        if xwords.answer(_clue_num_, _orientation_).lower() in set(_df_['answer_lower']):\n",
    "            xwords.guess(_clue_num_, _orientation_, xwords.answer(_clue_num_, _orientation_))\n",
    "    _sorter_.append((xwords.characterLevelAccuracy(), _model_))\n",
    "_sorter_.sort(reverse=True)\n",
    "for _tuple_ in _sorter_:\n",
    "    print(f'{_tuple_[1]:>24} | {_tuple_[0]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Best possible w/ the small models... if the correct answer were chosen...\n",
    "# ... 0.83 character level accuracy\n",
    "#\n",
    "xwords.clearAll()\n",
    "for _tuple_ in xwords.entries:\n",
    "    _clue_num_, _orientation_ = _tuple_\n",
    "    _df_ = df_answers.query('clue_number == @_clue_num_ and orientation == @_orientation_')\n",
    "    if len(_df_) == 0: continue\n",
    "    if xwords.answer(_clue_num_, _orientation_).lower() in set(_df_['answer_lower']):\n",
    "        xwords.guess(_clue_num_, _orientation_, xwords.answer(_clue_num_, _orientation_))\n",
    "xwords.characterLevelAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Which models were the only one to get the right answer?\n",
    "#\n",
    "# llama4:scout  2\n",
    "# cogito:70b    1\n",
    "# llama3.3      1\n",
    "# qwen3:8b      1\n",
    "#\n",
    "_lu_ = {'model':[], 'clue_number':[], 'orientation':[]}\n",
    "for _tuple_ in xwords.entries:\n",
    "    _clue_num_, _orientation_ = _tuple_\n",
    "    _answer_                  = xwords.answer(_clue_num_, _orientation_).lower()\n",
    "    _df_ = df_answers.query('clue_number == @_clue_num_ and orientation == @_orientation_ and answer_lower == @_answer_')\n",
    "    if len(set(_df_['model'])) != 1: continue\n",
    "    _lu_['model'].append(list(set(_df_['model']))[0])\n",
    "    _lu_['clue_number'].append(_clue_num_)\n",
    "    _lu_['orientation'].append(_orientation_)\n",
    "rt.histogram(pd.DataFrame(_lu_), bin_by='model', w=256, h=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
