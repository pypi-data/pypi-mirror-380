{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import exists\n",
    "import time\n",
    "from rtsvg import *\n",
    "rt = RACETrack()\n",
    "ofv = rt.ontologyFrameworkInstance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Keep track of all emails seen\n",
    "#\n",
    "emails_seen = set()\n",
    "\n",
    "#\n",
    "# Email File First // file has utf8 issues :(\n",
    "#\n",
    "_bin_ = open('../../../data/2014_vast/MC1/email headers.csv', 'rb').read()\n",
    "print(len(_bin_))\n",
    "as_str = []\n",
    "for i in range(len(_bin_)):\n",
    "    c = chr(_bin_[i])\n",
    "    if (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z') or (c >= '0' and c <= '9') or \\\n",
    "       c == ' ' or c == '.' or c == ',' or c == '_' or \\\n",
    "       c == '-' or c == '!' or c == '@' or c == '\"' or \\\n",
    "       c == ':' or c == '\\t' or c == '\\n' or c == '\\r' or \\\n",
    "       c == '/' or c == '?' or c == ')' or c == '(' or \\\n",
    "       c == \"'\":\n",
    "        as_str.append(c)\n",
    "    elif ord(c) == 146:\n",
    "        as_str.append(\"'\")\n",
    "    else:\n",
    "        print('\"'+c+'\"', ord(c))\n",
    "_str_ = ''.join(as_str)\n",
    "print(len(_str_))\n",
    "open('../../../data/2014_vast/MC1/email_headers_fixed.csv', 'wt').write(_str_)\n",
    "df_email = pl.read_csv('../../../data/2014_vast/MC1/email_headers_fixed.csv')\n",
    "df_email.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Ontology Based On:\n",
    "# https://www.semanticdesktop.org/ontologies/2007/03/22/nmo/\n",
    "#\n",
    "_lu_ = {'__id_email__':     [],\n",
    "        'timestamp':        [],\n",
    "        'timestamp_str':    [],\n",
    "        '__id_sender__':    [],\n",
    "        'sender':           [],\n",
    "        '__id_recipient__': [],\n",
    "        'recipient':        [],\n",
    "        'subject':          []}\n",
    "for i in range(len(df_email)):\n",
    "    _id_              = ofv.createId('nmo:Email')\n",
    "    _fm_              = df_email['From'][i].strip().lower()\n",
    "    emails_seen.add(_fm_)\n",
    "    _fm_id_           = ofv.resolveUniqIdAndUpdateLookups(_fm_, 'nmo:EmailAddress', 'uniq', 'sbj,obj')\n",
    "    _timestamp_str_   = df_email['Date'][i]\n",
    "    _timestamp_       = str(pd.to_datetime(_timestamp_str_))\n",
    "    #_ts_as_int_       = ofv.resolveUniqIdAndUpdateLookups(_timestamp_, 'xsd:dateTime', 'dttm', 'obj')\n",
    "    _subject_         = df_email['Subject'][i].strip()\n",
    "    #_subject_literal_ = ofv.resolveUniqIdAndUpdateLookups(_subject_, 'nmo:MessageHeader', 'cont', 'obj')\n",
    "    #ofv.bufferTripleToAddLater(_id_, 'nmo:emailFrom',   _fm_id_)\n",
    "    #ofv.bufferTripleToAddLater(_id_, 'nmo:sentDate',    _ts_as_int_)\n",
    "    #ofv.bufferTripleToAddLater(_id_, 'nmo:headerValue', _subject_literal_)\n",
    "    _to_              = df_email['To'][i]\n",
    "    for _to_actual_ in _to_.split(','):\n",
    "        _to_actual_ = _to_actual_.strip().lower()\n",
    "        emails_seen.add(_to_actual_)\n",
    "        _to_actual_id_ = ofv.resolveUniqIdAndUpdateLookups(_to_actual_, 'nco:EmailAddress', 'uniq', 'sbj,obj')\n",
    "        #ofv.bufferTripleToAddLater(_id_, 'nmo:emailTo', _to_actual_id_)\n",
    "        _lu_['__id_email__']    .append(_id_),           _lu_['timestamp'] .append(_timestamp_),    _lu_['timestamp_str'].append(_timestamp_str_)\n",
    "        _lu_['__id_sender__']   .append(_fm_id_),        _lu_['sender']    .append(_fm_)\n",
    "        _lu_['__id_recipient__'].append(_to_actual_id_), _lu_['recipient'] .append(_to_actual_)\n",
    "        _lu_['subject']         .append(_subject_)\n",
    "#ofv.appendBufferedTriplesAndClearBuffer()\n",
    "#len(ofv.df_triples)\n",
    "df_email_ont = pl.DataFrame(_lu_)\n",
    "df_email_ont = rt.columnsAreTimestamps(df_email_ont, 'timestamp')\n",
    "df_email_ont_mapping = [ ['@__id_email__', 'nmo:emailFrom',   '@__id_sender__'],\n",
    "                         ['@__id_email__', 'nmo:sendDate',    '@timestamp'],\n",
    "                         ['@__id_email__', 'nmo:headerValue', '@subject'],\n",
    "                         ['@__id_email__', 'nmo:emailTo',     '@__id_recipient__'] ]\n",
    "ofv.addTabularDataFrame(df_email_ont, df_email_ont_mapping)\n",
    "df_email_ont.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee_recs = pl.read_excel('../../../data/2014_vast/MC1/EmployeeRecords.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixDate(_date_):\n",
    "    if type(_date_) != str: return _date_\n",
    "    _split_ = _date_.split('-')\n",
    "    _year_, _month_, _day_ = int(_split_[2]), int(_split_[0]), int(_split_[1])\n",
    "    if _year_ < 30: _year_ += 2000\n",
    "    else:           _year_ += 1900\n",
    "    _date_str_ = f'{_year_}-{_month_:02}-{_day_:02}'\n",
    "    return _date_str_\n",
    "\n",
    "#\n",
    "# https://w3id.org/MON/person.owl\n",
    "# https://www.w3.org/TR/vocab-org/\n",
    "#\n",
    "#\n",
    "for i in range(len(df_employee_recs)):\n",
    "    _id_ = ofv.createId('mon:Person')\n",
    "    _last_, _first_, _dob_, _pob_, _gender_ = df_employee_recs['LastName'][i], df_employee_recs['FirstName'][i], df_employee_recs['BirthDate'][i], df_employee_recs['BirthCountry'][i], df_employee_recs['Gender'][i]\n",
    "    _last_id_   = ofv.resolveUniqIdAndUpdateLookups(_last_,         'xsd:string', 'ambi', 'obj')\n",
    "    _first_id_  = ofv.resolveUniqIdAndUpdateLookups(_first_,        'xsd:string', 'ambi', 'obj')\n",
    "    _pob_id_    = ofv.resolveUniqIdAndUpdateLookups(_pob_,          'xsd:string', 'uniq', 'obj')\n",
    "    _dob_id_    = ofv.resolveUniqIdAndUpdateLookups(fixDate(_dob_), 'xsd:date',   'date', 'obj')\n",
    "    _gender_id_ = ofv.resolveUniqIdAndUpdateLookups(_gender_,       'mon:Sex',    'cata', 'obj')\n",
    "\n",
    "    ofv.bufferTripleToAddLater(_id_, 'mon:hasBirthDate',  _dob_id_)\n",
    "    ofv.bufferTripleToAddLater(_id_, 'mon:hasBirthPlace', _pob_id_)\n",
    "    ofv.bufferTripleToAddLater(_id_, 'mon:hasGender',     _gender_id_)\n",
    "    ofv.bufferTripleToAddLater(_id_, 'mon:lastName',      _last_id_)\n",
    "    ofv.bufferTripleToAddLater(_id_, 'mon:firstName',     _first_id_)\n",
    "\n",
    "    _employment_department_  = df_employee_recs['CurrentEmploymentType'][i]\n",
    "    _employment_title_       = df_employee_recs['CurrentEmploymentTitle'][i]\n",
    "    _employment_start_date_  = df_employee_recs['CurrentEmploymentStartDate'][i]\n",
    "\n",
    "    _employment_id_          = ofv.createId('org:Employment')\n",
    "    ofv.bufferTripleToAddLater(_id_, 'hasEmployment',  _employment_id_)\n",
    "\n",
    "    _employment_department_id_    = ofv.resolveUniqIdAndUpdateLookups(_employment_department_,          'org:Organization',      'uniq',     'sbj,obj')\n",
    "    _employment_title_id_         = ofv.resolveUniqIdAndUpdateLookups(_employment_title_,               'org:EmploymentTitle',   'ambi',     'sbj,obj')\n",
    "    _employment_start_date_id_    = ofv.resolveUniqIdAndUpdateLookups(fixDate(_employment_start_date_), 'xsd:date',              'date',     'obj')\n",
    "\n",
    "    ofv.bufferTripleToAddLater(_employment_id_, 'employedBy',          _employment_department_id_)\n",
    "    ofv.bufferTripleToAddLater(_employment_id_, 'hasEmploymentTitle',  _employment_title_id_)\n",
    "    ofv.bufferTripleToAddLater(_employment_id_, 'hasStartDate',        _employment_start_date_id_)\n",
    "\n",
    "    _email_address_          = df_employee_recs['EmailAddress'][i].strip().lower()\n",
    "    emails_seen.add(_email_address_)\n",
    "    _email_address_id_       = ofv.resolveUniqIdAndUpdateLookups(_email_address_, 'nmo:EmailAddress', 'uniq', 'sbj,obj')\n",
    "\n",
    "    ofv.bufferTripleToAddLater(_id_, 'nmo:hasEmailAddress', _email_address_id_)\n",
    "\n",
    "ofv.appendBufferedTriplesAndClearBuffer()\n",
    "len(ofv.df_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_params = {'relationships':[('sbj','obj')], 'link_opacity':0.1, 'node_labels':ofv.nodeLabels()}\n",
    "_rtg_ = rt.interactiveGraphPanel(ofv.df_triples, ln_params, w=1000, h=600)\n",
    "if exists('../../../data/2014_vast/layout_example.parquet'): _rtg_.loadLayout('../../../data/2014_vast/layout_example.parquet')\n",
    "# _rtg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rt.link(ofv.df_triples, **ln_params)\n",
    "#_rtg_.selectEntities('Morlun',method='substring')\n",
    "#_rtg_.selectedEntities()\n",
    "#_rtg_.saveLayout('../../../data/2014_vast/layout_example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-docx\n",
    "import docx\n",
    "import re\n",
    "\n",
    "def findClosestEmail(filename):\n",
    "    filename = filename.lower()\n",
    "    best_match, best_count = None, 0\n",
    "    for _email_ in emails_seen:\n",
    "        username = _email_.split('@')[0]\n",
    "        parts = re.split(' |\\\\-|\\\\.', username)\n",
    "        part_count = 0\n",
    "        for _part_ in parts:\n",
    "            if _part_ in filename: part_count += 1\n",
    "        if part_count > best_count: best_count, best_match = part_count, _email_\n",
    "    return best_match\n",
    "\n",
    "_lu_ = {'email':[], '__id_email__':[], 'resumefile':[], 'associated_entity':[], 'associated_entity_type':[]}\n",
    "_base_resumes_dir_ = '../../../data/2014_vast/MC1/resumes/'\n",
    "for _file_ in os.listdir(_base_resumes_dir_):\n",
    "    _email_    = findClosestEmail(_file_)\n",
    "    if _email_ is None: print(f'No email for {_file_}')\n",
    "    _email_id_ = ofv.resolveUniqIdAndUpdateLookups(_email_, 'nmo:EmailAddress', 'uniq', 'sbj,obj')\n",
    "    _docx_     = docx.Document(_base_resumes_dir_+_file_)\n",
    "    for _para_ in _docx_.paragraphs:\n",
    "        if len(_para_.text) > 0:\n",
    "            _extracts_ = rt.textExtractEntities(_para_.text)\n",
    "            for _extract_ in _extracts_:\n",
    "                _lu_['email'].append(_email_)\n",
    "                _lu_['__id_email__'].append(_email_id_)\n",
    "                _lu_['resumefile'].append(_file_)\n",
    "                _lu_['associated_entity'].append(_extract_[0])\n",
    "                _lu_['associated_entity_type'].append(_extract_[1])\n",
    "df_resumes = pl.DataFrame(_lu_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_resume_params = {'relationships':[('email','associated_entity')], 'link_opacity':0.1, 'node_labels':ofv.nodeLabels()}\n",
    "_g_resume_   = rt.createNetworkXGraph(df_resumes, ln_resume_params['relationships'])\n",
    "_pos_        = rt.hyperTreeLayout(_g_resume_)\n",
    "ln_resume_params['pos'] = _pos_\n",
    "_rtg_resume_ = rt.interactiveGraphPanel(df_resumes, ln_resume_params, w=1000, h=600)\n",
    "#_rtg_resume_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_base_dir_ = '../../../data/2014_vast/MC1/News Articles'\n",
    "_lu_ = {'__id_article__':[], 'directory':[], 'file':[], 'source':[], 'title':[], 'published':[], 'article':[], 'author':[], 'location':[]}\n",
    "for _file_ in os.listdir(_base_dir_):\n",
    "    _txt_raw_     = open(os.path.join(_base_dir_, _file_), 'rb').read()        \n",
    "    _txt_better_  = str(_txt_raw_, encoding='latin').split('\\n')\n",
    "    _txt_         = []\n",
    "    for _line_ in _txt_better_:\n",
    "        _line_ = _line_.strip()\n",
    "        if _line_ == '' or _line_ == '\\n' or _line_ == '\\r': continue\n",
    "        _txt_.append(_line_)\n",
    "    _source_           = _txt_[0]\n",
    "    _title_            = _txt_[1]\n",
    "    _author_           = _txt_[2]\n",
    "    def numberInString(_string_): return any(char.isdigit() for char in _string_)\n",
    "    if '/' in _author_ or numberInString(_author_): \n",
    "        _author_, _published_ = None, _author_\n",
    "        _just_article_txt_    = _txt_[3:]\n",
    "    else:               \n",
    "        _published_           = _txt_[3]\n",
    "        _just_article_txt_    = _txt_[4:]\n",
    "    if 'drug' in _published_.lower():\n",
    "        _published_ = None\n",
    "        _just_article_txt_ = _txt_[3:]\n",
    "\n",
    "    _location_ = None\n",
    "    if ' - ' in _just_article_txt_[0][:50]:\n",
    "        _possible_ = _just_article_txt_[0][:_just_article_txt_[0].find(' - ')].strip()\n",
    "        if ',' in _possible_:\n",
    "            _location_ = _possible_.lower()\n",
    "            _just_article_txt_[0] = _just_article_txt_[0][_just_article_txt_[0].find(' - ')+3:].strip()\n",
    "    _id_ = ofv.createId('NewsArticle')\n",
    "    _lu_['__id_article__'].append(_id_)\n",
    "    _lu_['directory'].append(_source_), _lu_['file'].append(_file_), _lu_['author'].append(_author_), _lu_['location'].append(_location_)\n",
    "    _lu_['title'].append(_title_), _lu_['source'].append(_source_), _lu_['published'].append(_published_), _lu_['article'].append('\\n'.join(_just_article_txt_))\n",
    "\n",
    "df_articles = pd.DataFrame(_lu_)\n",
    "df_articles = rt.columnsAreTimestamps(df_articles, 'published')\n",
    "df_articles.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_base_ontology_name_ = '../../../data/2014_vast/MC1/ontology_rep/20240711_ontology'\n",
    "ofv.to_files(_base_ontology_name_)\n",
    "#ofv_load = rt.ontologyFrameworkInstance(base_filename=_base_ontology_name_)\n",
    "#print(f'{len(ofv_load.df_triples)=} {len(ofv.df_triples)=}')\n",
    "#print(f'{len(ofv_load.uid_lu)=} {len(ofv.uid_lu)=}')\n",
    "#set(ofv_load.uid_lu) - set(ofv.uid_lu), set(ofv.uid_lu) - set(ofv_load.uid_lu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
