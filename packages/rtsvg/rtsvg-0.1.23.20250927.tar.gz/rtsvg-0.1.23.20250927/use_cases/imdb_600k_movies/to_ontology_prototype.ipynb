{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '../../rtsvg')\n",
    "from rtsvg import *\n",
    "rt = RACETrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonpath_ng import jsonpath, parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scanForward() - finds the next unescaped version of character c in x starting at i\n",
    "def scanForward(x, i, c):\n",
    "    in_escape = False\n",
    "    while i < len(x):\n",
    "        if   x[i] == '\\\\' and in_escape == False: in_escape = True\n",
    "        else:\n",
    "            if x[i] == c and in_escape == False: return i\n",
    "            in_escape = False\n",
    "        i += 1\n",
    "    return None\n",
    "\n",
    "# literalize() - converts any single or double quoted strings into unique literal names\n",
    "# ... fails if inputs contain four underscore names that overlap with the format \"____lit{num}____\"\n",
    "def literalize(x):\n",
    "    l, lu = [], {}\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        c = x[i]\n",
    "        if   c == \"'\":\n",
    "            j = scanForward(x, i+1, \"'\")\n",
    "            if j is None: raise Exception(f'RTOntology.literalize() - unterminated string literal \"{x}\"')\n",
    "            _literal_name_ = f'____lit{len(lu.keys())}____' # Surely, no one would ever use four underscores in a literal... and don't call me Surely\n",
    "            lu[_literal_name_] = x[i+1:j]\n",
    "            l.append(_literal_name_)\n",
    "            i = j + 1\n",
    "        elif c == '\"':\n",
    "            j = scanForward(x, i+1, '\"')\n",
    "            if j is None: raise Exception(f'RTOntology.literalize() - unterminated string literal \"{x}\"')\n",
    "            _literal_name_ = f'____lit{len(lu.keys())}____' # Surely, no one would ever use four underscores in a literal... and don't call me Surely\n",
    "            lu[_literal_name_] = x[i+1:j]\n",
    "            l.append(_literal_name_)\n",
    "            i = j + 1\n",
    "        else:\n",
    "            l.append(c)\n",
    "            i += 1\n",
    "    return ''.join(l), lu\n",
    "\n",
    "# fillLiteratals() - fill in the literal values (opposite of literalize but not guaranteed to keep spaces)\n",
    "def fillLiterals(x, lu):\n",
    "    for k, v in lu.items():\n",
    "        x = x.replace(k, v)\n",
    "    return x\n",
    "\n",
    "# findClosingParen() - find the next closing paren taking other open/closes into consideration\n",
    "# ... requires that literals were taken care of... [see literalize function]\n",
    "def findClosingParen(s, i):\n",
    "    stack = 0\n",
    "    while i < len(s):\n",
    "        if   s[i] == '(':               stack += 1\n",
    "        elif s[i] == ')' and stack > 0: stack -= 1\n",
    "        elif s[i] == ')':               return i\n",
    "        i += 1\n",
    "    raise Exception(f'RTOntology.findClosingParen() - no closing paren found for \"{s}\"')\n",
    "\n",
    "# tokenizeParameters() - create a token list of function parameters\n",
    "# ... requires that literals were taken care of... [see literalize function]\n",
    "def tokenizeParameters(x):\n",
    "    r = []\n",
    "    while ',' in x or '(' in x:\n",
    "        if   ',' in x and '(' in x: # both... process the one that occurs first\n",
    "            i = x.index(',')\n",
    "            j = x.index('(')\n",
    "            if i < j:\n",
    "                r.append(x[:i])\n",
    "                x = x[i+1:]\n",
    "            else:\n",
    "                k = findClosingParen(x,j+1)\n",
    "                r.append(x[:k+1].strip())\n",
    "                x = x[k+1:]\n",
    "                if ',' in x: x = x[x.index(',')+1:] # if there's another comma, consume it\n",
    "        elif ',' in x: # just literals from here on out...\n",
    "            r.append(x[:x.index(',')].strip())\n",
    "            x = x[x.index(',')+1:]\n",
    "        elif '(' in x: # just one function call from here on out...\n",
    "            i = x.index('(')\n",
    "            j = findClosingParen(x,i+1)\n",
    "            r.append(x[:j+1].strip())\n",
    "            x = x[j+1:]\n",
    "            if ',' in x: x = x[x.index(',')+1:] # if there's another comma, consume it\n",
    "    x = x.strip()\n",
    "    if len(x) > 0:\n",
    "        r.append(x)\n",
    "    return r\n",
    "\n",
    "# parseTree() - create a parse tree representation of a ontology node description\n",
    "def parseTree(x, node_value=None, node_children=None, node_name=None, lit_lu=None):\n",
    "    if node_value is None:\n",
    "        node_value = {}\n",
    "        node_children = {}\n",
    "        node_name = 'root'\n",
    "\n",
    "    if lit_lu is None:\n",
    "        x, lit_lu = literalize(x)\n",
    "    if '(' in x:\n",
    "        i          = x.index('(')\n",
    "        j          = findClosingParen(x, i+1)\n",
    "        fname      = x[0:i].strip()\n",
    "        parms      = tokenizeParameters(x[i+1:j])\n",
    "        node_value   [node_name] = lit_lu[fname] if fname in lit_lu else fname\n",
    "        node_children[node_name] = []    # functions have children... even if it's an empty list of children\n",
    "        for child_i in range(len(parms)):\n",
    "            child_name = f'{node_name}.{child_i}'\n",
    "            node_children[node_name].append(child_name)\n",
    "            parseTree(parms[child_i], node_value, node_children, child_name, lit_lu)\n",
    "    else:\n",
    "        x                        = x.strip()\n",
    "        node_value   [node_name] = lit_lu[x] if x in lit_lu else x\n",
    "        node_children[node_name] = None # literals have no children\n",
    "    return node_value, node_children\n",
    "\n",
    "# solveParseTree() - evaluate a parse tree\n",
    "def solveParseTree(values, children, filled, i, node=None):\n",
    "    if node is None: node = 'root'\n",
    "    if   children[node] is None and isJsonPath(values[node]):\n",
    "        return filled[values[node]][i]  # jsonpath filled in value from the json\n",
    "    elif children[node] is None:\n",
    "        return values[node]             # constant / literal\n",
    "    else:\n",
    "        parms = [solveParseTree(values, children, filled, i, x) for x in children[node]]\n",
    "        return eval(f'{values[node]}(*parms)')\n",
    "\n",
    "# upToStar() - upto the cth '[*]'\n",
    "def upToStar(x, c):\n",
    "    i = 0\n",
    "    while c > 0:\n",
    "        j = x.index('[*]', i)\n",
    "        i = j + 3\n",
    "        c -= 1\n",
    "    return x[:i]\n",
    "\n",
    "# fillStars() - fill the the stars in the specified order\n",
    "def fillStars(x, i, j=None, k=None):\n",
    "    if '[*]' not in x: return x # for example ... \"$.id\"\n",
    "    _index_ = x.index('[*]')\n",
    "    x = x[:_index_] + f'[{i}]' + x[_index_+3:]\n",
    "    if j is not None and '[*]' in x:\n",
    "        _index_ = x.index('[*]')\n",
    "        x = x[:_index_] + f'[{j}]' + x[_index_+3:]\n",
    "    if k is not None and '[*]' in x:\n",
    "        _index_ = x.index('[*]')\n",
    "        x = x[:_index_] + f'[{k}]' + x[_index_+3:]\n",
    "    return x\n",
    "\n",
    "# isJsonPath() - check if the string is a jsonpath\n",
    "def isJsonPath(_str_): \n",
    "    return _str_.startswith('$.') or _str_.startswith('$[')\n",
    "\n",
    "#\n",
    "# fillJSONPathElementsByJSONPath() - unoptimized version using jsonpath-ng\n",
    "# - to_fill should only have values with a [*] in them\n",
    "#\n",
    "def fillJSONPathElementsByJSONPath(to_fill, myjson):\n",
    "    longest_by_star_path, min_stars, max_stars = None, None, None\n",
    "    filled = {}\n",
    "    for x in to_fill:\n",
    "        star_count = x.count('[*]')\n",
    "        if min_stars is None or star_count < min_stars: min_stars = star_count\n",
    "        if max_stars is None or star_count > max_stars: max_stars = star_count\n",
    "        if longest_by_star_path is None or len(x) > len(longest_by_star_path): longest_by_star_path = x\n",
    "        filled[x] = []\n",
    "\n",
    "    # fill in the json values into the filled dict\n",
    "    if min_stars == max_stars: # shortcut if we only do the same number of stars\n",
    "        _length_to_fill_ = None\n",
    "        for v in filled.keys():\n",
    "            if isJsonPath(v) and '[*]' in v:\n",
    "                filled[v] = [match.value if match.value != {} else None for match in parse(v).find_or_create(myjson)]\n",
    "                _this_length_ = len(filled[v])\n",
    "                if   _length_to_fill_ is None:          _length_to_fill_ = _this_length_\n",
    "                elif _length_to_fill_ != _this_length_: raise Exception(f'RTOntology.__applyTemplate__() - unequal number of values for {v} ({_length_to_fill_=} vs {_this_length_=})')\n",
    "        for v in filled.keys():\n",
    "            if   isJsonPath(v) and '[*]' not in v:\n",
    "                _match_ = parse(v).find(myjson)[0].value\n",
    "                filled[v] = [_match_] * _length_to_fill_\n",
    "            elif isJsonPath(v) == False:\n",
    "                filled[v] = [v] * _length_to_fill_\n",
    "    else:\n",
    "        star_count = longest_by_star_path.count('[*]')\n",
    "        if star_count   == 1:\n",
    "            for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                for v in filled.keys():\n",
    "                    if isJsonPath(v):\n",
    "                        _matches_ = parse(fillStars(v, i)).find(myjson)\n",
    "                        if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                        else:                   filled[v].append(None)\n",
    "                    else:\n",
    "                        filled[v].append(v)\n",
    "        elif star_count == 2:\n",
    "            for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                for j in range(len(parse(upToStar(fillStars(longest_by_star_path,i), 1)).find(myjson))):\n",
    "                    for v in filled.keys():\n",
    "                        if isJsonPath(v):\n",
    "                            _matches_ = parse(fillStars(v, i, j)).find(myjson)\n",
    "                            if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                            else:                   filled[v].append(None)\n",
    "                        else:\n",
    "                            filled[v].append(v)\n",
    "        elif star_count == 3:\n",
    "            for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                for j in range(len(parse(upToStar(fillStars(longest_by_star_path,i), 1)).find(myjson))):\n",
    "                    for k in range(len(parse(upToStar(fillStars(longest_by_star_path,i,j), 1)).find(myjson))):\n",
    "                        for v in filled.keys():\n",
    "                            if isJsonPath(v):\n",
    "                                _matches_ = parse(fillStars(v, i, j, k)).find(myjson)\n",
    "                                if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                                else:                   filled[v].append(None)\n",
    "                            else:\n",
    "                                filled[v].append(v)\n",
    "        else:\n",
    "            raise Exception(f'RTOntology.__applyTemplate__() - max of three stars supported -- {star_count} found')\n",
    "    return filled\n",
    "\n",
    "#\n",
    "# isLiteral() - true if it's a proper literal for json key string\n",
    "#\n",
    "def isLiteral(v):\n",
    "    if v == '': return False\n",
    "    for i in range(len(v)):\n",
    "        if i == 0 and v[i] >= '0' and v[i] <= '9': return False # can't start with a number\n",
    "        if (v[i] >= 'a' and v[i] <= 'z') or (v[i] >= 'A' and v[i] <= 'Z') or (v[i] >= '0' and v[i] <= '9') or (v[i] == '_'): pass\n",
    "        else: return False\n",
    "    return True\n",
    "\n",
    "#\n",
    "# endsWithAny() - does a string end with any of these?\n",
    "#\n",
    "def endsWithAny(_str_, _set_):\n",
    "    return any(_str_.endswith(x) for x in _set_)\n",
    "\n",
    "#\n",
    "# fillJSONPathElements() - uses self modifying code to optimize the filling of the structures based on jsonpath specifications.\n",
    "#\n",
    "def fillJSONPathElements(to_fill, myjson):\n",
    "    filled = {}\n",
    "    for x in to_fill: filled[x] = [] \n",
    "    filled_list = list(filled.keys())\n",
    "    longest_by_star_path  = filled_list[0]\n",
    "    for i in range(1, len(filled_list)): \n",
    "        if longest_by_star_path.count('[*]') < filled_list[i].count('[*]'): longest_by_star_path = filled_list[i]\n",
    "    to_eval, indent, _index_, _loop_vars_, _loop_i_, _path_, _star_path_, vars_set = [], 0, 1, ['i','j','k','l'], 0, '', '$', 0\n",
    "    while _index_ < len(longest_by_star_path):\n",
    "        _rest_ = longest_by_star_path[_index_:]\n",
    "        if   _rest_.startswith('[*]'):\n",
    "            to_eval.append(' '*indent+'for '+_loop_vars_[_loop_i_]+f' in range(len(myjson{_path_})):')\n",
    "            _path_      += f'[{_loop_vars_[_loop_i_]}]'\n",
    "            _star_path_ += f'[*]'\n",
    "            _index_, _loop_i_, indent = _index_+3, _loop_i_+1, indent+4\n",
    "            if _rest_.endswith('[*]'):\n",
    "                for i in range(len(filled_list)):\n",
    "                    if filled_list[i] == _star_path_:\n",
    "                        to_eval.append(' '*indent+f'_var{i}_ = myjson{_path_}')\n",
    "                        vars_set += 1\n",
    "            for i in range(len(filled_list)):\n",
    "                _filled_rest_ = filled_list[i][_index_:]\n",
    "                if _filled_rest_.count('[*]') == 0 and '.' not in _filled_rest_ and len(_filled_rest_) > 0:\n",
    "                    to_eval.append(' '*indent+f'_var{i}_ = myjson{_path_}' + _filled_rest_)\n",
    "                    vars_set += 1\n",
    "\n",
    "        elif _rest_[0] == '.':\n",
    "            _star_path_ += '.'\n",
    "            for i in range(len(filled_list)):\n",
    "                lit_maybe = filled_list[i][len(_star_path_):]\n",
    "                if isLiteral(lit_maybe):\n",
    "                    to_eval.append(' '*indent+f'if \"{lit_maybe}\" in myjson{_path_}:')\n",
    "                    to_eval.append(' '*(indent+4)+f'_var{i}_ = myjson{_path_}[\"{lit_maybe}\"]')\n",
    "                    to_eval.append(' '*indent+f'else: _var{i}_ = None')\n",
    "                    vars_set += 1\n",
    "            _index_ += 1\n",
    "        elif _rest_[0].isalpha() or _rest_[0] == '_':\n",
    "            l = len(_rest_)\n",
    "            if '.' in _rest_:                           l = _rest_.index('.')\n",
    "            if '[' in _rest_ and _rest_.index('[') < l: l = _rest_.index('[')\n",
    "            lit = _rest_[:l]\n",
    "            to_eval.append(' '*indent+f'if \"{lit}\" in myjson{_path_}:')\n",
    "            _path_      += f'[\"{lit}\"]'\n",
    "            _star_path_ += f'{lit}'\n",
    "            _index_, indent = _index_+l, indent+4\n",
    "        else:\n",
    "            print('Exception for the following script:\\n')\n",
    "            print('\\n'.join(to_eval)) \n",
    "            raise Exception(f'RTOntology.fillJSONPathElements() - parse error at {i}')\n",
    "\n",
    "        if vars_set >= len(filled_list):\n",
    "            for i in range(len(filled_list)):\n",
    "                to_eval.append(' '*indent+f'filled[\"{filled_list[i]}\"].append(_var{i}_)')\n",
    "            break\n",
    "\n",
    "    if to_eval[-1].endswith(':'): to_eval.append(' '*indent+'pass')\n",
    "    # print('\\n'.join(to_eval))\n",
    "    exec('\\n'.join(to_eval))\n",
    "    return filled\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class RTOntology(object):\n",
    "    # __init__() - prepare transform spec for use and initial instance variables\n",
    "    def __init__(self, xform_spec=None):\n",
    "        if xform_spec is not None: self.xform_spec_lines = self.__substituteDefines__(xform_spec)\n",
    "        else:                      self.xform_spec_lines = []\n",
    "        self.df_triples = None\n",
    "        self.uid_lu     = {}\n",
    "        self.rev_uid_lu = {}\n",
    "        self.time_lu    = {}\n",
    "        for x in ['fill.trace_json_paths', 'fill.collapse', 'fill.parse']: self.time_lu[x] = 0\n",
    "\n",
    "    # to_files() - write state to several files\n",
    "    def to_files(self, _base_name_):\n",
    "        self.df_triples.write_parquet(f'{_base_name_}.triples.parquet')\n",
    "        _lu_ = {'uid':[], 't0':[], 't1':[], 't2':[]}\n",
    "        for _uid_ in self.uid_lu:\n",
    "            _lu_['uid'].append(_uid_)\n",
    "            _lu_['t0'].append(self.uid_lu[_uid_][0])\n",
    "            _lu_['t1'].append(self.uid_lu[_uid_][1])\n",
    "            _lu_['t2'].append(self.uid_lu[_uid_][2])\n",
    "        pd.DataFrame(_lu_).to_parquet(f'{_base_name_}.uids.parquet')\n",
    "        if len(self.xform_spec_lines) > 0:\n",
    "            with open(f'{_base_name_}.xform_spec', 'wt') as f: f.write('\\n'.join(self.xform_spec_lines))\n",
    "\n",
    "    # fm_files() - read state from several files\n",
    "    def fm_files(self, _base_name_):\n",
    "        self.df_triples = pd.read_parquet(f'{_base_name_}.triples.parquet')\n",
    "        _lu_ = pd.read_parquet(f'{_base_name_}.uids.parquet')\n",
    "        uid_v, t0_v, t1_v, t2_v = _lu_['uid'].values, _lu_['t0'].values, _lu_['t1'].values, _lu_['t2'].values\n",
    "        for i in range(len(uid_v)):\n",
    "            self.uid_lu[uid_v[i]] = (t0_v[i], t1_v[i], t2_v[i])\n",
    "            if t2_v[i] == 'uniq':\n",
    "                _key_ = str(t0_v[i]) + '|' + str(t1_v[i])\n",
    "                self.rev_uid_lu[_key_] = uid_v[i]\n",
    "                \n",
    "    # __substituteDefines__() - subsitute defines\n",
    "    def __substituteDefines__(self, _txt_):\n",
    "        lines     = _txt_.split('\\n')\n",
    "        subs      = {}\n",
    "        completes = []\n",
    "        for _line_ in lines:\n",
    "            tokens = _line_.split()\n",
    "            if len(tokens) >= 3 and tokens[1] == '=':\n",
    "                subs[tokens[0]] = ' '.join(tokens[2:])\n",
    "            else:\n",
    "                for r in subs:\n",
    "                    if r in _line_:\n",
    "                        _line_ = _line_.replace(r, subs[r])\n",
    "                if len(_line_) > 0:\n",
    "                    completes.append(_line_)\n",
    "        return completes\n",
    "\n",
    "    # __applyTemplate__() - apply templated line in the transform to the json representation\n",
    "    def __applyTemplate__(self, \n",
    "                          myjson,        # json representation\n",
    "                          s_values,      s_children,    s_type,     s_disp, # subject params\n",
    "                          v_values,      v_children,                        # verb params   (it's only a string, no typing, unique to the schema)\n",
    "                          o_values,      o_children,    o_type,     o_disp, # object params\n",
    "                          g_values,      g_children,    g_type,     g_disp, # group params\n",
    "                          src_values,    src_children,                      # source params (it's only a string, no typing, unique to this ontological instance)\n",
    "                          ):\n",
    "        # resolve the jsonpath values        \n",
    "        all_values  = set(s_values.values()) | set(v_values.values()) | set(o_values.values())\n",
    "        if g_values   is not None: all_values |= set(g_values.values())\n",
    "        if src_values is not None: all_values |= set(src_values.values())\n",
    "\n",
    "        t0 = time.time()\n",
    "        path_values, starred_path_values, longest_by_star_path = [], [], None\n",
    "        for x in all_values:\n",
    "            if isJsonPath(x):\n",
    "                if '[*]' in x:\n",
    "                    starred_path_values.append(x)\n",
    "                    if   longest_by_star_path is None:                          longest_by_star_path = x\n",
    "                    elif longest_by_star_path.rindex('[*]') < x.rindex('[*]'):  longest_by_star_path = x\n",
    "                else:\n",
    "                    path_values.append(x)\n",
    "\n",
    "        # ensure that all jsonpath values are substrings of the longest star path\n",
    "        for x in starred_path_values:\n",
    "            x_until_last_star = x[:x.rindex('[*]')+3] # get the close bracket too\n",
    "            if longest_by_star_path[:len(x_until_last_star)] != x_until_last_star:\n",
    "                raise Exception(f'OntologyForViz.__applyTemplate__() - jsonpath are not subsets \"{x}\" vs \"{longest_by_star_path}\"')\n",
    "\n",
    "        # fill in the json values into the filled dict\n",
    "        if len(starred_path_values) > 0: filled = fillJSONPathElements(starred_path_values, myjson)\n",
    "        else:                            filled = {}\n",
    "\n",
    "        # ... double check that they are the same length\n",
    "        fill_len = None\n",
    "        for x in filled.keys():\n",
    "            if fill_len is None: fill_len = len(filled[x])\n",
    "            if len(filled[x]) != fill_len: raise Exception(f'OntologyForViz.__applyTemplate__() - unequal number of values for {x}')\n",
    "        if fill_len is None: fill_len = 1 # if there are no starred paths, then we need at least one filler (it's a constant path)\n",
    "\n",
    "        # Fix up the filled with either constants or with static json paths\n",
    "        for v in all_values:\n",
    "            if isJsonPath(v) and '[*]' in v: continue\n",
    "            if    isJsonPath(v): to_fill = [parse(v).find(myjson)[0].value]\n",
    "            else:                to_fill = [v]\n",
    "            filled[v] = to_fill * fill_len\n",
    "        t1 = time.time()\n",
    "        self.time_lu['fill.trace_json_paths'] += (t1-t0)\n",
    "\n",
    "        # collapse the parse trees based on the filled values\n",
    "        # ... double check that they are the same length\n",
    "        t0 = time.time()\n",
    "        l = None\n",
    "        for v in filled.keys():\n",
    "            if l is None: l = len(filled[v])\n",
    "            if len(filled[v]) != l: raise Exception(f'RTOntology.__applyTemplate__() - unequal number of values for {v}')\n",
    "        pre_df = {}\n",
    "        pre_df['sbj']    = [solveParseTree(s_values,   s_children,   filled, i) for i in range(l)]\n",
    "        pre_df['vrb']    = [solveParseTree(v_values,   v_children,   filled, i) for i in range(l)]\n",
    "        pre_df['obj']    = [solveParseTree(o_values,   o_children,   filled, i) for i in range(l)]\n",
    "        if g_values   is not None: pre_df['grp'] = [solveParseTree(g_values,   g_children,   filled, i) for i in range(l)]\n",
    "        if src_values is not None: pre_df['src'] = [solveParseTree(src_values, src_children, filled, i) for i in range(l)]\n",
    "        t1 = time.time()\n",
    "        self.time_lu['fill.collapse'] += (t1-t0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        for_df = {'sbj': [], 'stype': [], 'sdisp': [], 'vrb': [], 'obj': [], 'otype': [], 'odisp': [], 'grp':[], 'gdisp':[], 'src':[]}\n",
    "        for i in range(l):\n",
    "            #\n",
    "            # Subject (Required)\n",
    "            #\n",
    "            _sbj_, _sbj_type_, _sbj_disp_ = pre_df['sbj'][i], s_type, s_disp\n",
    "            if type(_sbj_) == tuple:\n",
    "                _sbj_type_ = _sbj_[1] if len(_sbj_) > 1 else s_type\n",
    "                _sbj_disp_ = _sbj_[2] if len(_sbj_) > 2 else s_disp\n",
    "                _sbj_      = _sbj_[0]\n",
    "            _sbj_uid_ = self.resolveUniqIdAndUpdateLookups(_sbj_, _sbj_type_, _sbj_disp_, 'sbj')\n",
    "            for_df['sbj'].append(_sbj_uid_), for_df['stype'].append(_sbj_type_), for_df['sdisp'].append(_sbj_disp_)\n",
    "\n",
    "            #\n",
    "            # Verb (Required)\n",
    "            #\n",
    "            _vrb_ = pre_df['vrb'][i]\n",
    "            for_df['vrb'].append(_vrb_)\n",
    "\n",
    "            #\n",
    "            # Object (Required)\n",
    "            #\n",
    "            _obj_, _obj_type_, _obj_disp_ = pre_df['obj'][i], o_type, o_disp\n",
    "            if type(_obj_) == tuple:\n",
    "                _obj_type_ = _obj_[1] if len(_obj_) > 1 else o_type\n",
    "                _obj_disp_ = _obj_[2] if len(_obj_) > 2 else o_disp\n",
    "                _obj_      = _obj_[0]\n",
    "            _obj_uid_ = self.resolveUniqIdAndUpdateLookups(_obj_, _obj_type_, _obj_disp_, 'obj')            \n",
    "            for_df['obj'].append(_obj_uid_), for_df['otype'].append(_obj_type_), for_df['odisp'].append(_obj_disp_)\n",
    "\n",
    "            #\n",
    "            # Grouping (Optional)\n",
    "            #\n",
    "            if g_values is not None:\n",
    "                _grp_, _grp_type_, _grp_disp_ = pre_df['grp'][i], g_type, g_disp\n",
    "                if type(_grp_) == tuple:\n",
    "                    _grp_type_ = _grp_[1] if len(_grp_) > 1 else g_type\n",
    "                    _grp_disp_ = _grp_[2] if len(_grp_) > 2 else g_disp\n",
    "                    _grp_      = _grp_[0]\n",
    "                _grp_uid_ = self.resolveUniqIdAndUpdateLookups(_grp_, _grp_type_, _grp_disp_, 'grp')\n",
    "                for_df['grp'].append(_grp_uid_)\n",
    "                for_df['gdisp'].append(_grp_disp_)\n",
    "            else:\n",
    "                for_df['grp'].append(None)\n",
    "                for_df['gdisp'].append(None)\n",
    "\n",
    "            #\n",
    "            # Sourcing (Optional)\n",
    "            #\n",
    "            if src_values is not None:\n",
    "                _src_ = pre_df['src'][i]\n",
    "                for_df['src'].append(str(_src_))\n",
    "            else:\n",
    "                for_df['src'].append(None)\n",
    "\n",
    "        t1 = time.time()\n",
    "        self.time_lu['fill.parse'] += (t1-t0)\n",
    "\n",
    "        _df_            = pl.DataFrame(for_df)\n",
    "        if len(_df_) > 0: self.df_triples = _df_ if self.df_triples is None else pl.concat([_df_, self.df_triples])\n",
    "\n",
    "    # resolveIdAndUpdateLookups() - resolve id and update lookups\n",
    "    # self.uid_lu[<interger>] = (id-from-input, type-from-input, disposition-from-input)\n",
    "    #\n",
    "    def resolveUniqIdAndUpdateLookups(self, _id_, _type_, _disp_, _occurs_in_):\n",
    "        _uniq_key_ = str(_id_)+'|'+str(_type_)\n",
    "        if _disp_ == 'uniq' and _uniq_key_ in self.rev_uid_lu: return self.rev_uid_lu[_uniq_key_]\n",
    "        my_uid = 100_000 + len(self.uid_lu.keys())\n",
    "        self.uid_lu[my_uid] = (_id_, _type_, _disp_)\n",
    "        if _disp_ == 'uniq':  self.rev_uid_lu[_uniq_key_] = my_uid\n",
    "        return my_uid\n",
    "\n",
    "    # parse() - parse json into ontology via specification\n",
    "    def parse(self, j):\n",
    "        for l in self.xform_spec_lines:\n",
    "            l, lu = literalize(l) # get rid of any literal values so it doesn't mess up the delimiters\n",
    "            if '#' in l: l = l[:l.index('#')].strip() # comments... hope the hash symbol doesn't occur anywhere in the template that isn't a comment\n",
    "            if len(l) == 0: continue\n",
    "\n",
    "            # Sourcing Information\n",
    "            src_values = src_children = None\n",
    "            if '^^^' in l:\n",
    "                src = l[l.index('^^^')+3:]\n",
    "                l   = l[:l.index('^^^')].strip()\n",
    "                src_values, src_children = parseTree(fillLiterals(src, lu))\n",
    "\n",
    "            # Grouping Information\n",
    "            g_values = g_children = g_type = g_disp = None\n",
    "            if '@@@' in l:\n",
    "                grp = l[l.index('@@@')+3:]\n",
    "                l   = l[:l.index('@@@')].strip()\n",
    "                g_uniq = None\n",
    "                if endsWithAny(grp, {'uniq', 'ambi', 'anon', 'yyyy', 'dura', 'cata', 'valu', 'cont'}) and '|' in grp:\n",
    "                    g_disp = grp[grp.rindex('|')+1:].strip()\n",
    "                    grp    = grp[:grp.rindex('|')]\n",
    "                else: g_disp = 'ambi'\n",
    "                g_type = None\n",
    "                if '|' in grp:\n",
    "                    g_type = grp[grp.rindex('|')+1:].strip()\n",
    "                    grp   = grp[:grp.rindex('|')]\n",
    "                g_node = grp\n",
    "                g_values, g_children = parseTree(fillLiterals(g_node, lu))\n",
    "                \n",
    "            svo = [x.strip() for x in l.split('---')]\n",
    "            if len(svo) == 3:\n",
    "                s, v, o = svo[0], svo[1], svo[2]\n",
    "\n",
    "                # Subject\n",
    "                s_uniq = None\n",
    "                if endsWithAny(s, {'uniq', 'ambi', 'anon', 'yyyy', 'dura', 'cata', 'valu', 'cont'}) and '|' in s:\n",
    "                    s_disp = s[s.rindex('|')+1:].strip()\n",
    "                    s      = s[:s.rindex('|')]\n",
    "                else: s_disp = 'ambi'\n",
    "                s_type = None\n",
    "                if '|' in s:\n",
    "                    s_type = s[s.rindex('|')+1:].strip()\n",
    "                    s      = s[:s.rindex('|')]\n",
    "                s_node = s\n",
    "                s_values, s_children = parseTree(fillLiterals(s_node, lu))\n",
    "\n",
    "                # Verb\n",
    "                v_values, v_children = parseTree(fillLiterals(v, lu))\n",
    "\n",
    "                # Object\n",
    "                o_uniq = None\n",
    "                if endsWithAny(o, {'uniq', 'ambi', 'anon', 'yyyy', 'dura', 'cata', 'valu', 'cont'}) and '|' in o:\n",
    "                    o_disp = o[o.rindex('|')+1:].strip()\n",
    "                    o      = o[:o.rindex('|')]\n",
    "                else: o_disp = 'ambi'\n",
    "                if '|' in o:\n",
    "                    o_type = o[o.rindex('|')+1:].strip()\n",
    "                    o      = o[:o.rindex('|')]\n",
    "                o_node = o\n",
    "                o_values, o_children = parseTree(fillLiterals(o_node, lu))\n",
    "                self.__applyTemplate__(j, s_values, s_children, s_type, s_disp, \n",
    "                                          v_values, v_children, \n",
    "                                          o_values, o_children, o_type, o_disp,\n",
    "                                          g_values, g_children, g_type, g_disp,\n",
    "                                          src_values, src_children)\n",
    "            else:\n",
    "                raise Exception(f'RTOntology.parse() - line \"{l}\" does not have three parts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_json_txt_ = '''\n",
    "{\"id\":1,\n",
    " \"people\":[{\"first\":\"John\", \"last\":\"Smith\", \"id\":10, \"citescore\":2.3, \"age\":30, \"city\":\"nyc\",          \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Joe\",  \"last\":\"Smith\", \"id\":20, \"citescore\":1.8, \"age\":35,                        \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Mary\", \"last\":\"Jones\", \"id\":30, \"age\":32, \"city\":\"philadelphia\", \"state\":\"pa\", \"country\":\"us\"}],\n",
    " \"knowsFrom\":[[10, 20, \"Conference A\"], \n",
    "              [20, 30, \"Conference B\"]],\n",
    " \"education\":[{\"id\":10, \"degreeReceived\":\"Ph.D. in Computer Science\",   \"university\":\"Stanford University\"},\n",
    "              {\"id\":10, \"degreeReceived\":\"Masters in Computer Science\", \"university\":\"University of Pennsylvania\"}],\n",
    " \"total_people\":3\n",
    "}'''\n",
    "_json_simple_  = json.loads(_json_txt_)\n",
    "def concatNames(_last_,_first_):\n",
    "    return _last_ + ' ' + _first_\n",
    "def combineAddress(_city_,_state_,_country_):\n",
    "    s = ''\n",
    "    if _city_    is not None: s += _city_\n",
    "    if _state_   is not None: s += ', ' + _state_    if (len(s) > 0) else _state_\n",
    "    if _country_ is not None: s += ', ' + _country_  if (len(s) > 0) else _country_\n",
    "    return s if (len(s) > 0) else 'Not Supplied'\n",
    "_xform_simple_ = '''\n",
    "_id_ = '$.people[*].id' | PersonID | uniq\n",
    "'$.id'                                --- \"hasEntryCount\"    --- '$.total_people' | xsd:integer                                                                           ^^^ \"IN_TEMPLATE\"\n",
    "_id_                                  --- \"hasName\"          --- concatNames('$.people[*].last', '$.people[*].first') | xsd:string                                        ^^^ \"IN_TEMPLATE\"\n",
    "_id_                                  --- \"hasCitationScore\" --- '$.people[*].citescore' | xsd:float   | valu                                                             ^^^ '$.id'    \n",
    "_id_                                  --- \"hasAge\"           --- '$.people[*].age'       | xsd:integer | valu                                                             ^^^ '$.id'\n",
    "_id_                                  --- \"isFrom\"           --- combineAddress('$.people[*].city', '$.people[*].state', '$.people[*].country') | CityStateCountry | uniq ^^^ '$.id'\n",
    "_id_                                  --- \"isFromCity\"       --- '$.people[*].city'      | City                                                                           ^^^ '$.id'\n",
    "'$.knowsFrom[*][0]' | PersonID | uniq --- \"knows\"            --- '$.knowsFrom[*][1]'     | PersonID    | uniq                 @@@ '$.knowsFrom[*][2]' | xsd:string | uniq ^^^ '$.id'\n",
    "'''\n",
    "ofv_simple = RTOntology(_xform_simple_)\n",
    "ofv_simple.parse(_json_simple_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{ofv_simple.uid_lu=}')\n",
    "ofv_simple.df_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "#print(len(parse('$[*]').find(_json_)))\n",
    "\n",
    "# Examples\n",
    "#jsp_expr = parse('$[*].name')\n",
    "#jsp_expr = parse('$[*].cast.[*].name') # but note that it doesn't distinguish the movie id\n",
    "#jsp_expr = parse('$..director.name')\n",
    "#jsp_expr = parse('$..name')\n",
    "#jsp_expr = parse('$..genre[*]')\n",
    "#[match.value for match in jsp_expr.find(_json_)][:3]\n",
    "\n",
    "# IMDB 600K Transform Map\n",
    "# ... maybe add \"@@@\" for grouping the triples together ... and then \"^^^\" for sourcing?\n",
    "_xform_map_ = '''\n",
    "__id__              = '$[*]._id'              | MovieID      | uniq\n",
    "__director__        = '$[*].director.name_id' | DirectorID   | uniq\n",
    "__castmember__      = '$[*].cast.[*].name_id' | CastMemberID | uniq\n",
    "__id__              --- \"hasTitle\"       --- '$[*].name'          | xsd:string          ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"yearReleased\"   --- '$[*].year'          | xsd:date     | yyyy ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"runTime\"        --- '$[*].runtime'       | xsd:duration | dura ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"hasGenre\"       --- '$[*].genre[*]'      | xsd.string   | cata ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"ratingValue\"    --- '$[*].ratingValue'   | xsd:float    | valu ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"summary\"        --- '$[*].summary_text'  | xsd:string   | cont ^^^ \"imdb_600k_international_movies\"\n",
    "__director__        --- \"directedMovie\"  --- __id__                                     ^^^ \"imdb_600k_international_movies\"\n",
    "__director__        --- \"hasName\"        --- '$[*].director.name' | xsd:string          ^^^ \"imdb_600k_international_movies\"\n",
    "__castmember__      --- \"castMemberOf\"   --- __id__                                     ^^^ \"imdb_600k_international_movies\"\n",
    "__castmember__      --- \"hasName\"        --- '$[*].cast.[*].name' | xsd:string          ^^^ \"imdb_600k_international_movies\"\n",
    "'''\n",
    "\n",
    "#\n",
    "# Updated Transform Map\n",
    "# ... the Person ID edge is now simplified to use the end jsonpath syntax\n",
    "# ... which isn't supported by the self modifying code example...\n",
    "#\n",
    "_xform_map_CONTAINS_REVERSE_ISSUE = '''\n",
    "__id__              = '$[*]._id'              | MovieID  | uniq\n",
    "__director__        = '$[*].director.name_id' | PersonID | uniq\n",
    "__castmember__      = '$[*].cast.[*].name_id' | PersonID | uniq\n",
    "__id__                       --- \"hasTitle\"       --- '$[*].name'          | xsd:string\n",
    "__id__                       --- \"yearReleased\"   --- '$[*].year'          | xsd:date\n",
    "__id__                       --- \"runTime\"        --- '$[*].runtime'       | xsd:duration\n",
    "__id__                       --- \"hasGenre\"       --- '$[*].genre[*]'      | xsd.string\n",
    "__id__                       --- \"ratingValue\"    --- '$[*].ratingValue'   | xsd:float\n",
    "__id__                       --- \"summary\"        --- '$[*].summary_text'  | xsd:string\n",
    "__director__                 --- \"directedMovie\"  --- __id__\n",
    "__castmember__               --- \"castMemberOf\"   --- __id__\n",
    "$..name_id | PersonID | uniq --- \"hasName\"        --- '$..name'            | xsd:string\n",
    "'''\n",
    "\n",
    "ofv = RTOntology(_xform_map_)\n",
    "_base_ = '../../../data/kaggle_imdb_600k/international-movies-json/'\n",
    "_files_ = os.listdir(_base_)\n",
    "print(f'{len(_files_)} files...')\n",
    "jsonparse_time_sum = ontology_time_sum = files_processed = 0\n",
    "for i in range(len(_files_)):\n",
    "    _file_ = _files_[i]\n",
    "    if (i > 0) and ((i % 50) == 0): print(f'{i:4} | json {jsonparse_time_sum/files_processed:0.3f}s | ontology {ontology_time_sum/files_processed:0.3f}s ...')\n",
    "    _txt_  = open(_base_ + _file_).read()\n",
    "    ts0 = time.time()\n",
    "    _json_ = json.loads(_txt_)\n",
    "    ts1 = time.time()        \n",
    "    ofv.parse(_json_)\n",
    "    ts2 = time.time()\n",
    "    jsonparse_time_sum += (ts1 - ts0)\n",
    "    ontology_time_sum  += (ts2 - ts1)\n",
    "    files_processed    += 1\n",
    "    if files_processed > 100: break\n",
    "\n",
    "print()\n",
    "print(f'{files_processed} files processed')\n",
    "print(f'json parse (per file):     {jsonparse_time_sum/files_processed:0.3f}s | total: {jsonparse_time_sum:0.3f}s')\n",
    "print(f'ontology parse (per file): {ontology_time_sum/files_processed:0.3f}s | total: {ontology_time_sum:0.3f}s')\n",
    "\n",
    "# just the first 10 files...\n",
    "# ... for all 10 template rows, it's 14.5s per file...  triples extracted is 36,547\n",
    "# ... cut down to 2 files... it's 4.8s per file after implementing the \"equal stars\" stub\n",
    "# ... ... however, to get the nulls to show up, you have to use find_or_create() from jsonpath-ng ...\n",
    "# ... ... and that creation sticks in an empty dictionary \"{}\" instead of a None...\n",
    "# ... ... with the self modifying code modification... now down to 0.015s per file\n",
    "# ... ... but this increases as more and more files are parsed... at 500 files, the average is as 0.24s / file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(ofv.df_triples)=}')\n",
    "print(f'{len(ofv.uid_lu)=}')\n",
    "print(f'{len(ofv.rev_uid_lu)=}')\n",
    "ofv.df_triples.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_uid_ = 284569\n",
    "#print(f'{ofv.uid_lu[_uid_]=}')\n",
    "#_tuple_ =ofv.uid_lu[_uid_]\n",
    "#_key_   = str(_tuple_[0])+'|'+str(_tuple_[1])\n",
    "#print(f'{ofv.rev_uid_lu[_key_]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofv.to_files('imdb_600k_movies')\n",
    "\n",
    "ofv2 = RTOntology()\n",
    "ofv2.fm_files('imdb_600k_movies')\n",
    "\n",
    "print(f'{len(ofv2.df_triples) == len(ofv.df_triples)} | {len(ofv2.df_triples)=}')\n",
    "print(f'{len(ofv2.uid_lu)     == len(ofv.uid_lu)} | {len(ofv2.uid_lu)=}')\n",
    "print(f'{len(ofv2.rev_uid_lu) == len(ofv.rev_uid_lu)} | {len(ofv2.rev_uid_lu)=}')\n",
    "\n",
    "for x in ofv2.uid_lu:\n",
    "    if x not in ofv.uid_lu:             print(f'{x} not in ofv.uid_lu')\n",
    "    if ofv2.uid_lu[x] != ofv.uid_lu[x]: print(f'{x} not the same in ofv.uid_lu [1]')\n",
    "for x in ofv.uid_lu:\n",
    "    if x not in ofv2.uid_lu:            print(f'{x} not in ofv2.uid_lu')\n",
    "    if ofv2.uid_lu[x] != ofv.uid_lu[x]: print(f'{x} not the same in ofv.uid_lu [2]')\n",
    "for x in ofv2.rev_uid_lu:\n",
    "    if x not in ofv.rev_uid_lu:         print(f'{x} not in ofv.rev_uid_lu')\n",
    "    if ofv2.rev_uid_lu[x] != ofv.rev_uid_lu[x]: print(f'{x} not the same in ofv.rev_uid_lu [1]')\n",
    "for x in ofv.rev_uid_lu:\n",
    "    if x not in ofv2.rev_uid_lu:        print(f'{x} not in ofv2.rev_uid_lu')\n",
    "    if ofv2.rev_uid_lu[x] != ofv.rev_uid_lu[x]: print(f'{x} not the same in ofv.rev_uid_lu [2]')\n",
    "\n",
    "ofv2.df_triples.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_star_set = {'$[*]._id', '$[*].cast.[*].name'}\n",
    "my_star_set = ['$[*]._id', '$[*].name']\n",
    "my_star_set = ['$[*]._id', '$[*].genre[*]']\n",
    "\n",
    "golden = fillJSONPathElementsByJSONPath(my_star_set, _json_)\n",
    "filled = fillJSONPathElements          (my_star_set, _json_)\n",
    "for v in filled.keys(): print(f'  {v:38}: {len(filled[v]):8} {len(golden[v]):8}')\n",
    "for v in golden.keys():\n",
    "    for i in range(len(golden[v])):\n",
    "        if filled[v][i] != golden[v][i]: print(f'WRONG:  {v}: {filled[v][i]} != {golden[v][i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_examples_ = [['$[*]._id',                       '$[*].name'],\n",
    "              ['$[*]._id',                       '$[*].year'],\n",
    "              ['$[*]._id',                       '$[*].runtime'],\n",
    "              ['$[*]._id',                       '$[*].genre[*]'],\n",
    "              ['$[*]._id',                       '$[*].ratingValue'],\n",
    "              ['$[*]._id',                       '$[*].summary_text'],\n",
    "              ['$[*].director.name_id',          '$[*]._id'],\n",
    "              ['$[*].cast.[*].name_id',          '$[*]._id'],\n",
    "              ['$[*].director.name_id',          '$[*].director.name'],\n",
    "              ['$[*].cast.[*].name_id',          '$[*].cast.[*].name']]\n",
    "for _example_ in _examples_:\n",
    "    print(_example_)\n",
    "    ts0 = time.time()\n",
    "    golden = fillJSONPathElementsByJSONPath(_example_, _json_)\n",
    "    ts1 = time.time()\n",
    "    filled = fillJSONPathElements          (_example_, _json_)\n",
    "    ts2 = time.time()\n",
    "    for v in filled.keys(): print(f'  {v:38}: {len(filled[v]):8} | {(ts2-ts1):0.2f}s \\t\\t {len(golden[v]):8} | {(ts1-ts0):0.2f}s')\n",
    "    for v in golden.keys():\n",
    "        for i in range(len(golden[v])):\n",
    "            results_differ = False\n",
    "            if   i >= len(filled[v]):          results_differ, reason = True, 'lengths differ'\n",
    "            elif filled[v][i] != golden[v][i]: results_differ, reason = True, 'values differ @ '+str(i) + ' : ' + str(filled[v][i]) + ' != ' + str(golden[v][i])\n",
    "            if results_differ: break\n",
    "        if results_differ: print(f'  Incorrect: {reason}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_json_txt_ = '''\n",
    "{\"id\":1,\n",
    " \"people\":[{\"first\":\"John\", \"last\":\"Smith\", \"id\":10, \"age\":30, \"city\":\"nyc\",          \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Joe\",  \"last\":\"Smith\", \"id\":20, \"age\":35,                        \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Mary\", \"last\":\"Jones\", \"id\":30, \"age\":32, \"city\":\"philadelphia\", \"state\":\"pa\", \"country\":\"us\"}],\n",
    " \"knowsFrom\":[[10, 20, \"Conference A\"], \n",
    "              [20, 30, \"Conference B\"]],\n",
    " \"education\":[{\"id\":10, \"degreeReceived\":\"Ph.D. in Computer Science\",   \"university\":\"Stanford University\"},\n",
    "              {\"id\":10, \"degreeReceived\":\"Masters in Computer Science\", \"university\":\"University of Pennsylvania\"}],\n",
    " \"total_people\":3\n",
    "}'''\n",
    "_json_simple_  = json.loads(_json_txt_)\n",
    "_examples_     = [\n",
    "['$.people[*].id',    '$.people[*].last', '$.people[*].first'],\n",
    "['$.people[*].id',    '$.people[*].age'],\n",
    "['$.people[*].id',    '$.people[*].city', '$.people[*].state', '$.people[*].country'],\n",
    "['$.people[*].id',    '$.people[*].city'],\n",
    "['$.knowsFrom[*][0]', '$.knowsFrom[*][1]', '$.knowsFrom[*][2]'],\n",
    "]\n",
    "for _example_ in _examples_:\n",
    "    print(_example_)\n",
    "    filled = fillJSONPathElements(_example_, _json_simple_)\n",
    "    for v in filled: print(f'  {v:38}: {len(filled[v]):8}')\n",
    "    print(pd.DataFrame(filled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
