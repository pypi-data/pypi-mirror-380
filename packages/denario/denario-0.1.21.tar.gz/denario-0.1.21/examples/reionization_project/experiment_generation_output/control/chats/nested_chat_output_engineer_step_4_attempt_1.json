[
  {
    "name": "engineer_response_formatter",
    "content": "**Code Explanation:**\n\nThis code implements a Bayesian parameter estimation framework using MCMC to constrain the parameters of our cosmic reionization model. It:\n1. Sets up the MCMC sampler using emcee\n2. Runs multiple walkers to explore the parameter space\n3. Analyzes convergence using autocorrelation time\n4. Generates corner plots to visualize parameter posteriors\n5. Computes summary statistics for all parameters\n6. Plots the best-fit reionization history with credible intervals\n\n**Python Code:**\n\n```python\n# filename: codebase/mcmc_sampling.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport emcee\nimport corner\nimport os\nimport time\nfrom multiprocessing import Pool\nfrom scipy.stats import norm\nimport pandas as pd\nfrom codebase.cosmic_reionization_model import solve_QHII\nfrom codebase.bayesian_framework import (\n    extract_observational_data, log_posterior, calculate_tau_e\n)\n\n# Create data directory if it doesn't exist\ndatabase_path = \"data/\"\nif not os.path.exists(database_path):\n    os.makedirs(database_path)\n\n# Set matplotlib parameters to avoid LaTeX rendering\nplt.rcParams['text.usetex'] = False\n\n\ndef run_mcmc(obs_data, nwalkers=32, nsteps=5000, ndim=5, z_range=(20.0, 5.0), \n             QHII_init=1e-4, include_tau_e=True, progress=False, discard=1000):\n    \"\"\"\n    Run MCMC sampling to obtain posterior distributions for model parameters.\n    \n    Parameters:\n    -----------\n    obs_data : dict\n        Dictionary containing observational data\n    nwalkers : int, optional\n        Number of walkers\n    nsteps : int, optional\n        Number of steps per walker\n    ndim : int, optional\n        Number of dimensions (parameters)\n    z_range : tuple, optional\n        Redshift range for model calculation\n    QHII_init : float, optional\n        Initial ionization fraction\n    include_tau_e : bool, optional\n        Whether to include CMB optical depth constraint\n    progress : bool, optional\n        Whether to show progress bar\n    discard : int, optional\n        Number of steps to discard as burn-in\n        \n    Returns:\n    --------\n    tuple\n        (sampler, samples, flat_samples) containing the emcee sampler and samples\n    \"\"\"\n    # Define parameter names and initial values\n    param_names = ['f0', 'alpha', 'beta', 'C0', 'gamma']\n    \n    # Fixed parameters\n    xi0 = 2.5e25\n    M_h = 1.0e10\n    \n    # Define initial positions for walkers with some dispersion\n    # Initial values based on literature and physical considerations\n    initial_positions = np.array([\n        0.1,    # f0: escape fraction normalization\n        2.0,    # alpha: redshift dependence of escape fraction\n        -0.5,   # beta: halo mass dependence of escape fraction\n        3.0,    # C0: clumping factor normalization\n        1.0     # gamma: redshift dependence of clumping factor\n    ])\n    \n    # Add dispersion to initial positions\n    pos = initial_positions + 0.01 * np.random.randn(nwalkers, ndim)\n    \n    # Ensure initial positions are within prior ranges\n    pos[:, 0] = np.clip(pos[:, 0], 0.01, 0.99)  # f0\n    pos[:, 1] = np.clip(pos[:, 1], -1.9, 4.9)   # alpha\n    pos[:, 2] = np.clip(pos[:, 2], -1.9, 1.9)   # beta\n    pos[:, 3] = np.clip(pos[:, 3], 1.1, 19.9)   # C0\n    pos[:, 4] = np.clip(pos[:, 4], 0.1, 2.9)    # gamma\n    \n    # Define log probability function for emcee\n    def log_prob(p):\n        # Add fixed parameters\n        full_params = tuple(p) + (xi0, M_h)\n        return log_posterior(full_params, obs_data, z_range, QHII_init, include_tau_e)\n    \n    # Set up the sampler\n    print(\"Setting up MCMC sampler with\", nwalkers, \"walkers and\", ndim, \"dimensions\")\n    \n    # Use multiprocessing for parallel sampling\n    with Pool() as pool:\n        sampler = emcee.EnsembleSampler(\n            nwalkers, ndim, log_prob, pool=pool\n        )\n        \n        # Run the sampler\n        print(\"Running MCMC sampling for\", nsteps, \"steps...\")\n        start_time = time.time()\n        sampler.run_mcmc(pos, nsteps, progress=progress)\n        end_time = time.time()\n        print(\"MCMC sampling completed in\", round(end_time - start_time, 2), \"seconds\")\n    \n    # Get the samples\n    samples = sampler.get_chain()\n    \n    # Discard burn-in and flatten the chain\n    flat_samples = sampler.get_chain(discard=discard, thin=15, flat=True)\n    \n    # Print acceptance fraction\n    print(\"Mean acceptance fraction:\", np.mean(sampler.acceptance_fraction))\n    \n    return sampler, samples, flat_samples, param_names\n\n\n\ndef check_convergence(sampler, discard=1000):\n    \"\"\"\n    Check convergence of the MCMC chains using autocorrelation time.\n    \n    Parameters:\n    -----------\n    sampler : emcee.EnsembleSampler\n        MCMC sampler\n    discard : int, optional\n        Number of steps to discard as burn-in\n        \n    Returns:\n    --------\n    dict\n        Dictionary containing convergence diagnostics\n    \"\"\"\n    # Calculate autocorrelation time\n    try:\n        tau = sampler.get_autocorr_time()\n        print(\"Autocorrelation time:\")\n        for i, t in enumerate(tau):\n            print(\"Parameter\", i, \":\", t)\n        \n        # Check if we have enough samples\n        n_steps = sampler.iteration\n        thin = int(np.max(tau) / 2)\n        n_effective = (n_steps - discard) / thin / tau\n        \n        print(\"Number of effective samples:\")\n        for i, n in enumerate(n_effective):\n            print(\"Parameter\", i, \":\", n)\n        \n        # Gelman-Rubin-like diagnostic\n        # Split chains in half and compare means\n        nwalkers = sampler.nwalkers\n        ndim = sampler.ndim\n        half_nwalkers = nwalkers // 2\n        \n        samples = sampler.get_chain(discard=discard)\n        \n        # Calculate means for each half\n        means1 = np.mean(samples[:, :half_nwalkers, :], axis=(0, 1))\n        means2 = np.mean(samples[:, half_nwalkers:, :], axis=(0, 1))\n        \n        # Calculate variances for each half\n        vars1 = np.var(samples[:, :half_nwalkers, :], axis=(0, 1))\n        vars2 = np.var(samples[:, half_nwalkers:, :], axis=(0, 1))\n        \n        # Calculate Gelman-Rubin R statistic\n        W = (vars1 + vars2) / 2  # Within-chain variance\n        B = ((means1 - means2)**2) / 2  # Between-chain variance\n        var_hat = W + B  # Pooled variance\n        R_hat = np.sqrt(var_hat / W)  # Gelman-Rubin statistic\n        \n        print(\"Gelman-Rubin R statistic:\")\n        for i, r in enumerate(R_hat):\n            print(\"Parameter\", i, \":\", r)\n        \n        converged = np.all(R_hat < 1.1) and np.all(n_effective > 50)\n        \n        return {\n            'tau': tau,\n            'n_effective': n_effective,\n            'R_hat': R_hat,\n            'converged': converged,\n            'thin': thin\n        }\n    except Exception as e:\n        print(\"Error calculating convergence diagnostics:\", str(e))\n        return {\n            'converged': False,\n            'error': str(e)\n        }\n\n\ndef plot_chains(samples, param_names, discard=1000):\n    \"\"\"\n    Plot the MCMC chains to visualize convergence.\n    \n    Parameters:\n    -----------\n    samples : array-like\n        MCMC samples\n    param_names : list\n        List of parameter names\n    discard : int, optional\n        Number of steps to discard as burn-in\n        \n    Returns:\n    --------\n    str\n        Filename of the saved plot\n    \"\"\"\n    nsteps, nwalkers, ndim = samples.shape\n    \n    fig, axes = plt.subplots(ndim, figsize=(10, 2*ndim), dpi=300)\n    \n    for i in range(ndim):\n        ax = axes[i]\n        ax.plot(samples[:, :, i], alpha=0.3)\n        ax.set_ylabel(param_names[i])\n        ax.axvline(discard, color='red', linestyle='--')\n    \n    axes[-1].set_xlabel(\"Step Number\")\n    plt.tight_layout()\n    \n    # Save the figure\n    filename = os.path.join(database_path, \"mcmc_chains_\" + str(np.random.randint(10000)) + \".png\")\n    plt.savefig(filename, dpi=300)\n    plt.close()\n    \n    print(\"Saved MCMC chains plot to:\", filename)\n    return filename\n\n\ndef plot_corner(flat_samples, param_names, truths=None):\n    \"\"\"\n    Create a corner plot of the parameter posteriors.\n    \n    Parameters:\n    -----------\n    flat_samples : array-like\n        Flattened MCMC samples\n    param_names : list\n        List of parameter names\n    truths : array-like, optional\n        True parameter values for comparison\n        \n    Returns:\n    --------\n    str\n        Filename of the saved plot\n    \"\"\"\n    # Create corner plot\n    fig = corner.corner(\n        flat_samples, \n        labels=param_names,\n        quantiles=[0.16, 0.5, 0.84],\n        show_titles=True,\n        title_kwargs={\"fontsize\": 12},\n        truths=truths\n    )\n    \n    # Save the figure\n    filename = os.path.join(database_path, \"corner_plot_\" + str(np.random.randint(10000)) + \".png\")\n    plt.savefig(filename, dpi=300)\n    plt.close()\n    \n    print(\"Saved corner plot to:\", filename)\n    return filename\n\n\ndef compute_summary_statistics(flat_samples, param_names):\n    \"\"\"\n    Compute summary statistics for the parameter posteriors.\n    \n    Parameters:\n    -----------\n    flat_samples : array-like\n        Flattened MCMC samples\n    param_names : list\n        List of parameter names\n        \n    Returns:\n    --------\n    pandas.DataFrame\n        DataFrame containing summary statistics\n    \"\"\"\n    # Calculate percentiles\n    percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)\n    \n    # Create summary statistics\n    stats = {\n        'parameter': param_names,\n        'median': percentiles[1],\n        'lower_error': percentiles[1] - percentiles[0],\n        'upper_error': percentiles[2] - percentiles[1],\n        'mean': np.mean(flat_samples, axis=0),\n        'std': np.std(flat_samples, axis=0)\n    }\n    \n    # Create DataFrame\n    df = pd.DataFrame(stats)\n    \n    # Save to CSV\n    filename = os.path.join(database_path, \"parameter_statistics.csv\")\n    df.to_csv(filename, index=False)\n    print(\"Saved parameter statistics to:\", filename)\n    \n    # Print summary\n    print(\"\\nParameter Summary Statistics:\")\n    for i, param in enumerate(param_names):\n        print(param + \":\", percentiles[1, i], \"-\", percentiles[1, i] - percentiles[0, i], \"+\", percentiles[2, i] - percentiles[1, i])\n    \n    return df\n\n\ndef plot_best_fit_model(flat_samples, obs_data, param_names, z_range=(20.0, 5.0), QHII_init=1e-4, n_samples=100):\n    \"\"\"\n    Plot the best-fit model and credible intervals.\n    \n    Parameters:\n    -----------\n    flat_samples : array-like\n        Flattened MCMC samples\n    obs_data : dict\n        Dictionary containing observational data\n    param_names : list\n        List of parameter names\n    z_range : tuple, optional\n        Redshift range for model calculation\n    QHII_init : float, optional\n        Initial ionization fraction\n    n_samples : int, optional\n        Number of random samples to draw for credible intervals\n        \n    Returns:\n    --------\n    str\n        Filename of the saved plot\n    \"\"\"\n    # Fixed parameters\n    xi0 = 2.5e25\n    M_h = 1.0e10\n    \n    # Get median parameter values\n    median_params = np.median(flat_samples, axis=0)\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n    \n    # Calculate best-fit model\n    full_params = tuple(median_params) + (xi0, M_h)\n    z_model, Q_HII_model = solve_QHII(z_range, full_params, QHII_init)\n    \n    # Plot best-fit model\n    ax.plot(z_model, Q_HII_model, 'k-', linewidth=2, label='Best-fit Model')\n    \n    # Calculate and plot credible intervals\n    Q_HII_samples = np.zeros((len(z_model), n_samples))\n    \n    # Randomly sample from the posterior\n    indices = np.random.randint(0, len(flat_samples), n_samples)\n    \n    for i, idx in enumerate(indices):\n        sample_params = tuple(flat_samples[idx]) + (xi0, M_h)\n        try:\n            _, Q_HII_sample = solve_QHII(z_range, sample_params, QHII_init)\n            Q_HII_samples[:, i] = Q_HII_sample\n        except Exception as e:\n            print(\"Error calculating model for sample\", i, \":\", str(e))\n            Q_HII_samples[:, i] = np.nan\n    \n    # Calculate percentiles\n    Q_HII_lower = np.nanpercentile(Q_HII_samples, 16, axis=1)\n    Q_HII_upper = np.nanpercentile(Q_HII_samples, 84, axis=1)\n    \n    # Plot credible interval\n    ax.fill_between(z_model, Q_HII_lower, Q_HII_upper, color='gray', alpha=0.3, label='68% Credible Interval')\n    \n    # Plot observational data\n    # Get unique measurement methods\n    methods = np.unique(obs_data['method'])\n    \n    # Define colors and markers for different methods\n    colors = ['blue', 'red', 'green', 'purple', 'orange']\n    markers = ['o', 's', '^', 'D', 'v']\n    \n    # Plot data points with error bars for each method\n    for i, method in enumerate(methods):\n        # Find indices for this method\n        idx = np.where(obs_data['method'] == method)[0]\n        \n        # Plot data points with error bars\n        ax.errorbar(\n            obs_data['z'][idx],\n            obs_data['Q_HII'][idx],\n            yerr=obs_data['Q_HII_err'][idx],\n            fmt=markers[i] + '-',\n            color=colors[i],\n            label=method\n        )\n    \n    ax.set_xlabel('Redshift')\n    ax.set_ylabel('Q_HII')\n    ax.legend()\n    plt.tight_layout()\n    \n    # Save the figure\n    filename = os.path.join(database_path, \"best_fit_model_\" + str(np.random.randint(10000)) + \".png\")\n    plt.savefig(filename, dpi=300)\n    plt.close()\n    \n    print(\"Saved best-fit model plot to:\", filename)\n    return filename\n```",
    "timestamp": "2025-07-28 18:55:42",
    "cost": {
      "Agent": [
        "engineer_response_formatter"
      ],
      "Model": [
        "o3-mini-2025-01-31"
      ],
      "Cost": [
        0.023485
      ],
      "Prompt Tokens": [
        4054
      ],
      "Completion Tokens": [
        4324
      ],
      "Total Tokens": [
        8378
      ]
    }
  },
  {
    "name": "executor",
    "content": "execution results:\nExecution output: \nTraceback (most recent call last):\n  File \u001b[35m\"/home/tda/PABLO/AstroPilot/examples/reionization_project/experiment_generation_output/control/codebase/mcmc_sampling.py\"\u001b[0m, line \u001b[35m5\u001b[0m, in \u001b[35m<module>\u001b[0m\n    import corner\n\u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'corner'\u001b[0m\n",
    "timestamp": "2025-07-28 18:55:43",
    "cost": {
      "Agent": [],
      "Model": [],
      "Cost": [],
      "Prompt Tokens": [],
      "Completion Tokens": [],
      "Total Tokens": []
    }
  }
]