# -*- coding: utf-8 -*-
"""
Created on Mon Aug 18 17:15:01 2025

@author: Leonard.Doyle
"""

import os

import numpy as np
import scipy
from scipy.optimize import curve_fit
import pandas as pd

import matplotlib.pyplot as plt
from matplotlib import colors

def sort_filtersettings_by_brightness(imgs):
    """
    Find brightness sorting of given sequence/filters.
    The input is typically generated by
    CALAs `archivingread` package with
    `archivingread.hdr_focus.all_in_one_load_images().
    The dataframe contains images which have already
    been dark-image subtracted etc.

    Input: Pandas dataframe of shape
    Index | SequenceNumber | Filtersetting | image
    0     | 1              | F1111         | (ndarray, )
    1     | 1              | F1111         | (ndarray, )
    ...
    11    | 2              | F1000         | (ndarray, )
    12    | 2              | F1000         | (ndarray, )
    ...
    The Filtersetting column is optional.
    If specified, it has to match the CALA format
    `Fxxxx` where `x= 0 or 1`.


    Output: Pandas dataframe of brightness sorted sequences
    Index | SequenceNumber | Filtersetting | Brightness
    0     | 8              | F0000         | 6382.22
    1     | 7              | F0010         | 3086.87
    ...

    Which can later be used to sort the actual big dataframe
    or at least traverse the contents in sorted order.
    """

    df = pd.DataFrame()
    df['SequenceNumber'] = imgs.SequenceNumber.unique()
    df['Filtersetting'] = imgs.Flags.unique() # assert they change together
    df['Brightness'] = 0.0

    for idx in df.index:
        seq = df.SequenceNumber.loc[idx]
        sub_df = imgs[imgs.SequenceNumber==seq]
        mean_of_all_imgs = sub_df.image.apply(np.nansum).mean()
        df.loc[idx, 'Brightness'] = mean_of_all_imgs

    sorted_sequences = df.sort_values('Brightness', ascending=False)
    sorted_sequences.reset_index(drop=True, inplace=True)

    return sorted_sequences


def generate_histograms(imgs,
                        bins=10000,
                        ):
    """
    For each image, create a histogram and place it in the
    same dataframe (inplace).
    """
    imgs['histogram'] = None # init empty object column
    imgs['histogram_xvals'] = None # init empty object column

    for idx in imgs.index:
        item = imgs.loc[idx]
        img = item.image
        img_nonnan = img[~np.isnan(img)]
        hist, bin_edges = np.histogram(img_nonnan, bins=bins, range=[0,1])
        # clip range at 0, since we will threshold anyway
        imgs.at[idx, 'histogram'] = hist

        #NB: I know this is a lot of duplication, but still
        # seemed cleaner than returning it out-of-band        
        hist_xvals = 0.5*(bin_edges[:-1] + bin_edges[1:]) # bin midpoints
        imgs.at[idx, 'histogram_xvals'] = hist_xvals


def estimate_ratios_on_histograms(imgs,
                                  sorted_sequences,
                                  threshold_lower,
                                  threshold_upper):
    """
    We are going slightly overboard here by getting the ratio for all
    next-level partners for all initial levels, but that should give us some
    solid averages.
    Since we do a refinement step, no need to interpolate or get even closer
    to the "best ratio", we are already very close.
    """
    if 'Filtersetting' in sorted_sequences.columns:
        filtersettings = sorted_sequences.Filtersetting.to_list()
    else:
        #TODO check entire script works with this
        # using SequenceNumber instead of filter setting as fallback
        raise NotImplementedError('Working without Filtersetting/Flags'
                                  + ' column not suported (yet)')
    
    relative_ratios = {filtersettings[0]: 1.0}
    relative_ratios_std = {filtersettings[0]: 0.0}

    maximum_guess_ratio = threshold_upper/threshold_lower # if the ratio were higher, would not have any overlap anymore!
    maximum_guess_ratio *= 0.8 # only go to 80%, makes analysis below easier, and hopefully no one will ever go this close
    guess_ratios = np.linspace(1,maximum_guess_ratio,30) #assume ratio between 1 to max in N steps, refine later

    for i, lower_filtersetting in enumerate(filtersettings[:-1]):
        higher_filtersetting = filtersettings[i+1]

        imgs_low = imgs[imgs.Flags==lower_filtersetting]
        imgs_high = imgs[imgs.Flags==higher_filtersetting]

        ratios_at_this_setting = []
        for i_ in (range(len(imgs_low))):
            hist_low = imgs_low.iloc[i_].histogram
            for j_ in range(len(imgs_high)):
                hist_high = imgs_high.iloc[j_].histogram
                hist_xvals = imgs_high.iloc[j_].histogram_xvals

                px_count_img_low = np.zeros_like(guess_ratios)
                px_count_img_high = np.zeros_like(guess_ratios)
                for i, guess_ratio in enumerate(guess_ratios):
                    count_low, count_high = _count_two_histograms_in_overlapping_range(
                        hist_xvals,
                        hist_low,
                        hist_high,
                        threshold_lower,
                        threshold_upper,
                        guess_ratio
                    )

                    px_count_img_low[i] = count_low
                    px_count_img_high[i] = count_high

                diff = px_count_img_low-px_count_img_high
                last_above_0 = np.where(diff>0)[0][-1] # assert there is always one

                #now we know roughly the ratio, refine again around this
                guess_ratios_fine = np.linspace(guess_ratios[last_above_0],  guess_ratios[last_above_0+1], 50)
                
                px_count_img_low_fine = np.zeros_like(guess_ratios_fine)
                px_count_img_high_fine = np.zeros_like(guess_ratios_fine)
                for i, guess_ratio in enumerate(guess_ratios_fine):
                    count_low, count_high = _count_two_histograms_in_overlapping_range(
                        hist_xvals,
                        hist_low,
                        hist_high,
                        threshold_lower,
                        threshold_upper,
                        guess_ratio
                    )
                    px_count_img_low_fine[i] = count_low
                    px_count_img_high_fine[i] = count_high
                    
                diff = px_count_img_low_fine-px_count_img_high_fine
                last_above_0 = np.where(diff>0)[0][-1]
                best_guess_ratio = guess_ratios_fine[last_above_0]
                ratios_at_this_setting.append(best_guess_ratio)
        ratios_at_this_setting = np.asarray(ratios_at_this_setting)
        
        relative_ratios[higher_filtersetting] = ratios_at_this_setting.mean()
        relative_ratios_std[higher_filtersetting] = ratios_at_this_setting.std()
        
    return relative_ratios, relative_ratios_std

def _count_two_histograms_in_overlapping_range(hist_xvals,
                                               hist_low,
                                               hist_high,
                                               threshold_lower,
                                               threshold_upper,
                                               ratio_high_to_low):
    thresh_high_lower = threshold_lower # unchanged
    thresh_high_upper = threshold_upper/ratio_high_to_low # e.g. 0.95 in 10x overexposed image == 0.095 in high image
    thresh_low_lower = threshold_lower*ratio_high_to_low # e.g 0.05 in high image == 0.5 in 10x overexposed image
    thresh_low_upper = threshold_upper # unchanged
    count_high = _count_histogram_in_range(hist_xvals, hist_high, thresh_high_lower, thresh_high_upper)
    count_low = _count_histogram_in_range(hist_xvals, hist_low, thresh_low_lower, thresh_low_upper)
    return count_low, count_high

def _count_histogram_in_range(hist_xvals, hist, threshold_lower, threshold_upper):
    selection = (hist_xvals<=threshold_upper) & (hist_xvals>=threshold_lower)
    selected = hist[selection]
    return np.sum(selected)


def blank_image_to_range(img, threshold_lower, threshold_upper):
    img2 = img.copy()
    img2[img2 <= threshold_lower] = np.nan
    img2[img2 >= threshold_upper] = np.nan
    return img2

def get_shift_by_correlation(img_ref, img_test):

    """
    Cross-correlate two images and return a tuple (shift_x, shift_y)
    how much the test image has to be shifted to overlay
    nicely on the reference image.

    If images supplied contain NaNs, these are blanked to 0.0 first.
    """
    img_ref_zeroed = img_ref.copy()
    img_ref_zeroed[np.isnan(img_ref_zeroed)] = 0
    img_test_zeroed = img_test.copy()
    img_test_zeroed[np.isnan(img_test_zeroed)] = 0
    
    correlation = scipy.signal.correlate(img_ref_zeroed, img_test_zeroed, 'same')
    # find correlation peak:
    cy, cx = np.unravel_index(np.argmax(correlation), correlation.shape)
    H, W = correlation.shape
    shift_y = cy-H//2
    shift_x = cx-W//2
    return (shift_x, shift_y)


def determine_lowest_level_distances(imgs,
                                     sorted_sequences,
                                     threshold_lower,
                                     threshold_upper,
                                     use_tqdm=None):
    
    if use_tqdm is None:
        # making dummy is easier than having if/else all around
        def tqdm(*args):
            return args
    else:
        tqdm = use_tqdm

    if 'Filtersetting' in sorted_sequences.columns:
        filtersettings = sorted_sequences.Filtersetting.to_list()
    else:
        #TODO check entire script works with this
        # using SequenceNumber instead of filter setting as fallback
        raise NotImplementedError('Working without Filtersetting/Flags'
                                  + ' column not suported (yet)')
    
    lowest_filtersetting = filtersettings[0]
    imgs_low = imgs[imgs.Flags==lowest_filtersetting].image
    # use first image as temporary reference, determine best "ground truth" later:
    img_ref = blank_image_to_range(imgs_low.iloc[0], threshold_lower, threshold_upper)

    # first image not shifted by definition
    shifts_x = [0, ]
    shifts_y = [0, ]

    for idx in tqdm(range(1, len(imgs_low.index))):
        # no need for relative scaling of thresholds, always in same range in this loop
        img_ = blank_image_to_range(imgs_low.iloc[idx], threshold_lower, threshold_upper)
        
        shift_x, shift_y = get_shift_by_correlation(img_ref, img_)
        shifts_x.append(shift_x)
        shifts_y.append(shift_y)
    shifts_x = np.asarray(shifts_x)
    shifts_y = np.asarray(shifts_y)
    shift_x_mean = shifts_x.mean()
    shift_y_mean = shifts_y.mean()

    distance_to_origin = np.sqrt((shifts_x - shift_x_mean)**2 + (shifts_y - shift_y_mean)**2)
    imgs.loc[imgs_low.index, 'distance_to_center'] = distance_to_origin
    sub_df_low = imgs[imgs.Flags==lowest_filtersetting]
    sorted_distances = sub_df_low.sort_values('distance_to_center').distance_to_center
    return sorted_distances


def shift_with_fillvalue(array, shift_x, shift_y, fillvalue=np.nan):
    # np.roll is a convient method to shift an array in some direction
    # however, the vanishing values come out the other end
    # so we need to write this ourselves it seems
    array = array.copy()
    if shift_x < 0:
        array[:,:-shift_x+1] = fillvalue
    elif shift_x > 0:
        array[:,-shift_x:] = fillvalue
    else:
        pass # 0=nothing to do, breaks otherwise
    if shift_y < 0:
        array[:-shift_y+1, :] = fillvalue
    elif shift_y > 0:
        array[-shift_y:, :] = fillvalue
    else:
        pass
    mask = np.isnan(array)
    array[mask] = 0.0
    array = np.roll(array, (shift_y, shift_x), axis=(0,1))
    mask = np.roll(mask, (shift_y, shift_x), axis=(0,1))
    array[mask] = np.nan
    return array


def auto_shift_and_stack_all(imgs,
                             sorted_sequences,
                             sorted_lowest_level_distances,
                             relative_ratios,
                             threshold_lower,
                             threshold_upper,
                             use_tqdm=None):
    """
    We now have input data that we trust in terms of dynamic range, dark
    images, blanked areas. Also, we should have reasonable ratios between
    each filter setting.

    The last step before being able to assemble the HDR focus is to determine
    by how much each exposure is shifted w.r.t to the reference.
    We will start again at the lowest level (brightest image) and gradually
    work our way up.

    As demonstrated in the `iceberg` test script, before cross-correlating to
    find the shift, we must first make sure that both images show the same
    information.
    Since we now have an estimate for the filter ratio between both, we should
    be able to blank both images to the common dynamic range.
    They should therefore appear visually similar, even though they are at
    different absolute scales.

    Next, use cross correlation to determine the number of pixels by which to
    shift the higher filter setting to match the lower. Setting all
    overexposed or NaN values to 0 will give them zero weight in the
    correlation and thus do exactly what we want.

    In a previous version of the script, we investigated in detail whether
    shifting has a pairwise unique answer, i.e. finding the shift of
    F0000_A -> F0010_B and F0000_A -> F0000_C will mean the shift from B to C
    is simply the difference in shift of A-B and A-C.
    Unfortunately it is not, there were deviations by +1 or -1 pixel in both
    dimensions, i.e. +-2 pixel offset worst case.

    If this had worked, we could easily determine all shifts with respect
    to all images. But since we always have to decide which images we are
    comparing, it would be too expensive to do all combinations.
    Instead:
    * start at lowest level, determine mutual shifts and find image closest to
    "center" of all occurences
    * in next level, find closest (which has to be shifted the least), shift
    it on top
    * in the next level, repeat (now correlating with previously shifted layer,
    but this is more or less on top of ground level)
    * in each step, mark used images as used so we do not re-use them
    * repeat for the next image in lowest batch (will be less "centered") ->
    gradually use up all higher levels

    This means we start with the "best guess" center representative in lowest
    level, and find always the closest matching in next level.
    For the last image in lowest batch, we might be left with unfitting
    corresponding images in higher levels, though.
    Together, this should yield some guesses of best focus and some outliers
    of badly stitched lower estimates...

    ### Determine shifts for ground level

    We will not actually shift the ground levels (since they make the basis
    for N different HDR stacks, no need to shift them).
    But we do need to calculate the shifts to find the most centered one and
    the "distance" of the others to that center one.

    Then, sort the lowest image batch by distance to iteratively work our way
    from closest to furthest while stacking higher levels.
   
    __NB__ In this section, we collect matching images, shift them around and
    blank them to their individual dynamic range. However we do not scale them
    yet, since a more accurate relative ratio will be established later.
    """

    """ ***** internal docs *****
    Observation when debugging shifts (`debug_shifts=True` inside function):
    Since we always shift the next level onto the lowest level, we can
    effectively think of selecting from each level the one that matches
    closest to the lowest level.

    Therefore, if one image in one intermediate level gets swapped (simulated
    by setting `used=True` on some index to skip it for debugging), the next
    levels on top will likely choose again the same close-neighbour index.

    When allowing reusage of images, indeed we observe that some "favorite"
    images are used 3-4 times, while others are never used.
    This indicates it does make sense to force preventing re-use to have each
    one exactly once.
    Even if the method to select best matching may not be perfect (lead to a
    17px shift for the worst case outlier).
    """

    if use_tqdm is None:
        # making dummy is easier than having if/else all around
        def tqdm(*args):
            return args
    else:
        tqdm = use_tqdm

    if 'Filtersetting' in sorted_sequences.columns:
        filtersettings = sorted_sequences.Filtersetting.to_list()
    else:
        #TODO check entire script works with this
        # using SequenceNumber instead of filter setting as fallback
        raise NotImplementedError('Working without Filtersetting/Flags'
                                  + ' column not suported (yet)')
    
    hdr_stacks = {}
    hdr_stacks_indices = {} # remember which image index was used for which stack

    imgs['used'] = False # new column to remember what images have been already used in HDR
    #imgs.loc[46, 'used'] = True #TODO DEBUG only to generate "no more unused images" error

    lowest_filtersetting = filtersettings[0]

    for idx in sorted_lowest_level_distances.index:
        current_hdr_stack = {}
        hdr_stacks[idx] = current_hdr_stack
        current_image_indices = {lowest_filtersetting: idx} # save indices of this stack for later, start with current image
        hdr_stacks_indices[idx] = current_image_indices
        
        imgs.loc[idx, 'used'] = True
        img_low = imgs.loc[idx].image # unshifted, unblanked

        img_low_blanked = blank_image_to_range(img_low, -np.inf, threshold_upper)
        # only for lowest level: don't blank threshold_lower, keep noise at lowest level

        current_hdr_stack[lowest_filtersetting] = img_low_blanked
        
        for filtersetting_idx in tqdm(range(1,len(filtersettings))):
            filter_high = filtersettings[filtersetting_idx]
            sub_df_high = imgs[(imgs.Flags==filter_high) & (imgs.used==False)]
            if len(sub_df_high.index) == 0:
                raise RuntimeError(f'Filtersetting {filter_high}: no more unused images left, aborting')

            relative_ratio = relative_ratios[filter_high]
            
            thresh_high_lower = threshold_lower # unchanged
            thresh_high_upper = threshold_upper/relative_ratio # e.g. 0.95 in 10x overexposed image == 0.095 in high image
            thresh_low_lower = threshold_lower*relative_ratio # e.g 0.05 in high image == 0.5 in 10x overexposed image
            thresh_low_upper = threshold_upper # unchanged
            
            img_low_commonblanked = blank_image_to_range(img_low,
                                                         thresh_low_lower,
                                                         thresh_low_upper)
            
            # * find shifts for all images in this level
            shifts_x = []
            shifts_y = []
            for j_, jdx in enumerate(sub_df_high.index):
                img_high = sub_df_high.loc[jdx].image
                img_high_commonblanked = blank_image_to_range(img_high,
                                                         thresh_high_lower,
                                                         thresh_high_upper)
                #NB cannot use center-shifted lowest image, otherwise we would have to account
                # for the original shift of the lower image
                # to simplify things for now, keep lowest level unshifted and shift
                # next levels always to be on top of that. In the very end of the script
                # we can still think about shifting all to center, but maybe not even useful in the end
                shift_x, shift_y = get_shift_by_correlation(img_low_commonblanked, img_high_commonblanked)
                shifts_x.append(shift_x)
                shifts_y.append(shift_y)
            shifts_x = np.asarray(shifts_x)
            shifts_y = np.asarray(shifts_y)
            distance_to_low = np.sqrt((shifts_x)**2 + (shifts_y)**2)
            
            # * find the closest shift
            closest_j = np.argmin(distance_to_low)
            closest_jdx = sub_df_high.index[closest_j]
            
            # * mark this as used and add to stack
            reusage_allowed = False
            if not reusage_allowed:
                imgs.loc[closest_jdx, 'used'] = True # flag to skip in future
            
            img_high = sub_df_high.loc[closest_jdx].image # again unblanked

            shift_x, shift_y = shifts_x[closest_j], shifts_y[closest_j]
            img_high_shifted = shift_with_fillvalue(img_high, shift_x, shift_y)
            img_high_shifted_blanked = blank_image_to_range(img_high_shifted,
                                                            threshold_lower,
                                                            threshold_upper)
            
            current_hdr_stack[filter_high] = img_high_shifted_blanked
            debug_shifts = False
            if debug_shifts:
                current_image_indices[filter_high] = (closest_jdx, shift_x, shift_y)
            else:
                current_image_indices[filter_high] = closest_jdx
            
            # * remember this as current level and progress to next
            img_low = img_high_shifted_blanked

    return hdr_stacks, hdr_stacks_indices


def fit_gaussian_to_hist(ratios_in_img, bins=100, plot=True):
    """
    Generated by prompting ChatGPT:
        ```
        give me a Gaussian fit function onto the distribution in this array:
        ratio_hist, ratio_hist_edges = np.histogram(ratios_in_img, bins=100)
        ```
    """
    
    def gaussian(x, amplitude, mean, stddev):
        return amplitude * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))

    ratio_hist, ratio_hist_edges = np.histogram(ratios_in_img, bins=bins)
    bin_centers = (ratio_hist_edges[:-1] + ratio_hist_edges[1:]) / 2
    initial_guess = [np.max(ratio_hist), np.mean(ratios_in_img), np.std(ratios_in_img)]
    popt, pcov = curve_fit(gaussian, bin_centers, ratio_hist, p0=initial_guess)

    ratio_median = np.median(ratios_in_img)

    if plot:
        plt.figure(figsize=(8, 3))
        plt.hist(ratios_in_img, bins=bins, alpha=0.5, label='Histogram')
        x_fit = np.linspace(bin_centers[0], bin_centers[-1], 1000)
        plt.plot(x_fit, gaussian(x_fit, *popt), 'r-', label='Gaussian Fit')
        plt.vlines([ratio_median], 0, ratio_hist.max())
        plt.xlabel("Ratio")
        plt.ylabel("Frequency")
        plt.title("Gaussian Fit to Histogram")
        plt.legend()
        plt.grid(True)
        plt.show()

    return *popt, ratio_median  # returns [amplitude, mean, stddev, median]

def determine_ratios_from_stacks(hdr_stacks,
                                 sorted_sequences,
                                 threshold_lower,
                                 threshold_upper,
                                 show_some_histograms=False,
                                 ):
    """
    We confirmed above that the overlap of the images seems acceptable.
    Now, we can revisit the ratio between two images, since we can not only
    compare the number of occurences of equivalent pixels, but are allowed
    to use their location for input.

    Due to the jitter and breathing of the focus, even after shifting often
    the ratio in one pixel will vastly change.
    Nevertheless, on average, the ratio should be close to the theoretical
    optimum.
    This can be investigated again with a histogram. We will have many
    outliers, but the majority of values will collect around the expected
    ratio in the histogram.

    Thankfully, due to thresholding, we eliminated all values close to 0,
    so the ratios are either `nan` or rather well-behaved (not sure if that
    makes them more accurate, though...)

    Initial testing showed that the median of each distribution is close to
    the "visual" center of the peak, but I deemed the Gauss fit to be a little
    more faithful (and maybe less influenced by noise), so the new best ratio
    is determined from a Gauss fit on the histogram distribution.

    Change `show_histograms=True` below to inspect the ratios (only for the
    first batch).


    __NB__ For reasons I don't understand (or am too lazy to think about)
    dividing `img_high/img_low` produces more symmetric histograms where the
    median is almost always spot on the Gauss fit. So let's keep it this
    direction.
    That just means we have to invert the ratios to match the convention we
    chose above
    """
    if 'Filtersetting' in sorted_sequences.columns:
        filtersettings = sorted_sequences.Filtersetting.to_list()
    else:
        #TODO check entire script works with this
        # using SequenceNumber instead of filter setting as fallback
        raise NotImplementedError('Working without Filtersetting/Flags'
                                  + ' column not suported (yet)')
    
    new_relative_ratios = []

    for k, hdr_stack in hdr_stacks.items():
        this_new_relative_ratios = [1,] # first one unity by def
        for lower_setting_idx in range(len(filtersettings)-1):
            f1 = filtersettings[lower_setting_idx]
            f2 = filtersettings[lower_setting_idx + 1]

            #WORKAROUND: in all levels except lowest, thresholds lower+upper
            # are applied
            # to also blank lowest for this test, repeat blanking here:
            img_low = blank_image_to_range(hdr_stack[f1],
                                           threshold_lower,
                                           threshold_upper)
            img_high = hdr_stack[f2]
            
            """
            __NB__ For reasons I don't understand (or am too lazy to think about)
            dividing `img_high/img_low` produces more symmetric histograms where
            the median is almost always spot on the Gauss fit. So let's keep it
            this direction.
            That just means we have to invert the ratios to match the
            convention we chose above.
            """

            ratio = img_high/img_low
            ratios_in_img = ratio[~np.isnan(ratio)]
            A, mean, stddev, median = fit_gaussian_to_hist(ratios_in_img,
                                                           plot=show_some_histograms)
            this_new_relative_ratios.append(mean)
        this_new_relative_ratios = np.asarray(this_new_relative_ratios)
        this_new_relative_ratios = 1/this_new_relative_ratios # to match convention above
        new_relative_ratios.append(this_new_relative_ratios)

        show_some_histograms = False # even if it was True, only show first batch, after that stop.
    new_relative_ratios = np.stack(new_relative_ratios)
    new_relative_ratios_mean = new_relative_ratios.mean(axis=0)
    new_relative_ratios_std = new_relative_ratios.std(axis=0)

    relative_ratios = {filtersettings[0]: 1.0}
    for i in range(1, len(filtersettings)):
        relative_ratios[filtersettings[i]] = new_relative_ratios_mean[i]
    relative_ratios_std = {filtersettings[0]: 0.0}
    for i in range(1, len(filtersettings)):
        relative_ratios_std[filtersettings[i]] = new_relative_ratios_std[i]
    
    return relative_ratios, relative_ratios_std


def calculate_absolute_ratios(relative_ratios):
    filtersettings = list(relative_ratios.keys())
    relative_ratio_values = list(relative_ratios.values())
    cumulative_ratios = [1,]
    for i in range(1, len(relative_ratio_values)):
        ratio_from_0_to_here = cumulative_ratios[-1] * relative_ratio_values[i]
        cumulative_ratios.append(ratio_from_0_to_here)
    absolute_ratios = {}
    for i in range(len(filtersettings)):
        absolute_ratios[filtersettings[i]] = cumulative_ratios[i]
    return absolute_ratios


def make_RGB_representation(img1, img2):
    """
    From input arrays, generate a MxNx3 array which is interpreted as RGB by matplotlib.
    Scale each array individually to range (min, max) and translate to 0..255 for uint8
    display.
    """
    assert img1.shape == img2.shape
    img_rgb = np.zeros((*img1.shape, 3),dtype=np.uint8)
    
    img1 = img1.copy()
    img1[img1<0] = 0
    img1[np.isnan(img1)] = 0
    img1 /= img1.max() #do this after removing NaN, else need np.nanmax()
    
    img2 = img2.copy()
    img2[img2<0] = 0
    img2[np.isnan(img2)] = 0
    img2 /= img2.max() #do this after removing NaN, else need np.nanmax()
    
    fake_log = False
    if fake_log:
        # apply sqrt to make it look slightly log-scale, at least boost low-intensity areas
        img1 = img1**0.5
        img2 = img2**0.5
        
    img1 *= 255
    img1 = img1.astype(np.uint8)
    img2 *= 255
    img2 = img2.astype(np.uint8)
    
    img_rgb[:,:,1] = img1
    img_rgb[:,:,0] = img2
    return img_rgb


def calculate_filter_transmissions_from_ratios(absolute_ratio_dict):
    """
    Try to reconstruct the actual filter transmission values from the
    data (where effects of multiple filters are combined).
    Work in the Log domain so filters are additive, not multiplicative
    (as we are anyway used to in the ND filter terminology)

    __NB__ The "raw" data we have is filter changes, e.g. from `F0100` to 
    `F1000` step we have `+F1` but `-F2`. Of course, in the absolute ratios
    this is much easier to handle. Since we just got the absolute ones by
    cumulative multiplication (cumulative addition in log domain) there
    should not be any difference in accuracy etc. using the absolute values
    directly, right?

    __NB__ A ratio of 2.5 from one level to the next actually means the
    filter reduces the transmission by 2.5-fold, i.e. the inverse. A filter
    will generally have a transmission of 1 (`log10(1)=0 = ND0`) or less
    (e.g. $T=10^{-0.3}\approx 0.5$), so the ND values stated here are
    negative, even if the convention is positive. We will simply neglect the
    sign in the end, but I think it is safer to calculate with negatives to
    avoid confusion.
    """
    absolute_transmissions_log = {k: np.log10(1/v) for k,v in absolute_ratio_dict.items()}
    
    known_combined_transmissions = []
    filter_combinations = []
    for f, fval in absolute_transmissions_log.items():
        # from e.g. "F0110" get the 4 ints (1 or 0) for filters
        contributions = [int(c != "0") for c in f[1:]]
        filter_combinations.append(contributions)
        known_combined_transmissions.append(fval)
    known_combined_transmissions = np.asarray(known_combined_transmissions)
    filter_combinations = np.asarray(filter_combinations)

    individual_filters_solution = np.linalg.lstsq(filter_combinations,
                                                  known_combined_transmissions)[0]
    
    """
    Reinsert the determined individual filter values for each level.
    Since this adds a little more information to the physics model, we will
    use the updated values for the HDR stacking (effectively exploiting our
    knowledge about the filter combinations for the first time).
    """
    reconstructed_absolute_transmissions_log = {f: val for (f, val) in zip(
        absolute_transmissions_log.keys(),
        filter_combinations @ individual_filters_solution)}
    reconstructed_absolute_ratios = {k: 1/10**val for k, val in
                                     reconstructed_absolute_transmissions_log.items()}
    
    filter_combinations_no_of_occurences = filter_combinations.sum(axis=0)
    individual_filters_solution[filter_combinations_no_of_occurences == 0] = np.nan
    filter_transmissions_log = {k: val for k, val in zip(
        ['F1000','F0100','F0010','F0001'],individual_filters_solution)}
    filter_transmissions = {k: 10**val for k, val in filter_transmissions_log.items()}
    # print('Reconstructed filter values:')
    # [print(f'{k} = {val*100:5.1f}%') for (k, val) in filter_transmissions.items()];
    return filter_transmissions, reconstructed_absolute_ratios



def get_tqdm(use_tqdm=None):
    if use_tqdm is None:
        # making dummy is easier than having if/else all around
        def tqdm(iter):
            return iter
    else:
        tqdm = use_tqdm
    return tqdm

def all_in_one_stacking(imgs,
                        threshold_lower,
                        threshold_upper,
                        use_tqdm=None,
                        inplace=True):
    tqdm = get_tqdm(use_tqdm)
    if not inplace:
        raise NotImplementedError('Not possible yet, must modify `imgs` inplace.')
    
    """
    The algorithm can either start at top or bottom, let's choose bottom for
    arguments sake.
    Find the sorting of the filter settings brightest to least bright (now
    after dark image subtraction).

    Of course we could rely on the input order, but it seems nicer and not
    much effort to have them auto-sorted.
    """
    sorted_sequences = sort_filtersettings_by_brightness(imgs)

    """
    next, try to infer the filter settings from the data

    The lowest (brightest) level has ratio 1 by definition, so we will not
    scale the camera signal.
    After that, let's iteratively work on the ratios from one level to the
    next.

    Since the images are potentially shifted with respect to each other due to
    jitter, we cannot compare pixel-by-pixel. Instead, blank both histograms
    to an overlapping range assuming some filter ratio and check if the number
    of pixels left in each image is equal. If it is, we found a good ratio. If
    it differs, most likely the filter ratio is wrong.
    """
    generate_histograms(imgs)
    initial_relative_ratios, initial_relative_ratios_std = estimate_ratios_on_histograms(imgs,
                                               sorted_sequences,
                                               threshold_lower,
                                               threshold_upper)

    #[print(f'Ratio {ff}->next {ratio:.2f} +- {100*std/ratio:.1f}%') for ff, ratio, std in zip(relative_ratios.keys(), relative_ratios.values(), relative_ratios_std.values())];
    """
    Over many different iterations and variations of this section of the
    script, the numbers always turned out similar, so I trust the general
    method.
    That being said, the jumps inside one measurement set are already several
    percent.

    This indicates we should definitely repeat the scaling once the images
    were shifted on top of each other.
    """

    """
    We now have input data that we trust in terms of dynamic range, dark
    images, blanked areas. Also, we should have reasonable ratios between each
    filter setting.

    The last step before being able to assemble the HDR focus is to determine
    by how much each exposure is shifted w.r.t to the reference.
    We will start again at the lowest level (brightest image) and gradually
    work our way up.

    As demonstrated in the `iceberg` test script, before cross-correlating
    to find the shift, we must first make sure that both images show the same
    information.
    Since we now have an estimate for the filter ratio between both, we should
    be able to blank both images to the common dynamic range.
    They should therefore appear visually similar, even though they are at
    different absolute scales.

    Next, use cross correlation to determine the number of pixels by which to
    shift the higher filter setting to match the lower. Setting all
    overexposed or NaN values to 0 will give them zero weight in the
    correlation and thus do exactly what we want.

    In a previous version of the script, we investigated in detail whether
    shifting has a pairwise unique answer, i.e. finding the shift of
    F0000_A -> F0010_B and F0000_A -> F0000_C will mean the shift from B to C
    is simply the difference in shift of A-B and A-C.
    Unfortunately it is not, there were deviations by +1 or -1 pixel in both
    dimensions, i.e. +-2 pixel offset worst case.

    If this had worked, we could easily determine all shifts with respect to
    all images. But since we always have to decide which images we are
    comparing, it would be too expensive to do all combinations.
    Instead:
    * start at lowest level, determine mutual shifts and find image closest
    to "center" of all occurences
    * in next level, find closest (which has to be shifted the least), shift
    it on top
    * in the next level, repeat (now correlating with previously shifted
    layer, but this is more or less on top of ground level)
    * in each step, mark used images as used so we do not re-use them
    * repeat for the next image in lowest batch (will be less "centered")
    -> gradually use up all higher levels

    This means we start with the "best guess" center representative in
    lowest level, and find always the closest matching in next level.
    For the last image in lowest batch, we might be left with unfitting
    corresponding images in higher levels, though.
    Together, this should yield some guesses of best focus and some outliers
    of badly stitched lower estimates...


    We will not actually shift the ground levels (since they make the basis
    for N different HDR stacks, no need to shift them).
    But we do need to calculate the shifts to find the most centered one and
    the "distance" of the others to that center one.

    Then, sort the lowest image batch by distance to iteratively work our way
    from closest to furthest while stacking higher levels.

    __NB__ In this section, we collect matching images, shift them around and
    blank them to their individual dynamic range. However we do not scale them
    yet, since a more accurate relative ratio will be established later.
    """
    sorted_distances = determine_lowest_level_distances(imgs,
                                                        sorted_sequences,
                                                        threshold_lower,
                                                        threshold_upper,
                                                        use_tqdm=tqdm)
    
    hdr_stacks, hdr_stacks_indices = auto_shift_and_stack_all(imgs,
                                                          sorted_sequences,
                                                          sorted_distances,
                                                          initial_relative_ratios,
                                                          threshold_lower,
                                                          threshold_upper,
                                                          use_tqdm=tqdm)
    
    """
    We confirmed above that the overlap of the images seems acceptable.
    Now, we can revisit the ratio between two images, since we can not
    only compare the number of occurences of equivalent pixels, but are
    allowed to use their location for input.

    Due to the jitter and breathing of the focus, even after shifting often
    the ratio in one pixel will vastly change.
    Nevertheless, on average, the ratio should be close to the theoretical
    optimum. Again use a histogram of possible filter ratios and find the
    "optimum one" by a Gaussian fit around the peak in the histogram.
    """
    relative_ratios, relative_ratios_std = determine_ratios_from_stacks(hdr_stacks,
                                                                    sorted_sequences,
                                                                    threshold_lower,
                                                                    threshold_upper,
                                                                    show_some_histograms=False)
    #[print(f'Ratio {ff}->next {ratio:.2f} +- {100*std/ratio:.1f}%') for ff, ratio, std in zip(relative_ratios.keys(), relative_ratios.values(), relative_ratios_std.values())];
    """
    We can see the resulting ratios still carry a lot of variation from stack to stack, keep this in mind when interpreting the data.
    """
    absolute_ratios = calculate_absolute_ratios(relative_ratios)

    """
    We should be able to exploit our knowledge that some ratios are actually
    made up by the same fundamental filters.
    Since the filters are multiplicative and therefore linear in log-domain,
    we can create a linear system of equations and solve for least squares.

    TODO this script will fail for e.g. HDRs of different exposure where
    F1234 is not available. Make it more generic in future.
    """
    filter_transmissions, reconstructed_absolute_ratios = calculate_filter_transmissions_from_ratios(absolute_ratios)

    """
    Cross-check: the deviation between the individually retrieved ratios and
    the forward calculated relative ratios based on reconstructed filter
    values is only on the percent level (probably we could have learned this
    immediately from the least squares residuals returned...)

    print('Reconstructed filter values:')
    [print(f'{k} = {val*100:5.1f}%') for (k, val) in filter_transmissions.items()];
    For `20250227` dataset we retrieve `F1=11.0%, F2=22.3%, F3=37.4%, F4=nan%`.

    Compare this to what is written in e.g. BSc Nico Werner:
    * F1 = 10.65%
    * F2 = 21.35% ("Filter seems to have been moved" compared to previously
      documented values)
    * F3 = 34% (not previously documented, "calibration by eye")
    * F4 = 49.6%

    In conclusion, we could retrieve the filter transmissions within a few
    percent margin of error. Since the spectrum changes from day to day, a
    reference calibration might not be much more accurate, but maybe overall
    it does indicate it would be good to use the filter values (would also
    save a lot of time in this script determining stuff automatically).
    """
     
    """
    Next, apply the now determined absolute scaling
    """
    absolute_hdr_stacks = {}

    for first_img_idx, hdr_stack in hdr_stacks.items():
        abs_hdr_stack = {}
        for f1, img_blanked in hdr_stack.items():
            img_blanked = img_blanked.copy()
            img_blanked *= reconstructed_absolute_ratios[f1]
            abs_hdr_stack[f1] = img_blanked
        absolute_hdr_stacks[first_img_idx] = abs_hdr_stack

    """
    Now we want to blend one HDR image from each stack.

    Some methods are:
    * "highest wins"
        * start with lower level (brighter image)
        * any information contained in next level (not NaN) will replace the
          pixels
        * and so on
    * "blending"
        * vertically stack all images, each one is blanked already
        * take the nanmean across the third dimension, this will take the mean
          where there is more than 1 value, just the base one otherwise as before
    * maybe we can get more clever (e.g. openCV blend functions, or custom
      weighting).

    The blending seems to be a good compromise, but probably we should test
    both methods and see how large (if any) the influence on final parameters
    like peak intensity is.

    Result: blending produces visually nicer images, but crucially it can
    alter the peak intensity variance: If two filtersettings are not
    overexposed at peak, the `nanmean` will have 2 or more values to choose
    from. By definition, the average will always be lower than the maximum
    outlier. So firstly, we will never see the one lucky image of highest peak
    intensity, always something lower. Secondly, when producing statistics
    later on, the variance of the peak intensity will be reduced. -> choose
    highest wins for now
    """
    method_highest_wins = True
    if method_highest_wins:
        full_hdr_imgs = {}
        for first_img_idx, hdr_stack in absolute_hdr_stacks.items():
            hdr_img = np.zeros_like(hdr_stack[list(hdr_stack.keys())[0]])
            for filtersetting, img in hdr_stack.items():
                hdr_img[~np.isnan(img)] = img[~np.isnan(img)] # paste any non-nan pixels
            full_hdr_imgs[first_img_idx] = hdr_img
    else:
        # blending method
        full_hdr_imgs = {}
        for first_img_idx, hdr_stack in absolute_hdr_stacks.items():
            hdr_list = []
            for filtersetting, img in hdr_stack.items():
                hdr_list.append(img)
            hdr_img_stacked = np.stack(hdr_list)
            hdr_img = np.nanmean(hdr_img_stacked, axis=0)
            full_hdr_imgs[first_img_idx] = hdr_img
    
    """## Shift all to peak
    Not sure if this is really the best solution, but there are a few pro
    arguments.
    We loose the ability to analyse the jitter, but we gain an important
    fact: the peak is always in the center pixel.

    From a theoretical standpoint, "tip-tilt aberration is minimized if the
    centroid is in the center, not the peak". However, in pragmatic reality,
    we are probably more interested e.g. in the energy enclosed around the
    peak.

    Since the validation plots rely on the original locations (or we
    would have to shift all stacks, too), keep also unshifted versions.
    """
    crop_to_size = 800
    cropped_hdr_imgs = {}
    for first_img_idx, hdr_img in full_hdr_imgs.items():
        hdr_img_shifted = center_peak_and_crop(hdr_img, crop_to_size)
        cropped_hdr_imgs[first_img_idx] = hdr_img_shifted

    validation_info = {}
    validation_info['relative_ratios'] = relative_ratios
    validation_info['relative_ratios_std'] = relative_ratios_std
    validation_info['filter_transmissions'] = filter_transmissions
    validation_info['absolute_ratios'] = absolute_ratios
    validation_info['reconstructed_absolute_ratios'] = reconstructed_absolute_ratios
    return cropped_hdr_imgs, full_hdr_imgs, absolute_hdr_stacks, validation_info

def generate_overview_plot_for_run(imgs,
                                   statistics_df,
                                   validation_info,
                                   beamtime_date,
                                   run_to_analyse,
                                   file_bitdepth,
                                   threshold_lower,
                                   threshold_upper,
                                   save_path='',
                                   save_filename='',
                                   ):
    """
    Show general info.
    Check that background subtraction worked as expected for a couple of
    exposures.
    """
    relative_ratios = validation_info['relative_ratios']
    relative_ratios_std = validation_info['relative_ratios_std']
    filter_transmissions = validation_info['filter_transmissions']
    absolute_ratios = validation_info['absolute_ratios']
    reconstructed_absolute_ratios = validation_info['reconstructed_absolute_ratios']
    save_overview_plots = False

    relative_ratio_string = '\n'.join([
        f'Ratio prev->{ff} {ratio:.2f} +- {100*std/ratio:.1f}%'
        for ff, ratio, std in zip(
            relative_ratios.keys(),
            relative_ratios.values(),
            relative_ratios_std.values())])

    fig, ((ax11, ax12, ax13,), (ax21, ax22, ax23,)) = plt.subplots(2,3,
                                                                figsize=(15,8),
                                                                #sharex='col',
                                                                #sharey='col',
                                                                dpi=70) # make small dpi to fit on screen,
                                                                    # for PNG has better resolution anyway

    # **** shot statistics overview *****
    ax = ax11
    statistics_df[statistics_df.is_dark==True].raw_image_mean.plot(ax = ax, ls='', marker='x', label='dark')
    statistics_df[statistics_df.is_skipped==True].raw_image_mean.plot(ax = ax, ls='', marker='x', label='skipped')
    statistics_df[statistics_df.is_bright==True].raw_image_mean.plot(ax = ax, ls='', marker='x', label='bright')
    ax.set_xlabel('Image in series')
    ax.set_ylabel('Mean pixel value in image')
    ax.legend()

    # **** test image last *****
    ax = ax21
    ax.set_axis_off()
    filtervalues = '\n'.join([f'{k} = {val*100:5.1f}%' for (k, val) in filter_transmissions.items()])
    text = '\n'.join([
        f"{beamtime_date} Run {run_to_analyse:02d}",
        f"bit depth {file_bitdepth}bit",
        f"used dynamic range {threshold_lower:.2f}->{threshold_upper:.2f}",
        f"relative ratios:",
        f"{relative_ratio_string}",
        f"reconstructed filter values:",
        f"{filtervalues}",])
    
    ax.text(0,0.95,
            text,
            verticalalignment='top')

    # **** test image first *****
    test_img_idx_min = imgs.index.min()
    test_img = imgs.image[imgs.index.min()]
    test_img_blurred = scipy.ndimage.gaussian_filter(test_img, 6) # blur to see more clearly and ignore high frequency fluctiations

    limit = 5e-3
    ax = ax12
    imh = ax.imshow(test_img, cmap='bwr', vmin=-limit, vmax=limit)
    ax.set_title('First test image - original')
    #https://stackoverflow.com/questions/18195758/set-matplotlib-colorbar-size-to-match-graph
    # hail the magic numbers -> actually they don't work well here :(
    #fig.colorbar(imh, ax=ax,fraction=0.040, pad=0.04)

    ax = ax13
    imh = ax.imshow(test_img_blurred, cmap='bwr', vmin=-limit, vmax=limit)
    ax.set_title('First test image - blurred')
    plt.colorbar(imh, ax=ax, label='fraction of dynamic range',fraction=0.046, pad=0.04)
    #fig.suptitle('Dark-subtracted test image in blue-red color scale heavily saturated');


    # **** test image last *****
    test_img_idx_max = imgs.index.max()
    item = imgs.loc[test_img_idx_max]
    test_img = item.image
    test_img_blurred = scipy.ndimage.gaussian_filter(test_img, 6) # blur to see more clearly and ignore high frequency fluctiations

    limit = 5e-3
    ax = ax22
    imh = ax.imshow(test_img, cmap='bwr', vmin=-limit, vmax=limit)
    ax.set_title('Last test image - original')
    #fig.colorbar(imh, ax=ax,fraction=0.046, pad=0.04)

    ax = ax23
    imh = ax.imshow(test_img_blurred, cmap='bwr', vmin=-limit, vmax=limit)
    ax.set_title('Last test image - blurred')
    plt.colorbar(imh, ax=ax, label='fraction of dynamic range',fraction=0.046, pad=0.04)
    #fig.suptitle('Dark-subtracted test image in blue-red color scale heavily saturated');


    #plt.tight_layout() #sometimes helps, sometimes messes up even more...

    # ********************
    fig.suptitle(f'HDR overview for {beamtime_date} run {run_to_analyse:02d}')

    if save_path:
        os.makedirs(save_path, exist_ok=True)
        if not save_filename:
            save_filename = f'{beamtime_date}_Run{run_to_analyse:02d}_HDR_overview.png'
        savepath = os.path.join(save_path, save_filename)
        fig.savefig(savepath, dpi=150)

def generate_hdr_validation_plots(imgs,
                                 cropped_hdr_imgs,
                                 full_hdr_imgs,
                                 absolute_hdr_stacks,
                                 validation_info,
                                 beamtime_date,
                                 run_to_analyse,
                                 file_bitdepth,
                                 threshold_lower,
                                 threshold_upper,
                                 save_path='',
                                 save_filename_pattern='',
                                 use_tqdm=None,
                                 ):
    """
    Generate a validation plot of the merged HDR image as well as some views
    of the input data.

    Normally in a contour plot the color indicates level.
    __Here it does not!__
    Instead, the color indicates which filter setting all contours of one
    color come from.

    With only a few contours, that should allow us to visually find ones
    which are close in more than 1 filtersetting.
    For an ideal image, the contours at a certain absolute level should
    perfectly match in all filter settings.
    For an image with wrong shift, they will be shifted.
    For an image with wrong absolute scale (error in filter calibration),
    the contours should grow or shrink with respect to same contour in
    neighbouring filter setting.

    _However, when inspecting actual data, I was not able to reliably detect
    by eye if a scaling error of 5% was present, even when it was introduced
    on purpose. So probably these plots only help if something is really off._
    """
    tqdm = get_tqdm(use_tqdm)
    if not save_filename_pattern:
        save_filename_pattern = ('{beamtime_date}'
                                 + '_Run{run_to_analyse:02d}'
                                 + '_HDR_stacking_{i_saved:02d}'
                                 +'_ID{first_img_idx:02d}.png')

    i_saved = 0
    for first_img_idx in tqdm(cropped_hdr_imgs):
        full_hdr_img = full_hdr_imgs[first_img_idx]
        cropped_hdr_img = cropped_hdr_imgs[first_img_idx]
        hdr_stack = absolute_hdr_stacks[first_img_idx]

        baseimg_fname = imgs.ImageFileName.loc[first_img_idx]
        baseimg_fname = os.path.basename(baseimg_fname) # strip path
        suptitle = f'HDR starting at image {baseimg_fname}'

        save_filename = save_filename_pattern.format(
            beamtime_date=beamtime_date,
            run_to_analyse=run_to_analyse,
            i_saved=i_saved,
            first_img_idx=first_img_idx,
        )

        generate_hdr_validation_plot(cropped_hdr_img,
                                     full_hdr_img,
                                     hdr_stack,
                                     suptitle,
                                     save_path,
                                     save_filename
                                     )
        i_saved += 1

def generate_hdr_validation_plot(cropped_hdr_img,
                                 full_hdr_img,
                                 hdr_stack,
                                 suptitle='',
                                 save_path='',
                                 save_filename='',
                                 ):
    """
    Generate a validation plot of the merged HDR image as well as some views
    of the input data.

    Normally in a contour plot the color indicates level.
    __Here it does not!__
    Instead, the color indicates which filter setting all contours of one
    color come from.

    With only a few contours, that should allow us to visually find ones
    which are close in more than 1 filtersetting.
    For an ideal image, the contours at a certain absolute level should
    perfectly match in all filter settings.
    For an image with wrong shift, they will be shifted.
    For an image with wrong absolute scale (error in filter calibration),
    the contours should grow or shrink with respect to same contour in
    neighbouring filter setting.

    _However, when inspecting actual data, I was not able to reliably detect
    by eye if a scaling error of 5% was present, even when it was introduced
    on purpose. So probably these plots only help if something is really off._
    """
    cy, cx = np.unravel_index(np.nanargmax(full_hdr_img), full_hdr_img.shape)
    cy, cx
    zoomrange_logscale = 100 #[pixels] on plot
    zoomrange_linscale = 50 #[pixels] on plot, less interesting stuff so zoom in

    """
    The automatic log scale "wastes" a lot of dynamic range on seeming
    noise, so try to adjust the scale a little.
    Use top left corner to find std-dev of background noise (guessing this
    corner will be clear of signal and representative).
    The ratio between this std-dev and the image max can give a good
    guideline for log-scale plotting.
    """
    cornersize = 100
    corner = full_hdr_img[:cornersize, :cornersize]
    std = np.nanstd(corner)
    useful_lower_logbase = 0.5*std # 0.5 simply by eye to make it look nice
    useful_lower_logbase

    fig, ((ax11, ax12, ax13), (ax21, ax22, ax23)) = plt.subplots(
        2,3,figsize=(15,8), dpi=70)
    # make small dpi to fit on screen,
    # for PNG has better resolution anyway
    
    # **** contour levels *****

    # ** build levels for contour plot **
    max_of_all_imgs = []
    for img in hdr_stack.values():
        max_of_all_imgs.append(np.nanmax(img))
    max_of_all_imgs = np.max(max_of_all_imgs)
    # apply a log level contour to show the pedestal as well as some peak features
    # show only very few levels, gets confusing anyway...
    number_of_contours = 4
    clevels = np.logspace(-2, 0, number_of_contours) * max_of_all_imgs

    ax = ax13
    cols = plt.cm.jet(np.linspace(0,1,len(hdr_stack))) # cycle color map manually
    for i, (filtersetting, img) in enumerate(hdr_stack.items()):
        ax.contour(img, levels=clevels, colors=cols[i])
        ax.plot(np.nan, np.nan, color=cols[i], label=filtersetting) # fake a legend
    ax.legend()
    ax.set_aspect('equal', adjustable='box') # required otherwise overconstraining xlim+ylim below
    ax.set_title(f'Contours at equal levels')
    ax.set_xlim(cx-zoomrange_linscale, cx+zoomrange_linscale)
    ax.set_ylim(cy+zoomrange_linscale, cy-zoomrange_linscale)
    
    # **** lineout x *****
    ax = ax22
    hdr_line_x = full_hdr_img[cy, :]
    for i, (filtersetting, img) in enumerate(hdr_stack.items()):
        line_x = img[cy, :]
        ax.plot(line_x, label=filtersetting)
    ax.plot(hdr_line_x, 'k', label='merged', lw=0.71)
    ax.set_xlim(cx-zoomrange_linscale, cx+zoomrange_linscale)
    ax.set_title(f'X-Lineout at peak Y={cy}')
    ax.legend()
    
    # **** lineout y *****
    ax = ax23
    hdr_line_y = full_hdr_img[:, cx]
    for i, (filtersetting, img) in enumerate(hdr_stack.items()):
        line_y = img[:, cx]
        ax.plot(line_y, label=filtersetting)#, lw=0.71)
    ax.plot(hdr_line_y, 'k', label='merged', lw=0.71)
    ax.set_xlim(cy-zoomrange_linscale, cy+zoomrange_linscale)
    ax.set_title(f'Y-Lineout at peak X={cx}')
    ax.legend()
    
    # **** HDR image *****
    ax = ax12
    imh = ax.imshow(full_hdr_img, cmap='inferno', norm=colors.LogNorm(vmin=useful_lower_logbase))
    fig.colorbar(imh, ax=ax)
    ax.set_xlim(cx-zoomrange_logscale,cx+zoomrange_logscale)
    ax.set_ylim(cy+zoomrange_logscale,cy-zoomrange_logscale) # remember flipped for images
    ax.set_title('Zoom of merged HDR, log scale')


    # **** cropped HDR image *****
    cy, cx = np.unravel_index(np.nanargmax(cropped_hdr_img), cropped_hdr_img.shape)
    zoomrange_logscale_cropped = 0 #[pixels] on plot, 0 for disabled
    cornersize = 100
    corner = cropped_hdr_img[:cornersize, :cornersize]
    std = np.nanstd(corner)
    useful_lower_logbase = 0.5*std # 0.5 simply by eye to make it look nice

    ax = ax11
    imh = ax.imshow(cropped_hdr_img, cmap='inferno',
                    norm=colors.LogNorm(vmin=useful_lower_logbase))
    fig.colorbar(imh, ax=ax)
    if zoomrange_logscale_cropped > 0:
        ax.set_xlim(cx-zoomrange_logscale_cropped,cx+zoomrange_logscale_cropped)
        ax.set_ylim(cy+zoomrange_logscale_cropped,cy-zoomrange_logscale_cropped) # remember flipped for images
        ax.set_title('Zoom of cropped merged HDR, log scale')
    else:
        ax.set_title('Cropped merged HDR, log scale')
    

    # **** empty corner *****
    ax = ax21
    ax.set_axis_off()

    # **** General *****
    fig.suptitle(suptitle)
    
    if save_path:
        os.makedirs(save_path, exist_ok=True)
        savepath = os.path.join(save_path, save_filename)
        fig.savefig(savepath, dpi=150)
        print('Saved', savepath)

def center_peak_and_crop(hdr_img,
                         crop_to_size):
    cy, cx = np.unravel_index(np.nanargmax(hdr_img), hdr_img.shape)
    Y, X = hdr_img.shape
    shift_x = X//2 - cx
    shift_y = Y//2 - cy
    hdr_img_shifted = shift_with_fillvalue(hdr_img, shift_x, shift_y)
    N_crop = crop_to_size//2
    if crop_to_size > 0:
        hdr_img_shifted_crop = hdr_img_shifted[max(Y//2-N_crop, 0):Y//2+N_crop, max(X//2-N_crop, 0):X//2+N_crop] #avoid negative values!
        hdr_img_shifted = hdr_img_shifted_crop
    if hdr_img_shifted.shape != (crop_to_size, crop_to_size):
        raise ValueError(f'The resulting crop is smaller than expected: {hdr_img_shifted.shape} != {(crop_to_size, crop_to_size)}')
    return hdr_img_shifted


def save_hdrs_to_file(cropped_hdr_imgs,
                      beamtime_date,
                      run_to_analyse,
                      save_path,
                      save_filename_pattern=''):

    if not save_filename_pattern:
        save_filename_pattern = ('{beamtime_date}'
                                 +'_Run{run_to_analyse:02d}'
                                 +'_HDR_{i_saved:02d}'
                                 +'_ID{first_img_idx:02d}.npy')
    os.makedirs(save_path, exist_ok=True)
    i_saved = 0
    for first_img_idx in cropped_hdr_imgs:
        hdr_img = cropped_hdr_imgs[first_img_idx]
        

        fname = save_filename_pattern.format(
            beamtime_date=beamtime_date,
            run_to_analyse=run_to_analyse,
            i_saved=i_saved,
            first_img_idx=first_img_idx,
        )
        savepath = os.path.join(save_path, fname)
        np.save(savepath, hdr_img)
        print('Saved', savepath)
        i_saved += 1
