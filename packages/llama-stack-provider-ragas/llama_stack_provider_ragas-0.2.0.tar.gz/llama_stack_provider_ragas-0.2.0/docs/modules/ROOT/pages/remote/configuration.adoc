= Remote Provider Configuration
:navtitle: Configuration

== Environment Variables

Create a `.env` file in the project root with the following variables:

[,properties]
----
# Required for both inline and remote
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Llama Stack server URL for remote provider
KUBEFLOW_LLAMA_STACK_URL=<your-llama-stack-url>

# Kubeflow Pipelines endpoint
KUBEFLOW_PIPELINES_ENDPOINT=<your-kfp-endpoint>

# Kubernetes namespace for Kubeflow
KUBEFLOW_NAMESPACE=<your-namespace>

# Container image for remote execution
KUBEFLOW_BASE_IMAGE=quay.io/diegosquayorg/my-ragas-provider-image:latest
----

== Environment Variable Details

`EMBEDDING_MODEL`::
The embedding model to use for RAGAS evaluation. This should match a model available in your Llama Stack configuration.

`KUBEFLOW_LLAMA_STACK_URL`::
The URL of the Llama Stack server that the remote provider will use for LLM generations and embeddings. If running Llama Stack locally, you can use https://ngrok.com/[ngrok] to expose it to the remote provider.

`KUBEFLOW_PIPELINES_ENDPOINT`::
The endpoint URL for your Kubeflow Pipelines server. You can get this by running:
+
[,bash]
----
kubectl get routes -A | grep -i pipeline
----

`KUBEFLOW_NAMESPACE`::
The name of the data science project where the Kubeflow Pipelines server is running.

`KUBEFLOW_BASE_IMAGE`::
The container image used to run the Ragas evaluation in the remote provider. See the `Containerfile` in the repository root for details on building a custom image.

== Distribution Configuration

The repository includes a sample Llama Stack distribution configuration that uses Ollama as a provider for inference and embeddings.

The remote provider is setup in the following lines of the `run-remote.yaml`:

[,yaml]
----
eval:
  - provider_id: trustyai_ragas
    provider_type: remote::trustyai_ragas
    module: llama_stack_provider_ragas.remote
    config:
      embedding_model: ${env.EMBEDDING_MODEL}
      kubeflow_config:
        results_s3_prefix: ${env.KUBEFLOW_RESULTS_S3_PREFIX}
        s3_credentials_secret_name: ${env.KUBEFLOW_S3_CREDENTIALS_SECRET_NAME}
        pipelines_endpoint: ${env.KUBEFLOW_PIPELINES_ENDPOINT}
        namespace: ${env.KUBEFLOW_NAMESPACE}
        llama_stack_url: ${env.KUBEFLOW_LLAMA_STACK_URL}
        base_image: ${env.KUBEFLOW_BASE_IMAGE}
----

To run with the sample distribution:

[,bash]
----
dotenv run uv run llama stack run distribution/run-remote.yaml
----
